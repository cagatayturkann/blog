{
    "title": "Building Intelligent E-commerce Experiences with RAG",
    "permalink": "/blog/en/building-intelligent-ecommerce-rag/",
    "summary": "In this article, we will explore how Retrieval-Augmented Generation (RAG) can transform e-commerce applications. We\u0026rsquo;ll discuss the implementation of vector databases, semantic search, conversation memory, and multi-language support through a practical example project.",
    "content": "Building Intelligent E-commerce Experiences with RAG Introduction E-commerce platforms are constantly evolving to provide more personalized and efficient shopping experiences. Traditional search and recommendation systems often fall short when it comes to understanding the nuanced intent behind customer queries. This is where Retrieval-Augmented Generation (RAG) comes in, combining the power of large language models with precise information retrieval to create more intelligent shopping assistants.\nIn this article, we\u0026rsquo;ll explore how to implement a RAG-based e-commerce assistant that can:\nUnderstand and respond to complex product queries Maintain conversation context for follow-up questions Support multiple languages seamlessly Provide accurate product information from a vector database Generate human-like responses while avoiding hallucinations You can access the complete code for this project at GitHub repository. You can test the assistant from here Assistant\n1. Understanding RAG for E-commerce What is RAG? Retrieval-Augmented Generation (RAG) is an AI architecture that enhances large language models (LLMs) by retrieving relevant information from external knowledge sources before generating responses. Unlike traditional LLMs that rely solely on their pre-trained knowledge, RAG systems can access up-to-date, domain-specific information.\nThe RAG process consists of three main steps:\nRetrieval: When a user asks a question, the system searches a database for relevant information Augmentation: The retrieved information is added to the prompt sent to the LLM Generation: The LLM generates a response based on both its pre-trained knowledge and the retrieved information Why RAG is Perfect for E-commerce E-commerce presents unique challenges that make RAG particularly valuable:\nProduct Catalogs Change Frequently: New products are added, prices change, and inventory fluctuates Specific Product Details Matter: Customers need accurate information about specifications, compatibility, and features Query Intent is Complex: \u0026ldquo;Show me a laptop good for gaming under $1000\u0026rdquo; requires understanding multiple constraints Follow-up Questions are Common: \u0026ldquo;Does it have a backlit keyboard?\u0026rdquo; requires maintaining context from previous questions Traditional search systems match keywords but struggle with semantic understanding. RAG bridges this gap by combining semantic search with the conversational abilities of LLMs.\nRAG vs. Traditional Search in E-commerce Feature Traditional Search RAG-Based Search Query Understanding Keyword matching Semantic understanding Result Relevance Based on text similarity Based on meaning and context Handling Ambiguity Limited Strong Follow-up Questions Requires new search Maintains context Response Format List of products Conversational with product details Product Knowledge Limited to indexed fields Can include all product details 2. Vector Databases and Embeddings The Role of Vector Databases Vector databases are specialized storage systems designed to efficiently store and query high-dimensional vectors (embeddings). In our e-commerce RAG system, they serve as the foundation for semantic search.\nKey features of vector databases include:\nSimilarity Search: Finding vectors that are semantically similar to a query vector Efficient Indexing: Algorithms like HNSW (Hierarchical Navigable Small World) for fast retrieval Hybrid Search: Combining vector similarity with traditional filters Metadata Storage: Keeping product information alongside vectors For our e-commerce RAG project, we chose Weaviate as our vector database due to its robust features, GraphQL API, and hybrid search capabilities.\nCreating Product Embeddings Embeddings are numerical representations of text that capture semantic meaning. In our system, we generate embeddings for product information using Google\u0026rsquo;s text-embedding-004 model.\nHere\u0026rsquo;s how we implement the embedding generation in our project:\nconst { GoogleGenerativeAI } = require(\u0026#39;@google/generative-ai\u0026#39;); require(\u0026#39;dotenv\u0026#39;).config(); // Get API key from environment variables const GEMINI_API_KEY = process.env.GEMINI_API_KEY; // Initialize API client const genAI = GEMINI_API_KEY ? new GoogleGenerativeAI(GEMINI_API_KEY) : null; /** * Generates an embedding vector for the given text * @param {String} text - Text to generate embedding for * @returns {Array} - Embedding vector */ async function getEmbedding(text) { const model = genAI.getGenerativeModel({ model: \u0026#39;text-embedding-004\u0026#39; }); const result = await model.embedContent(text); return result.embedding.values; } These embeddings allow us to convert product descriptions, titles, and features into vectors that can be compared for semantic similarity.\nImplementing Vector Search With our products embedded in the vector database, we can now implement semantic search. The following code demonstrates how we retrieve relevant products based on a user query:\n/** * Retrieves product information from Weaviate vector database * @param {String} query - Search query * @returns {Array} - Product information */ const getProductInfo = async (query) =\u0026gt; { try { const embedding = await getEmbedding(query); // Use fixed alpha value for vector search const alpha = 0.5; // Clean query text for GraphQL - make it safe const cleanQuery = query .substring(0, 100) // Keep query at reasonable length .replace(/[^\\w\\s]/gi, \u0026#39;\u0026#39;) // Remove non-alphanumeric characters .replace(/\u0026#34;/g, \u0026#39;\u0026#39;) // Remove double quotes .trim(); const data = JSON.stringify({ query: `{ Get { Ecommerce ( hybrid: { query: \u0026#34;${cleanQuery}\u0026#34; alpha: ${alpha}, vector: ${JSON.stringify(embedding)}, } limit: 3 ) { data _additional { score } } } }`, }); const config = { method: \u0026#39;post\u0026#39;, url: process.env.WEAVIATE_URL + \u0026#39;/v1/graphql\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, Authorization: `Bearer ${process.env.WEAVIATE_API_KEY}`, }, data: data, }; const response = await axios.request(config); return response.data.data.Get.Ecommerce.map((item) =\u0026gt; item.data); } catch (error) { console.error(\u0026#39;Error during Weaviate query:\u0026#39;, error); return []; } }; The hybrid parameter in our query allows us to combine vector search (semantic similarity) with keyword matching. The alpha value (0.5) gives equal weight to both approaches, providing a balance between semantic understanding and keyword relevance.\n3. Conversation Management and Context Maintaining Conversation History One of the key challenges in building an effective e-commerce assistant is maintaining conversation context. Users often ask follow-up questions that reference previous queries or responses.\nIn our project, we use MongoDB to store conversation history. Each conversation is stored as a document with an array of messages:\n/** * Creates a new conversation * @param {String} title - Conversation title (optional) * @returns {Promise\u0026lt;Object\u0026gt;} - Created conversation */ const createConversation = async (title = null) =\u0026gt; { try { // Assign default title if null const conversationTitle = title || \u0026#39;New Conversation\u0026#39;; const conversation = new Conversation({ title: conversationTitle, messages: [] }); await conversation.save(); return conversation; } catch (error) { console.error(\u0026#39;Error creating conversation:\u0026#39;, error); throw error; } }; /** * Adds a message to a conversation * @param {String} conversationId - Conversation ID * @param {String} role - Message role (user/assistant) * @param {String} content - Message content * @returns {Promise\u0026lt;Object\u0026gt;} - Updated conversation */ const addMessageToConversation = async (conversationId, role, content) =\u0026gt; { try { const conversation = await Conversation.findById(conversationId); if (!conversation) { throw new Error(`Conversation not found with ID: ${conversationId}`); } conversation.messages.push({ role, content, timestamp: new Date() }); conversation.updatedAt = new Date(); await conversation.save(); return conversation; } catch (error) { console.error(\u0026#39;Error adding message to conversation:\u0026#39;, error); throw error; } }; Detecting Follow-up Questions To provide a seamless conversation experience, our system needs to detect when a user is asking a follow-up question. We use a dedicated classifier agent for this purpose:\n/** * Determines if a message is a follow-up question based on conversation history * @param {Array} conversationHistory - Array of previous conversation messages * @returns {String} - JSON string indicating if message is a follow-up (true/false) */ const classifierAgent = async (conversationHistory) =\u0026gt; { try { // Format conversation history for the AI model const formattedHistory = conversationHistory .map((msg) =\u0026gt; `${msg.role === \u0026#39;user\u0026#39; ? \u0026#39;User\u0026#39; : \u0026#39;Assistant\u0026#39;}: ${msg.content}`) .join(\u0026#39;\\n\u0026#39;); const response = await axios({ method: \u0026#39;post\u0026#39;, url: \u0026#39;https://openrouter.ai/api/v1/chat/completions\u0026#39;, headers: { Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, data: { model: \u0026#39;google/gemini-2.0-flash-001\u0026#39;, messages: [ { role: \u0026#39;system\u0026#39;, content: `You are a sophisticated AI agent specializing in determining if a user\u0026#39;s message is a follow-up question to a previous conversation. A follow-up question is a question that refers to or builds upon a previous question or response in the conversation. For example, if a user asks about a product and then asks about its price, the second question is a follow-up question. Analyze the conversation history and determine if the last user message is a follow-up question. Return only a JSON boolean value (true or false) as plain text.`, }, { role: \u0026#39;user\u0026#39;, content: `Here is the conversation history:\\n${formattedHistory}\\n\\nIs the last user message a follow-up question?`, }, ], }, }); return response.data.choices[0].message.content.replace(/\\n+$/, \u0026#39;\u0026#39;); } catch (error) { console.error(\u0026#39;OpenRouter API error:\u0026#39;, error); return \u0026#39;false\u0026#39;; } }; When a follow-up question is detected, we expand the search query to include context from previous messages:\n// If it\u0026#39;s a follow-up question, include previous user questions if (isFollowUp \u0026amp;\u0026amp; conversationHistory.length \u0026gt;= 3) { // Find previous user messages (last 2 user messages) const userMessages = conversationHistory.filter((msg) =\u0026gt; msg.role === \u0026#39;user\u0026#39;).slice(-2); if (userMessages.length \u0026gt;= 2) { const previousUserMessage = userMessages[0].content; // Previous user message searchQuery = `${await translatorAgent(previousUserMessage)} ${translatedMessage}`; console.log(`Expanded search query (follow-up): \u0026#34;${searchQuery}\u0026#34;`); } } This approach ensures that when a user asks something like \u0026ldquo;What about its battery life?\u0026rdquo; after asking about a specific laptop, our system understands the context and provides relevant information.\n4. Multi-language Support Automatic Language Detection and Translation To make our e-commerce assistant accessible to a global audience, we implemented automatic language detection and translation. This allows users to interact with the system in their preferred language while maintaining the semantic search capabilities.\nWe use a translator agent to convert user queries to English for processing:\n/** * Translates user messages to English * @param {String} message - User message to translate * @returns {String} - Translated message in English */ const translatorAgent = async (message) =\u0026gt; { try { const response = await axios({ method: \u0026#39;post\u0026#39;, url: \u0026#39;https://openrouter.ai/api/v1/chat/completions\u0026#39;, headers: { Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, data: { model: \u0026#39;google/gemini-2.0-flash-001\u0026#39;, messages: [ { role: \u0026#39;system\u0026#39;, content: `You are a sophisticated AI agent specializing in translating user queries from multiple languages into English. Before translating, you identify and correct typos, grammatical errors, and punctuation mistakes in the source text to ensure clarity and readability. Your translations should maintain the original context and be culturally nuanced. Return only the translated text as plain text.`, }, { role: \u0026#39;user\u0026#39;, content: message, }, ], }, }); return response.data.choices[0].message.content.replace(/\\n+$/, \u0026#39;\u0026#39;); } catch (error) { console.error(\u0026#39;OpenRouter API error:\u0026#39;, error); return message; // Fallback to original message } }; The system then responds in the same language as the user\u0026rsquo;s query. This is handled by our AI service, which is instructed to match the language of the input:\nconst messageContent = ` You are a helpful shopping assistant that can answer questions about products. Based on the user\u0026#39;s question \u0026#34;${prompt}\u0026#34;, analyze the product information in the context: ${context} and return information only about the most relevant matching product(s). Answer questions related to the ${category} of the product(s). Answer as humanly as possible and in the same language as the ${prompt}, but be careful with special names such as brand, model, etc. - don\u0026#39;t translate those. // ... additional instructions ... `; Language-Aware Product Information When displaying product information, we ensure that the system maintains the user\u0026rsquo;s language preference. The AI generates descriptions in the user\u0026rsquo;s language, while preserving brand names, model numbers, and technical specifications.\nFor example, if a user asks in Turkish: \u0026ldquo;Oyun için iyi bir laptop önerir misiniz?\u0026rdquo;, the system will:\nTranslate the query to English: \u0026ldquo;Can you recommend a good laptop for gaming?\u0026rdquo; Perform semantic search using the English query Retrieve relevant gaming laptops from the vector database Generate a response in Turkish, describing the gaming laptops while keeping technical terms intact This approach provides a seamless multilingual experience without requiring separate product databases for each language.\n5. AI Response Generation Contextual Response Generation Once we\u0026rsquo;ve retrieved relevant product information, we need to generate a helpful, contextual response. We use the Gemini API for this purpose, with a fallback to OpenRouter if needed:\n/** * Generates a response using Gemini or falls back to OpenRouter API * @param {String} prompt - User message * @param {String} context - Product information or other context * @param {String} category - Message category * @param {Array} conversationHistory - Conversation history * @returns {String} - AI response */ const generateGeminiResponse = async (prompt, context, category, conversationHistory = []) =\u0026gt; { try { // Format conversation history for the AI model const formattedHistory = conversationHistory .map((msg) =\u0026gt; `${msg.role === \u0026#39;user\u0026#39; ? \u0026#39;User\u0026#39; : \u0026#39;Assistant\u0026#39;}: ${msg.content}`) .join(\u0026#39;\\n\u0026#39;); const messageContent = ` You are a helpful shopping assistant that can answer questions about products. Based on the user\u0026#39;s question \u0026#34;${prompt}\u0026#34;, analyze the product information in the context: ${context} and return information only about the most relevant matching product(s). Answer questions related to the ${category} of the product(s). Answer as humanly as possible and in the same language as the ${prompt}, but be careful with special names such as brand, model, etc. - don\u0026#39;t translate those. If the category is [PRODUCT_INFO] and the question is about general product information (like price, features, specifications etc), include [SHOW_PRODUCT_INFO] at the end of your response so that product information can be formatted and displayed to the user. If the question is specifically about product reviews, ratings or customer feedback, do not include [SHOW_PRODUCT_INFO]. Instead, focus on providing a summary of the reviews and ratings from the available data. Also consider follow up questions. If the user asks a follow up question about a product mentioned earlier, your answer should be about the SAME product. Do not switch to a different product unless the user explicitly asks about a new product. At the end of your response, include a JSON object with an array of product IDs that you referenced in your answer, in the format: [PRODUCT_IDS]{\u0026#34;ids\u0026#34;:[\u0026#34;id1\u0026#34;,\u0026#34;id2\u0026#34;]}[/PRODUCT_IDS] Here is the conversation history so far: ${formattedHistory} User: ${prompt} Assistant: `; try { // Use Gemini API const model = genAI.getGenerativeModel({ model: \u0026#39;gemini-2.0-flash\u0026#39;, systemInstruction: messageContent, }); const res = await model.generateContent(prompt); return res.response.text(); } catch (geminiError) { // Fall back to OpenRouter API if Gemini fails console.log(\u0026#39;Gemini API failed, trying OpenRouter API...\u0026#39;); const openRouterResponse = await axios.post( \u0026#39;https://openrouter.ai/api/v1/chat/completions\u0026#39;, { model: \u0026#39;anthropic/claude-3-opus:beta\u0026#39;, messages: [{ role: \u0026#39;user\u0026#39;, content: messageContent }], }, { headers: { Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, } ); return openRouterResponse.data.choices[0].message.content; } } catch (error) { console.error(\u0026#39;Error generating AI response:\u0026#39;, error); return \u0026#39;Sorry, an error occurred while generating a response: \u0026#39; + error.message; } }; Product Information Display To enhance the user experience, we format product information in a visually appealing way. The AI response includes a special tag [SHOW_PRODUCT_INFO] when product details should be displayed:\n/** * Formats product information into HTML for display * @param {Array} products - Array of product objects * @returns {String} - HTML formatted product information */ const formatProductInfo = (products) =\u0026gt; { if (!products || products.length === 0) { return \u0026#39;No relevant product information found.\u0026#39;; } if (products.error) { return products.message; } let productsHTML = \u0026#39;\u0026lt;div class=\u0026#34;products-container\u0026#34;\u0026gt;\u0026#39;; products.forEach((product) =\u0026gt; { const parsedProduct = typeof product === \u0026#39;string\u0026#39; ? JSON.parse(product) : product; productsHTML += ` \u0026lt;div class=\u0026#34;product-card\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;product-image\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;${parsedProduct.thumbnail || parsedProduct.images?.[0]}\u0026#34; alt=\u0026#34;${parsedProduct.title || \u0026#39;Product Image\u0026#39;}\u0026#34; loading=\u0026#34;lazy\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;product-info\u0026#34;\u0026gt; \u0026lt;h3 class=\u0026#34;product-title\u0026#34;\u0026gt;${parsedProduct.title || \u0026#39;Name not specified\u0026#39;}\u0026lt;/h3\u0026gt; \u0026lt;p class=\u0026#34;product-description\u0026#34;\u0026gt;${parsedProduct.description || \u0026#39;Description not specified\u0026#39;}\u0026lt;/p\u0026gt; \u0026lt;div class=\u0026#34;product-price-container\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;product-price\u0026#34;\u0026gt;$${parsedProduct.price || \u0026#39;Price not specified\u0026#39;}\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;a href=\u0026#34;/product/${parsedProduct.id}\u0026#34; class=\u0026#34;product-detail-button\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; More Details \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; `; }); productsHTML += \u0026#39;\u0026lt;/div\u0026gt;\u0026#39;; return productsHTML; }; The AI can also specify which products to display by including product IDs in its response:\n// Extract product IDs from AI response const productIdsMatch = response.match(/\\[PRODUCT_IDS\\](.*?)\\[\\/PRODUCT_IDS\\]/); let productsToShow; if (productIdsMatch) { // Get product IDs specified by AI const productIds = JSON.parse(productIdsMatch[1]).ids; // Filter products to only show those with specified IDs productsToShow = products.filter((product) =\u0026gt; { const parsedProduct = typeof product === \u0026#39;string\u0026#39; ? JSON.parse(product) : product; return productIds.includes(parsedProduct.id.toString()); }); // Remove ID tags from response response = response.replace(/\\[PRODUCT_IDS\\].*?\\[\\/PRODUCT_IDS\\]/, \u0026#39;\u0026#39;); } else { // Show all products if no IDs specified productsToShow = products; } // Format product information and add to response const productInfo = productService.formatProductInfo(productsToShow); response = response + productInfo; This approach allows the AI to provide a conversational response while also displaying structured product information when appropriate.\n6. Message Categorization Understanding User Intent To provide more relevant responses, we categorize user messages based on their intent. This helps the system determine how to respond and what information to include:\n/** * Categorizes user messages into predefined categories * @param {String} message - User message to categorize * @returns {String} - Category of the message */ const categorizerAgent = async (message) =\u0026gt; { try { const response = await axios({ method: \u0026#39;post\u0026#39;, url: \u0026#39;https://openrouter.ai/api/v1/chat/completions\u0026#39;, headers: { Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, data: { model: \u0026#39;google/gemini-2.0-flash-001\u0026#39;, messages: [ { role: \u0026#39;system\u0026#39;, content: `You are a sophisticated AI agent specializing in categorizing user queries into one of the following categories: [PRODUCT_INFO] - Questions about product information, features, specifications, price, etc. [PRODUCT_COMPARISON] - Questions comparing multiple products [PRODUCT_RECOMMENDATION] - Questions asking for product recommendations [PRODUCT_AVAILABILITY] - Questions about product availability [PRODUCT_REVIEWS] - Questions about product reviews or ratings [GENERAL_INQUIRY] - General questions not related to specific products [CUSTOMER_SERVICE] - Questions about customer service, shipping, returns, etc. [OTHER] - Any other type of question Return only the category as plain text.`, }, { role: \u0026#39;user\u0026#39;, content: message, }, ], }, }); return response.data.choices[0].message.content.replace(/\\n+$/, \u0026#39;\u0026#39;); } catch (error) { console.error(\u0026#39;OpenRouter API error:\u0026#39;, error); return \u0026#39;[GENERAL_INQUIRY]\u0026#39;; } }; The category is then used to tailor the AI\u0026rsquo;s response. For example, if the category is [PRODUCT_INFO], the system will include detailed product information in the response.\nCategory-Based Response Customization Different categories of questions require different types of responses. Our system adjusts its behavior based on the detected category:\n[PRODUCT_INFO]: Includes detailed product specifications and displays product cards [PRODUCT_COMPARISON]: Highlights differences between products [PRODUCT_RECOMMENDATION]: Suggests products based on user criteria [PRODUCT_REVIEWS]: Focuses on customer feedback and ratings [GENERAL_INQUIRY]: Provides general information without specific product details This categorization helps the system provide more relevant and helpful responses to a wide range of user queries.\n7. System Architecture and Implementation Clean Architecture Design Our ecommerceRAG project follows a clean architecture pattern with clear separation of concerns:\nControllers: Handle HTTP requests and responses Services: Contain business logic and external API interactions Routes: Define API endpoints Utils: Provide utility functions Config: Handle configuration settings Middlewares: Implement middleware functions This modular design makes the codebase more maintainable, testable, and scalable.\nMain Components The main components of our system include:\nChat Controller: Processes user messages and coordinates the response generation Vector Service: Handles interactions with the Weaviate vector database AI Service: Manages AI model interactions (Gemini and OpenAI) Product Service: Provides product-related services Conversation Service: Manages conversation persistence and retrieval Agent Controller: Contains agent logic for translation, categorization, and classification Processing Flow The main processing flow in our system is handled by the processChat function:\n/** * Processes chat messages and generates AI responses * @param {Object} req - Express request object * @param {Object} res - Express response object */ const processChat = async (req, res) =\u0026gt; { try { let { message, conversationId } = req.body; if (!message) { return res.status(400).json({ error: \u0026#39;Message is required\u0026#39; }); } // Categorize the message const category = await categorizerAgent(message); // Get existing conversation or create new one let conversation; try { if (conversationId) { conversation = await conversationService.getConversationById(conversationId); } else { conversation = await conversationService.createConversation(); } } catch (error) { // Create a new conversation in case of error conversation = await conversationService.createConversation(); } // Add user message to conversation await conversationService.addMessageToConversation(conversation._id, \u0026#39;user\u0026#39;, message); // Get conversation history (last 4 messages) const conversationHistory = await conversationService.getConversationMessages(conversation._id, 4); // Translate message to English let translatedMessage = await translatorAgent(message); // Determine if message is a follow-up question let isFollowUp = false; if (conversationHistory.length \u0026gt;= 3) { // Use classifierAgent to determine if it\u0026#39;s a follow-up const response = await classifierAgent(conversationHistory); const parsedResponse = JSON.parse(response); isFollowUp = parsedResponse; } // Prepare search query let searchQuery = translatedMessage; if (isFollowUp \u0026amp;\u0026amp; conversationHistory.length \u0026gt;= 3) { // Find previous user messages (last 2 user messages) const userMessages = conversationHistory.filter((msg) =\u0026gt; msg.role === \u0026#39;user\u0026#39;).slice(-2); if (userMessages.length \u0026gt;= 2) { const previousUserMessage = userMessages[0].content; // Previous user message searchQuery = `${await translatorAgent(previousUserMessage)} ${translatedMessage}`; } } // Get product information from Weaviate const products = await vectorService.getProductInfo(searchQuery); // Process response based on product information let response; if (products \u0026amp;\u0026amp; products.length \u0026gt; 0) { // Send product information to AI in JSON format const productContext = JSON.stringify(products); // Send category and conversation history to AI response = await aiService.generateGeminiResponse(message, productContext, category, conversationHistory); // Clean [SHOW_PRODUCT_INFO] tag const cleanedResponse = response.replace(\u0026#39;[SHOW_PRODUCT_INFO]\u0026#39;, \u0026#39;\u0026#39;); await conversationService.addMessageToConversation(conversation._id, \u0026#39;assistant\u0026#39;, cleanedResponse); // Check if product information should be displayed if (response.includes(\u0026#39;[SHOW_PRODUCT_INFO]\u0026#39;)) { // Process and display product information // ... (code omitted for brevity) } } else { // If no product information found const productInfo = JSON.stringify({ message: \u0026#39;No relevant product information found.\u0026#39; }); response = await aiService.generateGeminiResponse(message, productInfo, category, conversationHistory); const cleanedResponse = response.replace(\u0026#39;[SHOW_PRODUCT_INFO]\u0026#39;, \u0026#39;\u0026#39;); await conversationService.addMessageToConversation(conversation._id, \u0026#39;assistant\u0026#39;, cleanedResponse); } // Return response res.json({ response: response, conversationId: conversation._id.toString(), timestamp: new Date().toISOString(), }); } catch (error) { console.error(\u0026#39;Chat processing error:\u0026#39;, error); res.status(500).json({ error: \u0026#39;An error occurred while processing the message: \u0026#39; + error.message }); } }; This function orchestrates the entire process, from receiving a user message to returning an AI-generated response with relevant product information.\n8. Frontend Integration Chat Widget Implementation To make our RAG system accessible to users, we implemented a chat widget that can be easily integrated into any e-commerce website. The widget is built with vanilla JavaScript and can be added to any page with a simple script tag:\n\u0026lt;script src=\u0026#34;https://your-domain.com/js/widget.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; The widget provides a clean, intuitive interface for users to interact with our AI assistant:\n// Widget initialization function initWidget() { // Create widget HTML const widgetContainer = document.createElement(\u0026#39;div\u0026#39;); widgetContainer.innerHTML = createWidgetHTML(); document.body.appendChild(widgetContainer); // Set up event listeners const chatForm = document.getElementById(\u0026#39;chachaForm\u0026#39;); const messageInput = document.getElementById(\u0026#39;chachaMessageInput\u0026#39;); const chatMessages = document.getElementById(\u0026#39;chachaMessages\u0026#39;); // Initialize conversationId as null (no longer using localStorage) let conversationId = null; // Add welcome message addMessage(\u0026#39;Hello! How can I help you with your shopping today?\u0026#39;, \u0026#39;bot\u0026#39;); // Form submission handler chatForm.addEventListener(\u0026#39;submit\u0026#39;, async (e) =\u0026gt; { e.preventDefault(); const message = messageInput.value.trim(); if (!message) return; // Add user message to chat addMessage(message, \u0026#39;user\u0026#39;); messageInput.value = \u0026#39;\u0026#39;; messageInput.style.height = \u0026#39;auto\u0026#39;; // Add loading indicator const loadingMessage = addLoadingMessage(); try { // Send message to API const response = await fetch(\u0026#39;/api/chat\u0026#39;, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, body: JSON.stringify({ message, conversationId, }), }); // Remove loading indicator loadingMessage.remove(); if (response.ok) { const data = await response.json(); // Add AI response to chat if (data.response) { addMessage(data.response, \u0026#39;bot\u0026#39;, true); } else if (data.data \u0026amp;\u0026amp; data.data.message) { addMessage(data.data.message, \u0026#39;bot\u0026#39;, true); } else if (data.message) { addMessage(data.message, \u0026#39;bot\u0026#39;, true); } else { addMessage(\u0026#39;Sorry, an error occurred. Please try again.\u0026#39;, \u0026#39;bot\u0026#39;); } // Store conversation ID for future messages if (data.conversationId) { conversationId = data.conversationId; } } else { addMessage(\u0026#39;Sorry, an error occurred. Please try again.\u0026#39;, \u0026#39;bot\u0026#39;); } } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); loadingMessage.remove(); addMessage(\u0026#39;Sorry, an error occurred. Please try again.\u0026#39;, \u0026#39;bot\u0026#39;); } }); } ### Responsive Design The chat widget is designed to be responsive and work well on both desktop and mobile devices: ```javascript // Mobile device detection const isMobile = window.matchMedia(\u0026#39;(max-width: 480px)\u0026#39;).matches; // Chat toggle functionality chatToggle.addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; { chatContainer.classList.toggle(\u0026#39;collapsed\u0026#39;); // Prevent body scroll when chat is open on mobile if (isMobile \u0026amp;\u0026amp; !chatContainer.classList.contains(\u0026#39;collapsed\u0026#39;)) { document.body.style.overflow = \u0026#39;hidden\u0026#39;; } else if (isMobile) { document.body.style.overflow = \u0026#39;\u0026#39;; } }); // Window resize handler window.addEventListener(\u0026#39;resize\u0026#39;, () =\u0026gt; { const isMobileNow = window.matchMedia(\u0026#39;(max-width: 480px)\u0026#39;).matches; // Update body scroll behavior on resize if (isMobileNow \u0026amp;\u0026amp; !chatContainer.classList.contains(\u0026#39;collapsed\u0026#39;)) { document.body.style.overflow = \u0026#39;hidden\u0026#39;; } else if (isMobileNow) { document.body.style.overflow = \u0026#39;\u0026#39;; } }); HTML Rendering and Styling The widget includes CSS styles for a clean, modern appearance:\n.chacha-widget { position: fixed; bottom: 20px; right: 20px; z-index: 1000; font-family: \u0026#39;Segoe UI\u0026#39;, Tahoma, Geneva, Verdana, sans-serif; } .chat-toggle { width: 60px; height: 60px; border-radius: 50%; background-color: #2563EB; color: white; border: none; cursor: pointer; display: flex; align-items: center; justify-content: center; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15); transition: all 0.3s ease; } .chat-container { position: absolute; bottom: 80px; right: 0; width: 350px; height: 500px; background-color: white; border-radius: 12px; box-shadow: 0 5px 25px rgba(0, 0, 0, 0.2); display: flex; flex-direction: column; transition: all 0.3s ease; overflow: hidden; } /* Additional styles omitted for brevity */ The widget dynamically renders HTML for messages and product cards, providing a rich user experience.\n9. Testing and Evaluation API Testing with Postman To ensure our system works correctly, we created a comprehensive Postman collection for testing all endpoints:\n{ \u0026#34;info\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ecommerceRAG API\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A collection for testing the ecommerceRAG API endpoints\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;https://schema.getpostman.com/json/collection/v2.1.0/collection.json\u0026#34; }, \u0026#34;item\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Chat\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Endpoints for chat functionality\u0026#34;, \u0026#34;item\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Process Chat Message\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;header\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;Content-Type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;application/json\u0026#34; } ], \u0026#34;body\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;raw\u0026#34;: \u0026#34;{\\n \\\u0026#34;message\\\u0026#34;: \\\u0026#34;What laptops do you have?\\\u0026#34;,\\n \\\u0026#34;conversationId\\\u0026#34;: null\\n}\u0026#34; }, \u0026#34;url\u0026#34;: { \u0026#34;raw\u0026#34;: \u0026#34;{{baseUrl}}/api/chat\u0026#34;, \u0026#34;host\u0026#34;: [ \u0026#34;{{baseUrl}}\u0026#34; ], \u0026#34;path\u0026#34;: [ \u0026#34;api\u0026#34;, \u0026#34;chat\u0026#34; ] }, \u0026#34;description\u0026#34;: \u0026#34;Process a chat message and get a response from the AI\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;Process Follow-up Message\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;header\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;Content-Type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;application/json\u0026#34; } ], \u0026#34;body\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;raw\u0026#34;, \u0026#34;raw\u0026#34;: \u0026#34;{\\n \\\u0026#34;message\\\u0026#34;: \\\u0026#34;What is the price range?\\\u0026#34;,\\n \\\u0026#34;conversationId\\\u0026#34;: \\\u0026#34;{{conversationId}}\\\u0026#34;\\n}\u0026#34; }, \u0026#34;url\u0026#34;: { \u0026#34;raw\u0026#34;: \u0026#34;{{baseUrl}}/api/chat\u0026#34;, \u0026#34;host\u0026#34;: [ \u0026#34;{{baseUrl}}\u0026#34; ], \u0026#34;path\u0026#34;: [ \u0026#34;api\u0026#34;, \u0026#34;chat\u0026#34; ] }, \u0026#34;description\u0026#34;: \u0026#34;Process a follow-up message in an existing conversation\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Conversations\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Endpoints for managing conversations\u0026#34;, \u0026#34;item\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Get All Conversations\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;url\u0026#34;: { \u0026#34;raw\u0026#34;: \u0026#34;{{baseUrl}}/api/conversations\u0026#34;, \u0026#34;host\u0026#34;: [ \u0026#34;{{baseUrl}}\u0026#34; ], \u0026#34;path\u0026#34;: [ \u0026#34;api\u0026#34;, \u0026#34;conversations\u0026#34; ] }, \u0026#34;description\u0026#34;: \u0026#34;Get a list of all conversations\u0026#34; } }, // Additional endpoints omitted for brevity ] }, { \u0026#34;name\u0026#34;: \u0026#34;Products\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Endpoints for product information\u0026#34;, \u0026#34;item\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Get Product Detail\u0026#34;, \u0026#34;request\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;url\u0026#34;: { \u0026#34;raw\u0026#34;: \u0026#34;{{baseUrl}}/product/{{productId}}\u0026#34;, \u0026#34;host\u0026#34;: [ \u0026#34;{{baseUrl}}\u0026#34; ], \u0026#34;path\u0026#34;: [ \u0026#34;product\u0026#34;, \u0026#34;{{productId}}\u0026#34; ] }, \u0026#34;description\u0026#34;: \u0026#34;Get detailed information about a specific product\u0026#34; } } ] } ] } Performance Evaluation We evaluated our system on several key metrics:\nResponse Time: The average time to generate a response Relevance: How well the retrieved products match the user query Conversation Coherence: How well the system maintains context in follow-up questions Language Support: Accuracy of responses in different languages Our testing showed that the system performs well across these metrics, with particularly strong results in maintaining conversation context and providing relevant product information.\n10. Future Improvements While our current implementation provides a solid foundation for an e-commerce RAG system, there are several areas for future improvement:\nPersonalization Future versions could incorporate user preferences and purchase history to provide more personalized recommendations. This would involve:\nTracking user interactions and preferences Incorporating user profile information in the RAG process Adjusting product rankings based on user history Enhanced Product Comparison We could improve the system\u0026rsquo;s ability to compare products by:\nImplementing structured product attribute extraction Creating specialized prompts for comparison queries Developing visual comparison displays Voice Interface Adding voice input and output would make the system more accessible and convenient, especially for mobile users:\nImplementing speech-to-text for user queries Adding text-to-speech for AI responses Supporting voice-specific interaction patterns Analytics and Feedback Loop Implementing analytics would help improve the system over time:\nTracking successful vs. unsuccessful interactions Analyzing common user queries and pain points Using feedback to fine-tune the RAG process Conclusion In this article, we\u0026rsquo;ve explored how Retrieval-Augmented Generation (RAG) can transform e-commerce applications by providing more intelligent, context-aware product search and recommendations. We\u0026rsquo;ve implemented a complete RAG system that combines vector databases, semantic search, conversation memory, and multi-language support to create a powerful shopping assistant.\nThe key components of our system include:\nVector Database: Storing product embeddings for semantic search Conversation Management: Maintaining context for follow-up questions Multi-language Support: Allowing users to interact in their preferred language AI Response Generation: Creating helpful, contextual responses Message Categorization: Understanding user intent Clean Architecture: Ensuring maintainability and scalability By combining these components, we\u0026rsquo;ve created a system that can understand complex product queries, maintain conversation context, and provide accurate, helpful responses across multiple languages.\nYou can access the complete code for this project at GitHub repository and use it as a starting point for your own RAG-based applications.\nBu blog yazısı, ecommerceRAG projenizi detaylı bir şekilde anlatıyor ve RAG teknolojisinin e-ticaret uygulamalarında nasıl kullanılabileceğini kapsamlı bir şekilde açıklıyor. Yazı, sizin blog yazım tarzınızı takip ederek: 1. Teorik bilgileri açık ve anlaşılır bir şekilde sunuyor 2. Kod örnekleriyle pratik uygulamaları gösteriyor 3. Projenin mimari yapısını ve modüler tasarımını açıklıyor 4. Vektör veritabanları, semantik arama ve konuşma bağlamı gibi önemli konuları detaylandırıyor 5. Çok dilli destek ve otomatik çeviri özelliklerini vurguluyor 6. Gelecekteki geliştirme alanlarını belirtiyor Markdown formatında hazırlanmış bu yazıyı blog sitenize ekleyebilir ve gerekirse içeriği kendi ihtiyaçlarınıza göre düzenleyebilirsiniz. ",
    "tags": ["rag","vectordb","nodejs","ai"],
    "categories": ["AI"],
    "lang": "en"
} 