[{
        "title": "RAG: Building Next-Generation AI Systems with Retrieval Augmented Generation",
        "permalink": "/blog/en/rag-building-next-gen-ai-systems/",
        "summary": "This article explores the fundamentals of Retrieval Augmented Generation (RAG), how it enhances LLM capabilities through external knowledge retrieval, and its practical applications across industries. We\u0026rsquo;ll examine RAG architecture, implementation strategies, and best practices for creating more accurate, reliable, and context-aware AI systems.",
        "content": "RAG: Building Next-Generation AI Systems with Retrieval Augmented Generation Introduction Large Language Models (LLMs) have revolutionized how machines understand and generate human language. However, despite their impressive capabilities, these models have inherent limitations: they only know what they\u0026rsquo;ve been trained on, information may be outdated, and they occasionally \u0026ldquo;hallucinate\u0026rdquo; facts. Retrieval Augmented Generation (RAG) has emerged as a transformative approach to address these challenges by combining the generative power of LLMs with the precision of information retrieval systems.\nIn this article, we\u0026rsquo;ll explore:\nThe fundamental principles and architecture of RAG systems How RAG enhances LLM capabilities and overcomes their limitations Key components of effective RAG implementations Advanced techniques for optimizing RAG performance Real-world applications across different industries Best practices and future directions for RAG technology 1. Understanding RAG: Principles and Architecture What is Retrieval Augmented Generation? Retrieval Augmented Generation (RAG) is an AI framework that enhances language models by incorporating external knowledge sources. Instead of relying solely on information encoded in the model\u0026rsquo;s parameters, RAG retrieves relevant documents or data from an external knowledge base and uses this information to generate more accurate, current, and context-aware responses.\n[GÖRSEL: Diagram showing basic RAG architecture with user query flow through retriever and generator components]\nThe core architecture of a RAG system consists of two primary components:\nRetriever: Responsible for finding and retrieving relevant information from the knowledge base Generator: The language model that uses both the retrieved information and its own parametric knowledge to generate the final response This hybrid approach combines the strengths of both systems:\nThe retriever provides factual precision, up-to-date information, and domain-specific knowledge The generator contributes language understanding, reasoning capabilities, and natural language generation The Evolution of RAG RAG has evolved significantly since its introduction:\nEarly Information Retrieval: Traditional search engines relied on keyword matching, which often missed semantic meaning Neural Information Retrieval: Introduced neural networks to better understand semantic relationships Dense Passage Retrieval: Improved embedding-based approaches for more accurate document retrieval Modern RAG Systems: Combine sophisticated retrieval mechanisms with powerful generative models Each evolutionary step has enhanced the system\u0026rsquo;s ability to find and incorporate relevant knowledge into the generation process.\n2. Key Components of Effective RAG Systems Knowledge Base Construction The foundation of any RAG system is its knowledge base—the external repository of information that the model can access. Creating an effective knowledge base involves several critical considerations:\nData Selection and Curation:\nChoose authoritative, accurate sources Ensure data diversity and comprehensive coverage Maintain information quality and consistency Document Processing:\nText extraction from various formats (PDF, HTML, etc.) Cleaning and normalization Metadata enrichment Chunking Strategies:\nDocument segmentation into appropriate-sized chunks Semantic vs. fixed-size chunking Chunk overlap considerations The quality, coverage, and organization of the knowledge base significantly impact the overall performance of a RAG system.\nEmbedding and Vector Representations At the heart of modern RAG systems is the conversion of text into numerical vector representations (embeddings) that capture semantic meaning.\nEmbedding Generation:\nText is transformed into high-dimensional vectors (typically 768-4096 dimensions) Similar meanings are positioned close to each other in vector space Different embedding models prioritize different semantic aspects Properties of Good Embeddings:\nSemantic similarity is reflected in vector proximity Robust to paraphrasing and linguistic variations Dimension-efficient representation of meaning Embedding Model Selection:\nGeneral-purpose vs. domain-specialized embeddings Size vs. performance tradeoffs Cross-lingual capabilities Vector Database Technologies Vector databases are specialized storage systems designed for efficient similarity search across high-dimensional vectors. They serve as the retrieval engine in RAG systems.\nKey Vector Database Features:\nApproximate Nearest Neighbor (ANN) algorithms for fast retrieval Indexing techniques that balance accuracy and speed Filtering capabilities based on metadata Scalability to billions of vectors Popular Vector Database Options:\nPinecone: Fully-managed, scalable vector search Weaviate: Open-source, semantic search engine Milvus: High-performance, distributed architecture Qdrant: Rust-based, lightweight, and flexible ChromaDB: Python-focused, easy to get started The choice of vector database significantly impacts retrieval speed, accuracy, and the overall system\u0026rsquo;s scalability.\n3. The RAG Pipeline: From Query to Response Query Processing and Understanding The RAG process begins with the user\u0026rsquo;s input query. Effective query processing involves:\nQuery Analysis:\nIntent recognition Entity extraction Constraint identification Query Transformation:\nQuery expansion (adding related terms) Query refinement (focusing on key elements) Rewriting for retrieval optimization Query Embedding:\nConverting the query to the same vector space as the knowledge base Applying the same embedding model used for documents Preserving the semantic intent in the vector representation Multi-stage Retrieval Strategies Modern RAG systems often employ sophisticated multi-stage retrieval pipelines:\nInitial Broad Retrieval:\nSemantic search using embedding similarity High recall, moderate precision Retrieves candidate documents from the vector database Re-ranking and Refinement:\nMore computationally intensive scoring of initial results Cross-attention between query and documents Re-ordering based on relevance scores Hybrid Retrieval Approaches:\nCombining dense (embedding-based) and sparse (keyword-based) retrieval Ensemble methods across multiple retrievers Domain-specific retrieval strategies [GÖRSEL: ] Context Integration and Response Generation The final stage in the RAG pipeline is generating a response using both the retrieved information and the LLM\u0026rsquo;s capabilities:\nContext Window Construction:\nSelecting the most relevant retrieved documents Ordering and structuring the context Managing token limits in the LLM prompt Prompt Engineering:\nCrafting effective instructions for the LLM Specifying how to use the retrieved information Setting the tone, format, and constraints Response Generation:\nLLM processes the query and retrieved context Synthesis of information into a coherent response Attribution to source documents when appropriate 4. Advanced RAG Techniques and Optimizations Recursive Retrieval and Retrieval Augmented Retrieval Standard RAG performs a single retrieval operation, but advanced implementations use multiple retrieval rounds:\nRecursive RAG:\nInitial response generation Analysis of information gaps Secondary targeted retrievals to fill those gaps Final comprehensive response generation Retrieval Augmented Retrieval:\nUsing the LLM to help refine the retrieval process Generating better search queries based on initial results Iterative improvement of retrieval quality Query Decomposition for Complex Questions Complex queries often encompass multiple sub-questions or aspects. Query decomposition addresses this by:\nBreaking down complex queries into simpler sub-queries Performing separate retrievals for each sub-query Integrating the retrieved information for a comprehensive response This approach significantly improves performance on multi-hop reasoning tasks and complex analytical questions.\nHypothetical Document Embeddings (HyDE) HyDE is an innovative technique that enhances retrieval quality:\nThe LLM first generates a hypothetical perfect document that would answer the query This hypothetical document is embedded and used for retrieval instead of the original query The retrieval results are often more relevant than those from direct query embedding This technique leverages the LLM\u0026rsquo;s reasoning abilities to improve the retrieval process itself.\nKnowledge Caching and Retrieval Memory Efficient RAG systems can benefit from various caching mechanisms:\nSession-Based Caching:\nRetaining retrieved documents within a conversation Building a working memory of relevant information Reducing redundant retrievals Cross-Session Knowledge Distillation:\nIdentifying frequently retrieved information Creating condensed knowledge summaries Prioritizing high-value information in the retrieval process 5. Evaluating RAG Systems Key Performance Metrics Measuring RAG performance requires a multifaceted approach:\nRetrieval Metrics:\nPrecision: Accuracy of retrieved documents Recall: Comprehensiveness of retrieved information Mean Reciprocal Rank (MRR): Position of first relevant document Normalized Discounted Cumulative Gain (nDCG): Ranking quality Generation Metrics:\nFactual accuracy Hallucination rate Answer relevance Response completeness End-to-End Metrics:\nUser satisfaction Query resolution rate Time-to-answer System latency [GÖRSEL: ] Human-in-the-Loop Evaluation Automated metrics provide valuable signals but should be complemented with human evaluation:\nExpert Review:\nDomain specialists assessing factual correctness Identifying subtle errors or misconceptions Evaluating nuanced aspects of responses User Feedback:\nDirect ratings from end-users Implicit signals (follow-up questions, refinements) A/B testing of different RAG configurations Continuous Improvement:\nIdentifying patterns in errors or weaknesses Targeted enhancement of knowledge gaps Iterative refinement of retrieval strategies 6. Real-World RAG Applications Enterprise Knowledge Management RAG systems are transforming how organizations access and leverage their institutional knowledge:\nInternal Documentation Access:\nConnecting employees with relevant policies, procedures, and documentation Reducing search time and improving information discovery Maintaining organizational knowledge continuity Customer Support Enhancement:\nProviding support agents with contextually relevant information Ensuring consistent and accurate responses Reducing resolution time for complex inquiries Compliance and Governance:\nEnsuring responses adhere to regulatory requirements Maintaining audit trails of information sources Reducing risk through accurate information retrieval Scientific Research and Healthcare The ability to retrieve and synthesize specialized knowledge makes RAG valuable in research and healthcare:\nLiterature Review Assistance:\nRetrieving relevant research papers and findings Summarizing state-of-the-art knowledge Identifying connections across diverse studies Clinical Decision Support:\nProviding clinicians with relevant medical literature Retrieving case studies and treatment guidelines Supporting evidence-based medicine with the latest research Drug Discovery and Development:\nAccessing information across pharmaceutical databases Synthesizing knowledge from diverse scientific domains Accelerating research through improved information access Education and Learning RAG is transforming educational experiences through personalized knowledge access:\nIntelligent Tutoring Systems:\nRetrieving explanations tailored to student questions Providing diverse learning resources on demand Adapting explanations to different learning styles Curriculum Development:\nAccessing and synthesizing educational standards Retrieving relevant teaching materials Creating comprehensive learning pathways 7. RAG Implementation Challenges and Solutions Retrieval Quality Optimization Retrieval errors represent one of the most significant challenges in RAG systems:\nCommon Retrieval Issues:\nSemantic mismatch between query and relevant documents Over-reliance on lexical overlap Difficulty with implicit knowledge needs Solutions:\nAdvanced query rewriting techniques Ensemble retrieval approaches Domain-specific embedding fine-tuning Continuous retrieval feedback loops Hallucination Mitigation Even with retrieval augmentation, LLMs can still produce inaccurate information:\nTypes of Hallucinations:\nIntrinsic: Originating from the model\u0026rsquo;s parametric knowledge Extrinsic: Misinterpreting or misrepresenting retrieved information Mitigation Strategies:\nExplicit attribution requirements Confidence scoring and uncertainty indication Retrieval verification loops Fact-checking against retrieved information System Latency and Performance RAG systems introduce additional computational steps that can impact response time:\nPerformance Bottlenecks:\nEmbedding generation time Vector search latency Context processing in large LLMs Optimization Approaches:\nEmbedding caching and precomputation Vector database query optimization Tiered retrieval architectures Asynchronous prefetching strategies 8. Future Directions in RAG Multimodal RAG Systems The evolution of RAG is moving beyond text to incorporate diverse data types:\nImage-Text Retrieval:\nFinding relevant images based on textual queries Retrieving text information based on image content Multimodal knowledge bases with mixed content types Audio and Video Integration:\nRetrieving information from speech, music, or sound effects Accessing knowledge from video content Cross-modal retrieval across diverse media types Self-Improving RAG Future RAG systems will continuously enhance their own performance:\nAutomatic Knowledge Base Refinement:\nIdentifying knowledge gaps from user interactions Prioritizing areas for knowledge expansion Automatically curating and updating information Retrieval Strategy Optimization:\nLearning optimal retrieval parameters from user feedback Adapting to different query types and domains Self-tuning for improved performance over time Agent-Based RAG Architectures RAG is evolving from a passive information system to an active reasoning framework:\nTool-Augmented RAG:\nCombining retrieval with external tool usage Dynamic information gathering based on initial retrievals Closed-loop verification with external systems Multi-Agent RAG Systems:\nSpecialized retrievers for different knowledge domains Collaborative information synthesis across agents Hierarchical decision-making for complex queries Conclusion Retrieval Augmented Generation represents a fundamental shift in how we build AI systems that interact with information. By combining the creative and reasoning capabilities of large language models with the precision and timeliness of information retrieval, RAG creates AI systems that are more accurate, trustworthy, and useful.\nThe key advantages of RAG include:\nEnhanced factual accuracy through external knowledge grounding Ability to access up-to-date information beyond the model\u0026rsquo;s training cutoff Domain adaptation without full model fine-tuning Transparent sourcing of information Reduced hallucination rates As RAG technologies continue to evolve, we can expect these systems to become even more sophisticated—incorporating multimodal data, engaging in more complex reasoning chains, and continuously improving their own performance. The future of AI lies not just in bigger models, but in smarter architectures that effectively combine neural and symbolic approaches to knowledge.\nOrganizations implementing RAG today are already seeing significant benefits in knowledge management, customer support, research, and many other domains. As these techniques become more accessible and refined, they will form the foundation of a new generation of AI systems that are not just impressive in their language capabilities, but reliable in their knowledge.\n",
        "tags": ["rag","llm","vectordb","embeddings"],
        "categories": ["AI"],
        "lang": "en"
    },{
        "title": "Modern AI Technologies and Node.js Integration",
        "permalink": "/blog/en/modern-ai-technologies-nodejs-integrations/",
        "summary": "In this article, we will explore the applications of artificial intelligence technologies in the JavaScript ecosystem. We will discuss how fundamental concepts such as AI agents, tool calling, conversation memory, and context management can be implemented in Node.js applications through example projects.",
        "content": "Modern AI Technologies and Node.js Integration Introduction Artificial intelligence technologies, especially language models, are rapidly transforming the software development world. Integrating AI technologies with a popular platform like Node.js offers incredible opportunities for developers working in the JavaScript ecosystem. In this article, we\u0026rsquo;ll explore how to implement modern AI concepts within the Node.js framework.\nThis article will help you:\nUnderstand AI terminology and fundamental concepts Integrate LLM (Large Language Model) with Node.js Understand creating AI agents and tool calling capabilities Use open-source AI technologies (Ollama, LiteLLM, LlamaCPP) in Node.js applications AI integration with N8n for automated workflows You can access the complete code for the projects at GitHub repository.\n1. AI Agents: Developing Intelligent Assistants What is an AI Agent? AI agents are software systems designed to perform specific tasks using LLMs (Large Language Models). These agents possess human-like thinking, decision-making, and interaction capabilities. An AI agent is an autonomous software component designed to perform specific tasks. These agents:\nReceive input: For example, queries from users or data from systems. Process: Analyze this data, make decisions, or run models. Provide output: Execute results or actions. Modern AI agents typically include these components:\nLLM (Brain): Natural language understanding and generation capability Memory: Ability to remember past interactions Tools: Ability to interact with APIs, databases, and other systems Planning: Ability to break complex tasks into subtasks AI Agents can be reactive, meaning they don\u0026rsquo;t collect data, analyze environment, etc. without any user trigger, or they can be autonomous, which is more common. An autonomous agent, for example in a smart home system, can continuously monitor the environment, detect specific conditions, and make its own decisions. Such an agent continuously collects data from sources like sensors, analyzes this data, and independently initiates or adjusts certain tasks based on this analysis.\nCreating an AI Agent with Node.js Let\u0026rsquo;s create a simple autonomous agent example using Node.js and OpenAI API. In this example, the agent generates \u0026ldquo;sensor data\u0026rdquo; at specific intervals and acts according to the decision it receives from OpenAI. First, let\u0026rsquo;s install the required package.\nnpm install openai Then our code is as follows:\nconst { OpenAI } = require(\u0026#39;openai\u0026#39;); // OpenAI configuration: Add your API key here const openai = new OpenAI({ apiKey: \u0026#39;your-api-key\u0026#39;, }); // Autonomous agent function async function autonomousAgent() { console.log(\u0026#39;Agent started working...\u0026#39;); // Step 1: Simulate sensor data (e.g., a random number) const sensorData = Math.random() * 100; console.log(\u0026#39;Sensor data:\u0026#39;, sensorData); // Step 2: Send prompt to OpenAI for decision based on sensor data const prompt = `Sensor data: ${sensorData}. Return a single word \u0026#34;Alarm\u0026#34; if the value is greater than 50, or \u0026#34;Normal\u0026#34; if it\u0026#39;s 50 or less.`; try { const response = await openai.chat.completions.create({ model: \u0026#39;gpt-4o-mini\u0026#39;, messages: [{ role: \u0026#39;user\u0026#39;, content: prompt }], max_tokens: 10, temperature: 0.3, }); const decision = response.choices[0].message.content.trim(); console.log(\u0026#39;Agent Decision:\u0026#39;, decision); // Step 3: Take action based on OpenAI\u0026#39;s decision if (decision === \u0026#39;Alarm\u0026#39;) { console.log(\u0026#39;Alarm condition detected, initiating necessary actions...\u0026#39;); // Here you can add actions like triggering alarms or sending notifications } else if (decision === \u0026#39;Normal\u0026#39;) { console.log(\u0026#39;Situation normal, in standby mode...\u0026#39;); // Additional actions for normal state can be added here } else { console.log(\u0026#39;Unknown decision, rechecking...\u0026#39;); // Error handling for unexpected situations } } catch (error) { console.error(\u0026#39;Error occurred:\u0026#39;, error); } } // Run the agent automatically every 30 seconds setInterval(autonomousAgent, 10000); // Run the program once immediately when it starts autonomousAgent(); Let\u0026rsquo;s explain this code in more detail:\nSensor Data Generation: The agent generates a random number each time it runs. This number is considered as sample sensor data. Decision Process: The generated sensor data is sent to OpenAI API within a prompt. This prompt asks to return either \u0026ldquo;Alarm\u0026rdquo; or \u0026ldquo;Normal\u0026rdquo; based on the data. Action: The response from OpenAI determines the agent\u0026rsquo;s decision. If the response is \u0026ldquo;Alarm\u0026rdquo;, the agent performs actions that trigger the alarm state; if \u0026ldquo;Normal\u0026rdquo;, it remains in standby mode. Autonomy: The agent works independently at regular intervals (here every 30 seconds) without user intervention, evaluating sensor data and making decisions automatically. This example demonstrates the concept of an autonomous agent as a structure that is not dependent solely on user input, self-triggering, and making decisions based on specific conditions. Thus, the system can take action based on environmental data while working independently in a specific time cycle.\nAI Agent Working Principle AI agents typically follow these steps:\nThinking: Creating a plan for the requested task Tool Selection: Deciding which tool to use to complete the task Action: Performing a specific action using the selected tool Observation: Examining the result of the action Progress: Starting a new thinking-action cycle based on the result This approach is known as ReAct (Reasoning and Acting) and enables LLM to combine thinking with action.\nWhy is This Feature Important? You can delegate automatable operations to intelligent assistants You can provide a more natural interface to human users You can simplify complex workflows with modular tools 2. Embedding, RAG, VectorDB Embedding Embedding is the process of converting text or other data types into numerical vectors. These vectors represent the semantic features of the data in numerical format. For example, the words \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;cat\u0026rdquo; are represented by vectors close to each other, while \u0026ldquo;car\u0026rdquo; is represented by a more distant vector.\nEmbeddings are used for:\nCalculating text similarity Document classification Semantic search Creating recommendations Sentiment analysis Text summarization Natural language processing tasks The working principle of embeddings is as follows:\nTokenization: Text is first divided into smaller pieces (tokens) Vector Transformation: Each token is converted into a vector of hundreds or thousands of dimensions Normalization: Vectors are normalized to make them comparable The most important feature of embeddings is that content with similar meanings is positioned close to each other in vector space. This allows for meaning-based results in text-based searches rather than word matching. For example, the words \u0026ldquo;happy\u0026rdquo; and \u0026ldquo;joyful\u0026rdquo; are positioned close to each other in vector space, while the word \u0026ldquo;sad\u0026rdquo; is located at a distant point.\nRAG (Retrieval Augmented Generation) RAG is a technique that enriches the existing knowledge base of large language models (LLM) with external sources. The RAG system works as follows:\nRetrieval:\nDocuments or information related to the user question are retrieved from the database Most relevant content is found using semantic search Documents are ranked by embedding similarity Augmentation:\nFound information is added to the prompt sent to LLM Information is arranged by order of importance Context window is optimized Generation:\nLLM generates response using enriched context Response can reference given sources Reliability score can be calculated Advantages of RAG:\nUse of current information Increased accuracy Customized responses Reduced hallucinations Traceability of sources Dynamic information update capability Domain-specific knowledge integration Vector DB Vector databases are specialized database systems for storing embedding vectors and performing similarity searches on these vectors. Unlike traditional databases, they can quickly calculate similarity between vectors.\nVector DB Core Features:\nEfficient storage of high-dimensional vectors Similarity-based search (cosine similarity, euclidean distance) Fast nearest neighbor queries Scalability CRUD operations Metadata filtering Batch processing support Vector indexing Vector DBs are particularly used in these areas:\nSemantic document search Recommendation systems Image and audio similarity analysis Natural language processing applications Face recognition systems Anomaly detection Cross-modal search (text-to-image, image-to-text) Popular Vector DB solutions:\nPinecone: Fully managed, scalable solution Weaviate: Open source, self-hosted option Milvus: High-performance, distributed architecture Qdrant: Rust-based, fast and lightweight ChromaDB: Python-focused, ideal for getting started 3. Tool Calling: Adding New Capabilities to AI What is Tool Calling? Tool calling is the ability of an AI model to call external functions or APIs. This allows AI to perform operations beyond its knowledge boundaries.\nModern LLMs have the ability to make tool calls in JSON format. This allows the model to define the action it wants to perform and the necessary parameters.\nTool Calling with Node.js Let\u0026rsquo;s implement tool calling using the OpenAI API. In the example below, we\u0026rsquo;re creating an agent that demonstrates the concept of \u0026ldquo;tool calling\u0026rdquo; using Node.js. This agent calls two different \u0026ldquo;tools\u0026rdquo;:\nWeather API (OpenWeatherMap): The agent retrieves current weather data for a specific location. OpenAI API: Using the weather data it receives, it requests a brief analysis of how the day is going. By using these two tools sequentially, we can see how the agent collects data from external sources and processes this data.\nFirst, let\u0026rsquo;s install the required libraries\nnpm install axios openai Örnek kodumuz ise aşağıdaki şekilde.\nconst axios = require(\u0026#39;axios\u0026#39;); const { OpenAI } = require(\u0026#39;openai\u0026#39;); // Enter your API keys const WEATHER_API_KEY = \u0026#39;\u0026lt;your-api-key\u0026gt;\u0026#39;; // Your OpenWeatherMap API key // Location to query weather for const LOCATION = \u0026#39;Istanbul\u0026#39;; // OpenAI configuration const openai = new OpenAI({ apiKey: \u0026#39;\u0026lt;your-api-key\u0026gt;\u0026#39;, }); // Step 1: Function to call Weather API async function getWeatherData(location) { try { const response = await axios.get( `https://api.openweathermap.org/data/2.5/weather?q=${location}\u0026amp;appid=${WEATHER_API_KEY}\u0026amp;units=metric` ); return response.data; } catch (error) { console.error(\u0026#39;Weather API error:\u0026#39;, error); return null; } } // Step 2: Function to send data to OpenAI API and get analysis async function analyzeWeather(weatherData) { if (!weatherData) return \u0026#39;Could not retrieve weather data.\u0026#39;; // Create prompt based on weather data const prompt = `Currently in ${weatherData.name}, the weather is ${weatherData.weather[0].description} and temperature is ${weatherData.main.temp}°C. Based on this weather, how is the day going and what kind of activity would you recommend? Give a brief summary.`; try { const response = await openai.chat.completions.create({ model: \u0026#39;gpt-4o-mini\u0026#39;, messages: [{ role: \u0026#39;user\u0026#39;, content: prompt }], max_tokens: 60, temperature: 0.7, }); return response.choices[0].message.content.trim(); } catch (error) { console.error(\u0026#39;OpenAI API error:\u0026#39;, error); return \u0026#39;Weather analysis failed.\u0026#39;; } } // Main function: Calls tools in sequence async function main() { console.log(\u0026#39;Starting Tool Calling Agent...\u0026#39;); // Call tool to get weather data const weatherData = await getWeatherData(LOCATION); console.log(\u0026#39;Received Weather Data:\u0026#39;, weatherData); // Send data to OpenAI API and get analysis const analysis = await analyzeWeather(weatherData); console.log(\u0026#39;Analysis Result:\u0026#39;, analysis); } main(); Bu örnekte, agent \u0026ldquo;tool calling\u0026rdquo; yaparak iki farklı dış kaynağı (OpenWeatherMap ve OpenAI) kullanıyor. Öncelikle hava durumu verisi toplanıyor; ardından bu veri, bir analiz için OpenAI API\u0026rsquo;sine gönderiliyor. Böylece, ajanın kendi yeteneklerinin ötesinde dış kaynaklardan faydalanması sağlanıyor.\nFarklı Araç Türleri AI sistemlerinize entegre edebileceğiniz bazı araç türleri:\nVeritabanı İşlemleri: Veri sorgulama, ekleme, güncelleme API Çağrıları: Harici servislere bağlanma (hava durumu, borsa, haberler) Dosya İşlemleri: Dosya okuma, yazma, dönüştürme Hesaplamalar: Karmaşık matematiksel işlemler Takvim/Zamanlama: Etkinlik oluşturma, hatırlatıcı ayarlama Tool Calling Best Practices Açık Tanımlamalar: Araçlarınızı açık ve net tanımlayın, parametreleri dokumentasyon içerisinde detaylandırın Güvenlik Kontrolleri: Kullanıcı girdilerini doğrulayın Graceful Failure: Araçlar hata verdiğinde zarif bir şekilde geri dönüş yapın Aşamalı Geliştirme: Önce basit araçlarla başlayın, sonra karmaşıklığı artırın 4. Conversation Memory: Adding Memory to AI What is Conversation Memory? Conversation memory is the ability of an AI system to remember previous interactions and respond accordingly. This feature ensures that the conversation is consistent and contextual.\nWhy is it Important? Provides contextual consistency. Users don\u0026rsquo;t need to provide context every time. That is, during dialogue, the agent \u0026lsquo;remembers\u0026rsquo; previous messages. Thus, information given at the beginning of a conversation ensures that subsequent responses are more meaningful and consistent. AI can reference previous requests Provides more natural and human-like interactions Here\u0026rsquo;s an example usage of conversation memory. A chatbot keeps the user\u0026rsquo;s name, areas of interest, or previously asked questions in its memory and references this information in subsequent responses. Thus, each message becomes connected and the conversation gains a natural flow. This memory is usually implemented by storing a certain portion of previous messages. Language models use this historical information as input to produce responses with a better understanding of context. Effective use of conversation memory brings challenges such as memory size and maintaining information currency in long dialogues. As a result, conversation memory is a critical component for AI Agents to produce more \u0026lsquo;intelligent\u0026rsquo; and contextual responses. This feature allows for long and meaningful conversations.\nConversation Memory with Node.js Below is an example of a chatbot that maintains conversation memory. In this example, each user\u0026rsquo;s chat history is stored in MongoDB and we send the history as a prompt to OpenAI when a new message arrives.\nFirst, let\u0026rsquo;s install the required libraries:\nnpm install mongodb express openai Here\u0026rsquo;s our example code:\nconst express = require(\u0026#39;express\u0026#39;); const { MongoClient } = require(\u0026#39;mongodb\u0026#39;); const { OpenAI } = require(\u0026#39;openai\u0026#39;); const app = express(); app.use(express.json()); const OPENAI_API_KEY = \u0026#39;\u0026lt;your-api-key\u0026gt;\u0026#39;; const MONGO_URI = \u0026#39;mongodb://localhost:27017\u0026#39;; const DATABASE_NAME = \u0026#39;conversationDB\u0026#39;; const COLLECTION_NAME = \u0026#39;conversations\u0026#39;; // OpenAI configuration const openai = new OpenAI({ apiKey: OPENAI_API_KEY, }); // MongoDB connection const client = new MongoClient(MONGO_URI, { useUnifiedTopology: true }); let conversationCollection; client .connect() .then(() =\u0026gt; { const db = client.db(DATABASE_NAME); conversationCollection = db.collection(COLLECTION_NAME); console.log(\u0026#39;Connected to MongoDB.\u0026#39;); }) .catch((err) =\u0026gt; console.error(\u0026#39;MongoDB connection error:\u0026#39;, err)); // Chat endpoint app.post(\u0026#39;/chat\u0026#39;, async (req, res) =\u0026gt; { const { userId, message } = req.body; if (!userId || !message) { return res.status(400).send({ error: \u0026#39;userId and message are required.\u0026#39; }); } try { // Get user\u0026#39;s previous chat history let conversation = await conversationCollection.findOne({ userId }); if (!conversation) { conversation = { userId, messages: [] }; } // Add user message to history conversation.messages.push({ role: \u0026#39;user\u0026#39;, content: message }); // Convert conversation history to prompt let prompt = \u0026#39;\u0026#39;; conversation.messages.forEach((msg) =\u0026gt; { prompt += (msg.role === \u0026#39;user\u0026#39; ? \u0026#39;User\u0026#39; : \u0026#39;Agent\u0026#39;) + `: ${msg.content}\\n`; }); prompt += \u0026#39;Agent:\u0026#39;; // For agent\u0026#39;s response // OpenAI API call const response = await openai.chat.completions.create({ model: \u0026#39;gpt-4o-mini\u0026#39;, messages: [{ role: \u0026#39;user\u0026#39;, content: prompt }], max_tokens: 100, temperature: 0.7, }); const agentReply = response.choices[0].message.content.trim(); // Add agent response to history conversation.messages.push({ role: \u0026#39;agent\u0026#39;, content: agentReply }); // Save updated conversation to MongoDB (upsert) await conversationCollection.updateOne({ userId }, { $set: { messages: conversation.messages } }, { upsert: true }); res.send({ reply: agentReply }); } catch (error) { console.error(\u0026#39;Error processing chat:\u0026#39;, error); res.status(500).send({ error: \u0026#39;Server error\u0026#39; }); } }); app.listen(3000, () =\u0026gt; { console.log(\u0026#39;Server running on port 3000.\u0026#39;); }); Example cURL request:\ncurl --location \u0026#39;http://localhost:3000/chat\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;userId\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;what\u0026#39;s the temperature in Istanbul\u0026#34; }\u0026#39; Here\u0026rsquo;s how an example scenario might develop: User =\u0026gt; What\u0026rsquo;s the temperature in Istanbul today? Agent =\u0026gt; It\u0026rsquo;s 23 degrees and sunny in Istanbul User =\u0026gt; What about Izmir? Agent =\u0026gt; It\u0026rsquo;s 26 degrees and sunny in Izmir.\nThe agent maintains history here, so in the second question, even though words like \u0026rsquo;temperature\u0026rsquo; or \u0026lsquo;weather\u0026rsquo; aren\u0026rsquo;t mentioned, just from the \u0026ldquo;what about Izmir?\u0026rdquo; query, it knows to provide weather information because it maintains the conversation history.\n5. Model Context Protocol: Managing LLM Context Model Context Protocol (MCP) refers to specific rules and configurations about what information should be provided to a language model (e.g., GPT-4, Claude 3.7 Sonnet), in what format, and how. Let\u0026rsquo;s explore the following topics to better understand this concept.\nDefining and Formatting Context What is Context =\u0026gt; Language models rely solely on the input (prompt) sent to them when generating responses. This input can include various elements such as conversation history, system instructions, user questions, and even data obtained from tool calls. What is Protocol =\u0026gt; The term protocol refers to the rules that determine how this information will be organized, ordered, and formatted. For example, OpenAI\u0026rsquo;s ChatGPT model separates chat history with \u0026lsquo;system\u0026rsquo;, \u0026lsquo;user\u0026rsquo;, and \u0026lsquo;assistant\u0026rsquo; roles. This structure determines which information the model will prioritize.\nConversation Memory and Context Management Conversation Memory =\u0026gt; Conversation memory is a mechanism that enables remembering previous messages. However, the model doesn\u0026rsquo;t directly store state; context is provided by adding previous messages to the prompt each time. Token Limit =\u0026gt; Models have a limit on the total number of tokens they can process. Therefore, decisions such as which past messages to include and which information to summarize are handled within the scope of \u0026ldquo;model context protocol\u0026rdquo;. This ensures the model sees the most relevant information.\nDynamic and Static Information Dynamic Information =\u0026gt; Information that is continuously updated during conversation (e.g., user messages, responses from tools) is added to the prompt as part of dynamic context. Static Information =\u0026gt; There may also be some system instructions, fixed commands, or information that specify how the model should behave. For example, requesting the model to respond in a specific tone or language.\nApplication Example Let\u0026rsquo;s say we\u0026rsquo;re creating a chatbot. MCP in this chatbot project could work with this logic:\nSystem Message =\u0026gt; \u0026ldquo;This is a customer support chat. Provide your answers in a polite and helpful tone.\u0026rdquo; User Messages =\u0026gt; Past messages like \u0026ldquo;Hello, I have an issue with my account\u0026rdquo; Tool Calls =\u0026gt; If data is being retrieved from external APIs, this data is also included in the prompt appropriately Prompt Creation =\u0026gt; All this information is combined in a specific order and format (e.g., User: \u0026hellip;, Agent: \u0026hellip;) and sent to the model to generate a response. In summary, MCP serves as a guide that determines what information will be given to the language model, how this information will be structured, and how the model will produce more effective responses. Creating a proper protocol helps the model better understand context and provide more consistent, relevant answers. This concept is particularly critical in long and complex dialogues for keeping the right information within the model\u0026rsquo;s comprehensible limits.\n6. Fine-tuning: Customizing LLMs Fine-tuning is the process of customizing a pre-trained language model (LLM) for a specific task or domain. This process ensures that the model produces more accurate and consistent responses for a particular use case.\nWhen to Use Fine-tuning? Fine-tuning is particularly useful in these situations:\nTasks requiring specific domain knowledge Outputs requiring consistent format or style Situations requiring brand voice and tone alignment Technical terminology usage Multiple similar task repetitions Fine-tuning Advantages Better Performance:\nMore accurate answers in domain-specific tasks More consistent output format Reduced hallucinations Cost Optimization:\nAbility to use shorter prompts Less token consumption Faster response times Customized Behavior:\nResponses aligned with brand language Consistent tone and style Ability to learn specific rules Fine-tuning Process Data Preparation:\nCollecting training data Data cleaning and formatting Creating prompt-completion pairs Model Selection:\nDetermining base model Adjusting model parameters Selecting training strategy Training:\nHyperparameter optimization Monitoring training metrics Cross-validation checks Evaluation:\nModel performance analysis A/B tests Human evaluation Fine-tuning Best Practices Data Quality:\nUse high-quality training data Ensure data diversity Create balanced data distribution Model Selection:\nChoose appropriate model size for task Consider cost-performance balance Evaluate base model performance Training Strategy:\nApply gradual fine-tuning Prevent overfitting Conduct regular evaluation Deployment:\nImplement model versioning Set up performance monitoring mechanisms Create feedback loop 7. Ollama, LiteLLM, LlamaCPP: Open Source AI Solutions Open-source LLMs and tools make AI development processes more accessible. In this section, we\u0026rsquo;ll explore popular open-source AI solutions you can use in your Node.js applications.\nOllama: Ollama is a platform that allows users to run large language models (e.g., LLaMA, GPT derivatives) on their own machines, typically providing a user-friendly interface. Such applications are designed to facilitate model installation, updates, and integration; thus, enabling secure and fast access locally without depending on internet connection. LiteLLM: Litellm is a library or tool developed with a \u0026ldquo;lightweight\u0026rdquo; approach to facilitate working with large language models. It typically offers minimal resource usage, flexibility, and rapid prototyping capabilities. Such tools focus on running existing large models with fewer resources rather than model training. LlamaCpp: LlamaCpp is a C++ optimized implementation of large language models like Meta\u0026rsquo;s LLaMA model. This library aims to efficiently run quantized models on CPU. Thus, it becomes possible to run LLM locally without needing powerful GPUs. Comparison of Open Source AI Solutions Feature Ollama LiteLLM LlamaCPP Ease of Setup ★★★★★ ★★★★☆ ★★☆☆☆ Performance ★★★★☆ ★★★★☆ ★★★★★ API Compatibility ★★★☆☆ ★★★★★ ★★☆☆☆ Resource Usage ★★★☆☆ ★★★★☆ ★★★★★ Multi-Model Support ★★★★☆ ★★★★★ ★★★☆☆ Cost Free Free* Free In summary, while they all aim to facilitate the use of large language models in local or optimized environments:\nOllama offers integrated solutions by simplifying usage as a more comprehensive platform, Litellm aims to provide flexibility to developers with its lightweight and modular structure, LlamaCpp offers a C++ optimized solution particularly in terms of performance and low resource usage. These tools can be preferred based on the hardware and usage scenario they will be run on. For example, litellm can be used for rapid prototyping in your development environment, while llamaCpp might be preferred for performance-focused local applications; Ollama can appeal to those seeking a more comprehensive user experience.\n8. N8n: AI Workflow Automation N8n is an open-source automation tool that allows you to automate workflows with no or minimal code. N8n\u0026rsquo;s flexible structure allows you to easily integrate AI services with other applications. N8n is completely open-source; this means you can access, customize, and adapt the source code according to your needs. Thanks to its drag-and-drop interface, you can design flows consisting of \u0026ldquo;nodes\u0026rdquo;, each performing a specific operation. This allows even users with little technical knowledge to create complex workflows.\nN8n provides integration with numerous APIs and services, allowing you to transfer data or perform automatic operations between different systems such as email, database, social media, file storage. You can host N8n on your own servers. This ensures complete control over your data and provides an advantage especially in privacy or compliance matters.\nFor example, if you want to add a record to a database when receiving an email in a customer support process, then send a notification via Slack, you can arrange these steps visually with n8n and make them automatic.\nAI Workflows Possible with N8n Document Processing and Summarization\nAutomatically summarizing email attachments Analyzing PDF documents and extracting data Customer Support Automation\nAnalyzing and classifying incoming customer questions Generating automatic responses to simple questions, routing complex ones Social Media Management\nAutomatically generating content on specific topics Classifying and responding to comments with sentiment analysis Data Analysis and Reporting\nAnalyzing and summarizing periodic data Detecting data anomalies and sending alerts Advantages of N8n and AI Integration Automation Without Code: Even non-technical team members can create complex AI workflows Multi-Service Integration: You can incorporate AI into all your business processes with 200+ integrations Self-Hosting: You can run on your own infrastructure for AI operations containing sensitive data Flexible Triggers: You can initiate AI operations with timer, webhook, event-based triggers Final Words AI technologies are rapidly evolving, and their integration in the Node.js ecosystem is becoming increasingly easier. The techniques we\u0026rsquo;ve seen in this article provide a solid foundation for developing AI-powered applications.\nYou can develop more powerful and intelligent applications using modern AI technologies such as embeddings, RAG (Retrieval Augmented Generation), and vector databases, along with fundamental concepts like AI agents, tool calling, conversation memory, and context management. With fine-tuning, you can customize your models to achieve more successful results in domain-specific tasks.\nTogether with open-source models and automation tools like N8n, you can develop solutions such as:\nChatbots that produce more accurate and context-aware responses Intelligent document processing and analysis systems Semantic search and recommendation engines Automated workflows Domain-specific AI assistants You can access all code examples shown in this article and more at GitHub Repository.\nGood luck on your artificial intelligence journey!\n",
        "tags": ["ai","nodejs"],
        "categories": ["AI","Node.js","JavaScript"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 9: Sample Project",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part9/",
        "summary": "In this article, we will explore the process of developing a Todo application using TypeScript and Node.js with JWT and Google OAuth authentication. We will discuss the experience of designing a modern REST API by implementing TypeScript\u0026rsquo;s type safety, MongoDB integration, and clean architecture principles.",
        "content": "Developing a Node.js Authentication API with TypeScript: Comprehensive Guide Introduction Type safety and code quality are becoming increasingly important when developing modern web applications. In this tutorial, we will learn TypeScript\u0026rsquo;s powerful features through a real project. While developing a Todo application with JWT and Google OAuth authentication, we will implement TypeScript\u0026rsquo;s core concepts and best practices. You can access the complete project at this GitHub repository.\nThis project will provide you with:\nPractice in writing secure code with TypeScript Experience in designing a modern REST API Authentication and Authorization implementation Using TypeScript with MongoDB Applying Clean Architecture principles TypeScript Features and Project Structure We\u0026rsquo;ll develop a secure and scalable API using TypeScript\u0026rsquo;s core features. Let\u0026rsquo;s explain each feature with examples from our actual project code:\n1. TypeScript Basics (Basics) TypeScript\u0026rsquo;s basic building blocks are used in our project like this:\n// src/config/env.ts\u0026#39;de Tip Tanımlamaları const PORT: number = Number(process.env.PORT) || 3000; const JWT_EXPIRES_IN: string = \u0026#39;1d\u0026#39;; // src/middleware/auth.middleware.ts\u0026#39;de Type Assertion const decoded = jwt.verify(token, JWT_SECRET) as IJwtPayload; // Burada JWT\u0026#39;den gelen veriyi IJwtPayload tipine dönüştürüyoruz // src/interfaces/user.interface.ts\u0026#39;de Literal Types type UserRole = \u0026#39;user\u0026#39; | \u0026#39;admin\u0026#39;; // User modelinde kullanıcı rollerini sadece bu iki değerle sınırlıyoruz Projedeki Kullanım Örnekleri:\nPORT tanımı src/index.ts\u0026lsquo;de server başlatırken kullanılıyor Type assertion auth.middleware.ts\u0026lsquo;de JWT doğrulamasında kullanılıyor UserRole tipi IUser interface\u0026rsquo;inde kullanıcı rolünü kısıtlamak için kullanılıyor 2. Functions TypeScript\u0026rsquo;te functions are typed like this:\n// src/services/auth.service.ts\u0026#39;de Method Signatures interface IAuthService { login(credentials: IUserLogin): Promise\u0026lt;{ user: IUser; token: string }\u0026gt;; register(userData: IUserRegistration): Promise\u0026lt;IUser\u0026gt;; } // src/controllers/auth.controller.ts\u0026#39;de Implementation public async login(req: Request, res: Response): Promise\u0026lt;void\u0026gt; { const { email, password } = req.body; const result = await this.authService.login({ email, password }); // ... } Projedeki Kullanım Örnekleri:\nIAuthService interface\u0026rsquo;i auth.service.ts\u0026lsquo;de servis implementasyonunu tanımlıyor Controller\u0026rsquo;lardaki tüm handler functions use Request and Response types All async functions are typed with Promise return type 3. Object Types We model complex data structures with object types in our project:\n// src/config/database.ts\u0026#39;de Configuration Types type DatabaseConfig = { uri: string; options: { useNewUrlParser: boolean; useUnifiedTopology: boolean; }; }; // src/controllers/todo.controller.ts\u0026#39;de Request Types interface ITodoCreate { title: string; description?: string; // Optional property example } Projedeki Kullanım Örnekleri:\nDatabaseConfig type defines MongoDB connection settings ITodoCreate interface is used for request body validation in todo creation endpoint Optional properties allow partial updates in todo updates 4. Interfaces Interfaces are used in our project both for type definition and for contracts:\n// src/interfaces/base.interface.ts\u0026#39;de Base Interface interface IBaseEntity { _id: string; createdAt: Date; updatedAt: Date; } // src/interfaces/user.interface.ts\u0026#39;de Interface Extension interface IUser extends IBaseEntity { email: string; password?: string; name: string; role: UserRole; } Projedeki Kullanım Örnekleri:\nIBaseEntity defines common fields for all MongoDB models IUser interface defines User model schema and methods Interfaces ensure type safety in mongoose model definitions 5. TypeScript Compiler We configure TypeScript compiler specifically for our project:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es2016\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;strict\u0026#34;: true, \u0026#34;esModuleInterop\u0026#34;: true, \u0026#34;skipLibCheck\u0026#34;: true, \u0026#34;forceConsistentCasingInFileNames\u0026#34;: true, \u0026#34;outDir\u0026#34;: \u0026#34;./dist\u0026#34;, \u0026#34;rootDir\u0026#34;: \u0026#34;./src\u0026#34; } } Compiler Settings\u0026rsquo;s Importance:\nstrict: Enables strict type checking target: Allows us to use modern JavaScript features module: Uses Node.js compatible module system 6. Classes We implement OOP principles with TypeScript classes:\n// src/services/base.service.ts\u0026#39;de Abstract Base Class abstract class BaseService\u0026lt;T extends IBaseEntity\u0026gt; { constructor(protected model: Model\u0026lt;T\u0026gt;) {} abstract create(data: Partial\u0026lt;T\u0026gt;): Promise\u0026lt;T\u0026gt;; async findById(id: string): Promise\u0026lt;T | null\u0026gt; { return this.model.findById(id); } } // src/services/todo.service.ts\u0026#39;de Class Implementation class TodoService extends BaseService\u0026lt;ITodo\u0026gt; { async create(data: ICreateTodo): Promise\u0026lt;ITodo\u0026gt; { return this.model.create(data); } async markAsCompleted(id: string): Promise\u0026lt;ITodo | null\u0026gt; { return this.model.findByIdAndUpdate(id, { completed: true }); } } Why This Feature?\nAbstract classes enforce common behaviors Inheritance reduces code repetition Organizes service layer Projedeki Kullanım Örnekleri:\nBaseService defines basic CRUD operations for all services TodoService and AuthService extend this base class to add their own specific methods Abstract methods ensure each service must implement its own create method 7. Generics We use generics in our project like this:\n// src/services/base.service.ts\u0026#39;de Generic Service class CrudService\u0026lt;T extends IBaseEntity\u0026gt; { async findOne(filter: FilterQuery\u0026lt;T\u0026gt;): Promise\u0026lt;T | null\u0026gt; { return this.model.findOne(filter); } } // src/utils/response.ts\u0026#39;de Generic Response Handler function createResponse\u0026lt;T\u0026gt;(success: boolean, message: string, data?: T): IApiResponse\u0026lt;T\u0026gt; { return { success, message, data }; } // src/utils/error.ts\u0026#39;de Generic Error Handler class ApiError\u0026lt;T = unknown\u0026gt; extends Error { constructor(public statusCode: number, message: string, public data?: T) { super(message); } } Why This Feature?\nKeeps type safety while writing reusable code Creates functions that work with different data types Creates flexible structures with type parameters Projedeki Kullanım Örnekleri:\nCrudService works with different model types (User, Todo, etc.) createResponse creates consistent API responses for all data structures ApiError provides customizable error handling for different error types 8. Type Narrowing We safely perform runtime type checking and narrowing in TypeScript:\n// src/utils/error.ts\u0026#39;de Type Guards function isError(error: unknown): error is Error { return error instanceof Error; } // src/middleware/error.middleware.ts\u0026#39;de Error Handling function handleError(error: unknown): IApiResponse\u0026lt;null\u0026gt; { if (isError(error)) { return createResponse(false, error.message); } if (typeof error === \u0026#39;string\u0026#39;) { return createResponse(false, error); } return createResponse(false, \u0026#39;Unknown error occurred\u0026#39;); } // src/types/error.types.ts\u0026#39;de Discriminated Unions type ValidationError = { type: \u0026#39;validation\u0026#39;; fields: { [key: string]: string }; }; type AuthError = { type: \u0026#39;auth\u0026#39;; message: string; }; type AppError = ValidationError | AuthError; // src/utils/error-handler.ts\u0026#39;de Error Type Handling function handleAppError(error: AppError) { switch (error.type) { case \u0026#39;validation\u0026#39;: return error.fields; case \u0026#39;auth\u0026#39;: return error.message; } } Why This Feature?\nKeeps runtime type safety Improves error handling Works with Union types correctly Projedeki Kullanım Örnekleri:\nisError type guard in middleware detects error type correctly Error handling middleware distinguishes different error types Discriminated unions allow handling validation and auth errors separately Project Summary Our API will include the following features:\nUser Management\nRegistration and Login JWT Authentication Google OAuth Integration Role-based authorization Todo Operations\nCreate, read, update, delete todos User-specific todos Todo status changes Security and Validation\nInput validation Route protection Error handling Project Structure Our project is organized as follows:\nsrc/ ├── config/ # Configuration files ├── controllers/ # HTTP request handlers ├── interfaces/ # TypeScript interfaces ├── middleware/ # Express middleware ├── models/ # Mongoose models ├── routes/ # API routes ├── services/ # Business logic ├── utils/ # Helper functions └── index.ts # Application entry point Developing the Project with TypeScript 1. Project Setup and TypeScript Configuration First step is to integrate TypeScript into our project:\nmkdir nodejs-typescript-auth cd nodejs-typescript-auth npm init -y npm install typescript ts-node @types/node --save-dev Dependencies Let\u0026rsquo;s install necessary packages for our project:\n# Main dependencies npm install express mongoose dotenv jsonwebtoken bcrypt passport passport-google-oauth20 passport-jwt cors # Type definitions npm install @types/express @types/mongoose @types/jsonwebtoken @types/bcrypt @types/passport @types/passport-google-oauth20 @types/passport-jwt @types/cors --save-dev TypeScript Configuration { \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es2016\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;outDir\u0026#34;: \u0026#34;./dist\u0026#34;, \u0026#34;rootDir\u0026#34;: \u0026#34;./src\u0026#34;, \u0026#34;strict\u0026#34;: true, \u0026#34;esModuleInterop\u0026#34;: true, \u0026#34;skipLibCheck\u0026#34;: true, \u0026#34;forceConsistentCasingInFileNames\u0026#34;: true } } 2. Defining Data Models User Model Let\u0026rsquo;s define our User model with TypeScript interfaces:\n// src/interfaces/user.interface.ts // Base interface - basic user properties interface IBaseUser { email: string; name: string; } // Main user interface - includes all properties interface IUser extends IBaseUser { password?: string; // Optional: Google OAuth users may not have a password googleId?: string; // Optional: Only for users who sign in with Google role: \u0026#39;user\u0026#39; | \u0026#39;admin\u0026#39;; // Union type to limit roles comparePassword(candidatePassword: string): Promise\u0026lt;boolean\u0026gt;; } // Required fields for registration interface IUserRegistration { email: string; password: string; name: string; } // Required fields for login interface IUserLogin { email: string; password: string; } Todo Model Let\u0026rsquo;s define the interfaces needed for todo operations:\n// src/interfaces/todo.interface.ts interface ITodo { title: string; description?: string; completed: boolean; user: string; // Reference: User ID createdAt: Date; updatedAt: Date; } // Required fields for todo creation interface ICreateTodo { title: string; description?: string; } // Optional fields for todo updates interface IUpdateTodo { title?: string; description?: string; completed?: boolean; } 3. Service Layer Implementation Base Service Let\u0026rsquo;s create a generic base service to reduce code repetition:\n// src/services/base.service.ts abstract class BaseService\u0026lt;T\u0026gt; { constructor(protected model: Model\u0026lt;T\u0026gt;) {} async findById(id: string): Promise\u0026lt;T | null\u0026gt; { return this.model.findById(id); } async findOne(filter: FilterQuery\u0026lt;T\u0026gt;): Promise\u0026lt;T | null\u0026gt; { return this.model.findOne(filter); } async find(filter: FilterQuery\u0026lt;T\u0026gt;): Promise\u0026lt;T[]\u0026gt; { return this.model.find(filter); } } Auth Service Service to handle authentication operations:\n// src/services/auth.service.ts class AuthService extends BaseService\u0026lt;IUser\u0026gt; { public async register(userData: IUserRegistration): Promise\u0026lt;IUser\u0026gt; { const existingUser = await this.findOne({ email: userData.email }); if (existingUser) { throw new Error(\u0026#39;This email is already in use\u0026#39;); } const user = await this.model.create(userData); return user; } public async login(loginData: IUserLogin): Promise\u0026lt;{ user: IUser; token: string }\u0026gt; { const user = await this.findOne({ email: loginData.email }); if (!user || !(await user.comparePassword(loginData.password))) { throw new Error(\u0026#39;Invalid credentials\u0026#39;); } return { user, token: this.generateToken(user), }; } private generateToken(user: IUser): string { return jwt.sign({ id: user._id, email: user.email, role: user.role }, process.env.JWT_SECRET!, { expiresIn: \u0026#39;1d\u0026#39; }); } } Todo Service Service to handle todo operations:\n// src/services/todo.service.ts class TodoService extends BaseService\u0026lt;ITodo\u0026gt; { public async getAllTodos(userId: string): Promise\u0026lt;ITodo[]\u0026gt; { return this.find({ user: userId }); } public async createTodo(todoData: ICreateTodo, userId: string): Promise\u0026lt;ITodo\u0026gt; { return this.model.create({ ...todoData, user: userId, completed: false, }); } public async updateTodo(todoId: string, todoData: Partial\u0026lt;ITodo\u0026gt;, userId: string): Promise\u0026lt;ITodo | null\u0026gt; { return this.model.findOneAndUpdate({ _id: todoId, user: userId }, todoData, { new: true }); } } 4. Middleware Implementation TypeScript\u0026rsquo;s safe middleware writing:\n// src/middleware/auth.middleware.ts // Request type extension declare global { namespace Express { interface Request { user?: IUser; } } } export const isAuthenticated = async (req: Request, res: Response, next: NextFunction): Promise\u0026lt;void\u0026gt; =\u0026gt; { try { const token = req.headers.authorization?.split(\u0026#39; \u0026#39;)[1]; if (!token) { throw new Error(\u0026#39;Token not found\u0026#39;); } const decoded = jwt.verify(token, process.env.JWT_SECRET!) as IJwtPayload; const user = await UserModel.findById(decoded.id); if (!user) { throw new Error(\u0026#39;User not found\u0026#39;); } req.user = user; next(); } catch (error) { res.status(401).json({ success: false, message: \u0026#39;Authorization error\u0026#39;, error: error instanceof Error ? error.message : \u0026#39;Unknown error\u0026#39;, }); } }; 5. Controller Layer TypeScript\u0026rsquo;s safe controller:\n// src/controllers/todo.controller.ts class TodoController { constructor(private todoService: TodoService) {} public async getAllTodos(req: Request, res: Response): Promise\u0026lt;void\u0026gt; { try { const todos = await this.todoService.getAllTodos(req.user!._id); res.status(200).json({ success: true, message: \u0026#39;Todos fetched successfully\u0026#39;, data: todos, }); } catch (error) { res.status(500).json({ success: false, message: \u0026#39;Error fetching todos\u0026#39;, error: error instanceof Error ? error.message : \u0026#39;Unknown error\u0026#39;, }); } } } API Endpoints Auth Endpoints POST /api/auth/register - Request Body: { email: string, password: string, name: string } - Response: { success: boolean, message: string, data: { user: IUser, token: string } } POST /api/auth/login - Request Body: { email: string, password: string } - Response: { success: boolean, message: string, data: { user: IUser, token: string } } GET /api/auth/google - Google OAuth initiation endpoint GET /api/auth/google/callback - Google OAuth callback endpoint Todo Endpoints GET /api/todos - Headers: { Authorization: \u0026#34;Bearer ${token}\u0026#34; } - Response: { success: boolean, message: string, data: ITodo[] } POST /api/todos - Headers: { Authorization: \u0026#34;Bearer ${token}\u0026#34; } - Request Body: { title: string, description?: string } - Response: { success: boolean, message: string, data: ITodo } PUT /api/todos/:id - Headers: { Authorization: \u0026#34;Bearer ${token}\u0026#34; } - Request Body: { title?: string, description?: string, completed?: boolean } - Response: { success: boolean, message: string, data: ITodo } DELETE /api/todos/:id - Headers: { Authorization: \u0026#34;Bearer ${token}\u0026#34; } - Response: { success: boolean, message: string } Best Practices Type Safety\nAlways use specific types Avoid the any type Limit value sets with Union types Write reusable code with Generic types Code Organization\nUse separate folders for each layer Keep interfaces in relevant domain folders Generalize service layer with abstract classes Error Handling\nCreate custom error classes Use global error handler Standardize error messages Security\nStore sensitive information in environment variables Implement input validation Apply rate limiting Configure CORS policies correctly Conclusion In this project, we learned:\nWriting secure code with TypeScript\u0026rsquo;s type system Implementing OOP principles with TypeScript Designing a modern API architecture Implementing authentication and authorization TypeScript provided our project with important advantages such as:\nCompile-time error detection Better IDE support Self-documenting code Maintainability ",
        "tags": ["typescript","nodejs","microservices"],
        "categories": ["TypeScript","Node.js","Backend"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 8: Type Narrowing",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part8/",
        "summary": "In this article, we\u0026rsquo;ll explore type narrowing techniques in TypeScript. We\u0026rsquo;ll cover type checking methods like typeof, instanceof, type predicates, and discriminated unions.",
        "content": "Hello! In this part of our TypeScript series, we\u0026rsquo;ll explore type narrowing, one of the most powerful features of the type system. Type narrowing refers to the techniques that allow us to narrow down a variable\u0026rsquo;s type to a more specific type. These techniques help us work more safely with union types and complex type structures.\nWhat is Type Narrowing? Type narrowing is TypeScript\u0026rsquo;s ability to narrow down a variable\u0026rsquo;s type to a more specific type within a context. This feature is particularly useful when working with union types. For example, if a variable can be either a string or a number, but we know it\u0026rsquo;s definitely a string in a specific code block, TypeScript uses this information to enhance type safety.\ntypeof Type Guards The typeof operator is one of the most basic type narrowing methods in TypeScript:\nconst isTeenager = (age: number | string) =\u0026gt; { if (typeof age === \u0026#39;string\u0026#39;) { // Here age is definitely a string return age.charAt(0) === \u0026#39;1\u0026#39;; } else { // Here age is definitely a number return age \u0026gt; 12 \u0026amp;\u0026amp; age \u0026lt; 20; } }; isTeenager(\u0026#39;20\u0026#39;); // false isTeenager(13); // true Advantages of using typeof type guards:\nProvides type safety Improves IDE support and code completion features Prevents runtime errors Increases code readability Truthiness Type Guards We can also perform type narrowing using JavaScript\u0026rsquo;s truthiness feature:\nconst printLetters = (word: string | null) =\u0026gt; { if (!word) { console.log(\u0026#39;No word was provided.\u0026#39;); return; } // Here word is definitely a string word.split(\u0026#39;\u0026#39;).forEach((letter) =\u0026gt; console.log(letter)); }; printLetters(\u0026#39;Hello\u0026#39;); // H, e, l, l, o printLetters(null); // No word was provided. Truthiness check evaluates these values as false:\nfalse 0 \u0026quot;\u0026quot; null undefined NaN Equality Type Narrowing Equality comparisons are also used for type narrowing in TypeScript:\nconst someFunc = (x: string | boolean, y: string | number) =\u0026gt; { if (x === y) { // Here both x and y are definitely strings console.log(x.toUpperCase()); console.log(y.toLowerCase()); } else { // x: string | boolean // y: string | number console.log(x); console.log(y); } }; in Operator Type Guards JavaScript\u0026rsquo;s in operator checks if a property exists in an object. TypeScript uses this check for type narrowing:\ntype Cat = { meow: () =\u0026gt; void }; type Dog = { bark: () =\u0026gt; void }; const talk = (creature: Cat | Dog) =\u0026gt; { if (\u0026#39;meow\u0026#39; in creature) { // Here creature is definitely a Cat creature.meow(); } else { // Here creature is definitely a Dog creature.bark(); } }; const kitty: Cat = { meow: () =\u0026gt; console.log(\u0026#39;MEOWWW\u0026#39;) }; talk(kitty); // MEOWWW instanceof Narrowing The instanceof operator checks if a variable is an instance of a specific class:\nconst printFullDate = (date: Date | string) =\u0026gt; { if (date instanceof Date) { // Here date is definitely a Date return date.toUTCString(); } else { // Here date is definitely a string return new Date(date).toUTCString(); } }; console.log(printFullDate(new Date())); console.log(printFullDate(\u0026#39;2025-02-21\u0026#39;)); Type Predicates In TypeScript, you can write custom type guard functions. These functions have a return type in the format parameterName is Type:\ninterface Cat { meow: () =\u0026gt; void; } interface Dog { bark: () =\u0026gt; void; } // Type predicate function function isCat(pet: Cat | Dog): pet is Cat { return (pet as Cat).meow !== undefined; } let pet = getAnimal(); if (isCat(pet)) { // Here pet is definitely a Cat pet.meow(); } else { // Here pet is definitely a Dog pet.bark(); } Advantages of type predicates:\nYou can write custom type guard logic Prevents code duplication Centralizes type checks Improves readability Discriminated Unions Discriminated unions is a technique for distinguishing between related types using a common literal property:\ninterface Circle { kind: \u0026#39;circle\u0026#39;; radius: number; } interface Square { kind: \u0026#39;square\u0026#39;; sideLength: number; } type Shape = Circle | Square; function getArea(shape: Shape) { switch (shape.kind) { case \u0026#39;circle\u0026#39;: // Here shape is definitely a Circle return Math.PI * shape.radius ** 2; case \u0026#39;square\u0026#39;: // Here shape is definitely a Square return shape.sideLength ** 2; } } Advantages of discriminated unions:\nProvides type safety Easy to use with switch cases Excellent IDE support Easy to add new types Catches missing cases at compile time Best Practices Choosing the Right Type Guard\n// typeof for simple types function processValue(value: string | number) { if (typeof value === \u0026#39;string\u0026#39;) { return value.toUpperCase(); } return value.toFixed(2); } // instanceof for classes function processDate(date: Date | string) { if (date instanceof Date) { return date.toISOString(); } return new Date(date).toISOString(); } Effective Use of Type Predicates\ninterface User { id: number; name: string; } interface Admin extends User { role: \u0026#39;admin\u0026#39;; permissions: string[]; } function isAdmin(user: User): user is Admin { return \u0026#39;role\u0026#39; in user \u0026amp;\u0026amp; user.role === \u0026#39;admin\u0026#39;; } Properly Structuring Discriminated Unions\ninterface ApiSuccess { status: \u0026#39;success\u0026#39;; data: any; } interface ApiError { status: \u0026#39;error\u0026#39;; error: string; } type ApiResponse = ApiSuccess | ApiError; function handleResponse(response: ApiResponse) { if (response.status === \u0026#39;success\u0026#39;) { processData(response.data); } else { handleError(response.error); } } Conclusion Type narrowing is one of TypeScript\u0026rsquo;s most powerful features. With these techniques, you can:\nWrite safer code Reduce runtime errors Get maximum benefit from IDE support Manage complex type structures more easily In our next article, we\u0026rsquo;ll continue exploring other advanced features of TypeScript. See you soon!\n",
        "tags": ["typescript","type-narrowing","type-guards"],
        "categories": ["TypeScript"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 7: Generics",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part7/",
        "summary": "In this article, we\u0026rsquo;ll explore generics, one of TypeScript\u0026rsquo;s most powerful features. We\u0026rsquo;ll learn how to write reusable code while maintaining type safety.",
        "content": "Hello! In this part of our TypeScript series, we\u0026rsquo;ll explore generic structures that make your code more flexible and reusable. Generics allow us to write functions and classes that can work with different data types while maintaining type safety.\nWhat are Generic Structures? Generics is a feature that allows a function or class to work with different types. The key point here is maintaining type safety while providing this flexibility. Let\u0026rsquo;s start with a simple example:\n// Non-generic approach - Separate function for each type function getFirstNumber(arr: number[]): number { return arr[0]; } function getFirstString(arr: string[]): string { return arr[0]; } // Generic approach - Single function, all types function getFirst\u0026lt;T\u0026gt;(arr: T[]): T { return arr[0]; } // Usage const firstNumber = getFirst\u0026lt;number\u0026gt;([1, 2, 3]); // type number const firstText = getFirst\u0026lt;string\u0026gt;([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]); // type string Here, \u0026lt;T\u0026gt; is a type parameter that specifies which types the function will work with. Since TypeScript can infer types in most cases, we don\u0026rsquo;t need to explicitly specify the type parameter:\nconst firstNumber = getFirst([1, 2, 3]); // automatically infers number type const firstText = getFirst([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]); // automatically infers string type Built-in Generic Types TypeScript has several commonly used built-in generic types:\nArray let numbers: Array\u0026lt;number\u0026gt; = [1, 2, 3]; // number[] let texts: Array\u0026lt;string\u0026gt; = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]; // string[] Promise async function fetchData(): Promise\u0026lt;User\u0026gt; { const response = await fetch(\u0026#39;/api/user\u0026#39;); return response.json(); } Record\u0026lt;K,V\u0026gt; type UserRoles = Record\u0026lt;string, string[]\u0026gt;; const roles: UserRoles = { admin: [\u0026#39;read\u0026#39;, \u0026#39;write\u0026#39;, \u0026#39;delete\u0026#39;], editor: [\u0026#39;read\u0026#39;, \u0026#39;write\u0026#39;], user: [\u0026#39;read\u0026#39;], }; Writing Generic Functions Here are some key points to consider when writing generic functions:\n// Simple generic function function reverse\u0026lt;T\u0026gt;(items: T[]): T[] { return items.reverse(); } // Generic function - Arrow function syntax const filter = \u0026lt;T\u0026gt;(arr: T[], fn: (item: T) =\u0026gt; boolean): T[] =\u0026gt; { return arr.filter(fn); }; // Usage examples const numbers = reverse\u0026lt;number\u0026gt;([1, 2, 3]); const texts = reverse([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]); // Type inference const filteredArray = filter([1, 2, 3, 4], (n) =\u0026gt; n % 2 === 0); Multiple Type Parameters Sometimes we need multiple type parameters:\nfunction pair\u0026lt;T, U\u0026gt;(x: T, y: U): [T, U] { return [x, y]; } const result = pair\u0026lt;string, number\u0026gt;(\u0026#39;hello\u0026#39;, 42); const automatic = pair(\u0026#39;hello\u0026#39;, 42); // Type inference works Type Constraints We can use type constraints to specify what properties generic types should have:\ninterface Length { length: number; } function showLength\u0026lt;T extends Length\u0026gt;(arg: T): number { return arg.length; } // Works - string has length property showLength(\u0026#39;Hello\u0026#39;); // Works - array has length property showLength([1, 2, 3]); // Error - number doesn\u0026#39;t have length property // showLength(123); Generic Classes We can also use generic structures in classes:\nclass DataContainer\u0026lt;T\u0026gt; { private data: T[]; constructor(initialData: T[]) { this.data = initialData; } add(item: T): void { this.data.push(item); } get(index: number): T { return this.data[index]; } getAll(): T[] { return this.data; } } // Usage const numberContainer = new DataContainer\u0026lt;number\u0026gt;([1, 2, 3]); numberContainer.add(4); console.log(numberContainer.getAll()); // [1, 2, 3, 4] const textContainer = new DataContainer\u0026lt;string\u0026gt;([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]); textContainer.add(\u0026#39;d\u0026#39;); console.log(textContainer.get(0)); // \u0026#39;a\u0026#39; Generic Interface\u0026rsquo;s Interfaces can also use generic structures:\ninterface APIResponse\u0026lt;T\u0026gt; { data: T; status: number; message: string; } interface User { id: number; name: string; } // Usage function fetchUser(): Promise\u0026lt;APIResponse\u0026lt;User\u0026gt;\u0026gt; { return fetch(\u0026#39;/api/user\u0026#39;).then((res) =\u0026gt; res.json()); } // async/await usage async function getUser(): Promise\u0026lt;APIResponse\u0026lt;User\u0026gt;\u0026gt; { const response = await fetch(\u0026#39;/api/user\u0026#39;); return response.json(); } Generic Type Inference (Type Inference) TypeScript\u0026rsquo;s type inference system is very powerful. In most cases, we don\u0026rsquo;t need to explicitly specify generic types:\n// Explicit type parameter const x = getFirst\u0026lt;number\u0026gt;([1, 2, 3]); // Allow type inference const y = getFirst([1, 2, 3]); // automatically infers number type // Type inference for generic class const container = new DataContainer([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]); // automatically infers string[] type Best Practices Generic Naming Rules // Common generic type names: // T: Type (General type parameter) // K: Key (Primary type) // V: Value (Value type) // E: Element (Element type) // P: Properties (Property type) Using Type Constraints Correctly // Good usage interface HasId { id: number; } function getById\u0026lt;T extends HasId\u0026gt;(items: T[], id: number): T | undefined { return items.find((item) =\u0026gt; item.id === id); } Generic Constraint vs Union Types // Generic constraint usage function process\u0026lt;T extends string | number\u0026gt;(value: T): T { return value; } // Union type usage function process2(value: string | number): string | number { return value; } // Generic constraint is safer and maintains type information Keep Generics Simple // Complex function processData\u0026lt;T, U, V, W\u0026gt;( data: T[], transformer: (item: T) =\u0026gt; U, validator: (transformed: U) =\u0026gt; V, formatter: (validated: V) =\u0026gt; W ): W[] { // ... } // Better - Use separate interfaces for intermediate types interface DataProcessor\u0026lt;T, R\u0026gt; { transform(item: T): R; } function processData\u0026lt;T, R\u0026gt;(data: T[], processor: DataProcessor\u0026lt;T, R\u0026gt;): R[] { return data.map((item) =\u0026gt; processor.transform(item)); } Generic\u0026rsquo;s with Arrow Functions When using generic arrow functions in TypeScript, there are some syntax features to consider:\n// Standard generic arrow function const identity = (arg: T): T =\u0026gt; arg; // Prevent TSX conflict when using with React const identity = (arg: T): T =\u0026gt; arg; // Arrow function with multiple type parameters const pair = (first: T, second: U): [T, U] =\u0026gt; [first, second]; Generic\u0026rsquo;s with Asynchronous Operations Asynchronous operations are common in modern web applications. Generics can also help in this area:\n// Generic async function async function fetchData(url: string): Promise { const response = await fetch(url); return response.json(); } // Usage interface User { id: number; name: string; email: string; } // Type-safe API call const user = await fetchData(\u0026#39;/api/user/1\u0026#39;); console.log(user.name); // Type-safe access // Generic error handling interface APIError { code: number; message: string; } async function fetchWithError(): Promise { try { const response = await fetch(\u0026#39;/api/data\u0026#39;); if (!response.ok) { throw (await response.json()) as APIError; } return response.json(); } catch (error) { throw error as APIError; } } Generics with Utility Types TypeScript\u0026rsquo;s built-in utility types also use generic structures. Here are some common examples:\n// Partial - Makes all properties optional interface User { id: number; name: string; email: string; } type PartialUser = Partial; // { id?: number; name?: string; email?: string; } // Pick - Selects specific properties type UserBasicInfo = Pick; // { name: string; email: string; } // Omit - Removes specific properties type UserWithoutId = Omit; // { name: string; email: string; } // Record - Creates key-value structure type UserRoles = Record; // { [key: string]: string[] } Advanced Type Inference Topics TypeScript\u0026rsquo;s type inference system is very advanced. Here are some advanced examples:\n// Return type inference function createPair(first: T) { return { first, second: first, }; } // TypeScript automatically infers { first: T, second: T } type // Generic constraints with type inference interface HasLength { length: number; } function longest(a: T, b: T): T { return a.length \u0026gt;= b.length ? a : b; } // TypeScript makes separate inferences for string[] and string const longerArray = longest([1, 2], [1, 2, 3]); // type: number[] const longerString = longest(\u0026#39;123\u0026#39;, \u0026#39;12345\u0026#39;); // type: string Generic Type Alias vs Interface When using generic structures with type alias and interface, there are some differences:\n// Generic type alias type Container = { value: T; tag: string; }; // Generic interface interface Box { value: T; tag: string; } // They are used similarly const numberContainer: Container = { value: 42, tag: \u0026#39;number\u0026#39; }; const stringBox: Box = { value: \u0026#39;test\u0026#39;, tag: \u0026#39;text\u0026#39; }; // Interfaces can be extended interface LabeledBox extends Box { label: string; } // Type aliases can be extended with intersection types type LabeledContainer = Container \u0026amp; { label: string; }; Important Points and Tips Carefully Use Generic Constraints\nVery broad constraints reduce type safety Very narrow constraints reduce reusability Trust Type Inference\nTypeScript can infer the correct type in most cases Don\u0026rsquo;t explicitly specify generic types unnecessarily Maintain Readability\nGeneric type names should be meaningful Avoid too many type parameters Break down complex generic structures into smaller parts Conclusion Generics are one of the most powerful features of TypeScript. When used correctly:\nReduce code repetition Increase type safety Allow reusable and flexible code Provide better IDE support Especially when developing large projects or libraries, using generics can maximize the power of generics. Understanding and using generics is a must when developing with TypeScript.\nOur next article will explore other advanced features of TypeScript. See you soon!\nDefault Type Parameters We can assign default values to generic types:\nclass Queue\u0026lt;T = number\u0026gt; { private data: T[] = []; push(item: T) { this.data.push(item); } pop(): T | undefined { return this.data.shift(); } } // Uses number type by default const numberQueue = new Queue(); // Customized for string type const textQueue = new Queue\u0026lt;string\u0026gt;(); Generic Interfaces We can also use generic structures in interfaces:\ninterface APIResponse\u0026lt;T\u0026gt; { data: T; status: number; message: string; } interface User { id: number; name: string; } // Usage function fetchUser(): Promise\u0026lt;APIResponse\u0026lt;User\u0026gt;\u0026gt; { return fetch(\u0026#39;/api/user\u0026#39;).then((res) =\u0026gt; res.json()); } // Usage with async/await async function getUser(): Promise\u0026lt;APIResponse\u0026lt;User\u0026gt;\u0026gt; { const response = await fetch(\u0026#39;/api/user\u0026#39;); return response.json(); } ",
        "tags": ["typescript","generics","type-safety"],
        "categories": ["TypeScript"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 6: Classes",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part6/",
        "summary": "In this article, we will examine TypeScript\u0026rsquo;s class structure in detail. We will cover topics such as class definitions, access modifiers, getter/setter methods, and abstract classes.",
        "content": "Hello! In this part of our TypeScript series, we will examine classes, one of the fundamental building blocks of object-oriented programming. We\u0026rsquo;ll learn about the advantages of TypeScript\u0026rsquo;s class structure over JavaScript, how type safety is ensured, and the features you can use to make your code safer.\nThe Concept of Classes in TypeScript TypeScript classes support all features of JavaScript classes while providing additional type safety and powerful tools for object-oriented programming. Let\u0026rsquo;s start with a simple example:\n// A class in JavaScript class JsUser { constructor(name) {image.png this.name = name; } } // The same class in TypeScript class TsUser { name: string; // Type definition constructor(name: string) { // Parameter type this.name = name; } } In the TypeScript version:\nClass property types are explicitly specified Constructor parameters have type safety IDEs provide better code completion support Basic Class Structure Here are the basic structures we can use when creating a TypeScript class:\nclass User { // Class properties (fields) id: number; name: string; private email: string; readonly registrationDate: Date; // Constructor method constructor(id: number, name: string, email: string) { this.id = id; this.name = name; this.email = email; this.registrationDate = new Date(); } // Class method displayInfo(): string { return `${this.name} (ID: ${this.id})`; } } // Using the class const user = new User(1, \u0026#34;John Smith\u0026#34;, \u0026#34;john@example.com\u0026#34;); console.log(user.displayInfo()); // \u0026#34;John Smith (ID: 1)\u0026#34; In this example:\nWe defined types for class properties We specified parameter types in the constructor We defined an immutable property with readonly We restricted access with private We specified the method return type Class Fields and Access Modifiers TypeScript has three types of access modifiers: public, private, and protected. Let\u0026rsquo;s examine their usage and differences from JavaScript:\nPublic Access Modifier This is the default access level and doesn\u0026rsquo;t require any modifier:\nclass Car { brand: string; // public by default public model: string; // explicitly marked as public constructor(brand: string, model: string) { this.brand = brand; this.model = model; } } const car = new Car(\u0026#34;Toyota\u0026#34;, \u0026#34;Corolla\u0026#34;); console.log(car.brand); // Accessible console.log(car.model); // Accessible Private Access Modifier Private properties can only be accessed from within the class. In TypeScript, we can define private properties in two different ways:\nclass Account { private _balance: number; // TypeScript private #transactions: number[]; // JavaScript private field (#) constructor(initialBalance: number) { this._balance = initialBalance; this.#transactions = []; } deposit(amount: number): void { this._balance += amount; } addTransaction(amount: number): void { this.#transactions.push(amount); this._balance -= amount; } } const account = new Account(1000); // account._balance; // Error: Cannot access private property // account.#transactions; // Error: Cannot access private field Differences between JavaScript and TypeScript private implementations:\nTypeScript\u0026rsquo;s private modifier works at compile time JavaScript\u0026rsquo;s # private fields feature works at runtime Using # provides real access restriction The private keyword only provides protection on the TypeScript side Protected Access Modifier Protected properties can be accessed by the class itself and its subclasses:\nclass Animal { protected species: string; constructor(species: string) { this.species = species; } } class Cat extends Animal { meow(): string { return `I\u0026#39;m a ${this.species}, meow!`; // Can access species property } } const cat = new Cat(\u0026#34;cat\u0026#34;); // cat.species; // Error: Cannot access protected property from outside Readonly and Parameter Properties Readonly Properties The readonly modifier indicates that a property can only be assigned a value during initialization and cannot be changed afterward:\nclass Document { readonly id: string; readonly creationDate: Date; title: string; constructor(id: string, title: string) { this.id = id; // OK this.creationDate = new Date(); // OK this.title = title; } changeId(newId: string) { this.id = newId; // Error: Cannot assign to readonly property } } Parameter Properties Parameter properties, a special shorthand provided by TypeScript, automatically converts constructor parameters into class properties:\n// Long way class Product { readonly id: string; private _price: number; public stock: number; constructor(id: string, price: number, stock: number) { this.id = id; this._price = price; this.stock = stock; } } // Using parameter properties class ProductShort { constructor( readonly id: string, private _price: number, public stock: number ) {} } Thanks to this feature:\nYou write less code Property definitions and assignments are done automatically Code becomes more readable Getter and Setter Methods In TypeScript, you can define special access methods using get and set keywords:\nclass Product { private _price: number; constructor(price: number) { this._price = price; } // Getter method get price(): number { return this._price; } // Setter method set price(newPrice: number) { if (newPrice \u0026lt; 0) { throw new Error(\u0026#34;Price cannot be negative!\u0026#34;); } this._price = newPrice; } } const product = new Product(100); console.log(product.price); // Getter is called: 100 product.price = 150; // Setter is called // product.price = -50; // Throws error Getter and setter methods allow you to:\nControl access to a property Add value validation logic Monitor property changes Return computed values Classes and Interfaces In TypeScript, classes can implement one or more interfaces:\ninterface LivingBeing { name: string; isAlive: boolean; move(): void; } interface FoodConsumer { eat(food: string): void; } class Human implements LivingBeing, FoodConsumer { constructor(public name: string) { this.isAlive = true; } isAlive: boolean; move(): void { console.log(\u0026#34;Walking on two feet\u0026#34;); } eat(food: string): void { console.log(`Eating ${food}`); } } This structure:\nProvides type safety Makes code maintenance easier Ensures all interface requirements are met Abstract Classes Abstract classes are classes that cannot be instantiated directly and serve as templates for subclasses:\nabstract class Shape { abstract calculateArea(): number; abstract calculatePerimeter(): number; displayInfo(): string { return `Area: ${this.calculateArea()}, Perimeter: ${this.calculatePerimeter()}`; } } class Rectangle extends Shape { constructor(private width: number, private height: number) { super(); } calculateArea(): number { return this.width * this.height; } calculatePerimeter(): number { return 2 * (this.width + this.height); } } // const shape = new Shape(); // Error: Cannot instantiate abstract class const rectangle = new Rectangle(5, 3); console.log(rectangle.displayInfo()); // \u0026#34;Area: 15, Perimeter: 16\u0026#34; Advantages of abstract classes:\nYou can define common behaviors in one place You can force subclasses to implement certain methods Increases code reusability Important features of abstract classes:\nCan contain both abstract and concrete (normal) methods Can be used with generic types Can provide helper functions to subclasses with protected methods Prevent code duplication by gathering common behaviors in one place Abstract Classes vs Interfaces:\nAbstract classes can contain implementation while interfaces only define structure A class can implement multiple interfaces but can only extend one abstract class Abstract classes can contain constructors, interfaces cannot Abstract classes can use access modifiers (private, protected, public) Best Practices Choose Private Properties Correctly\nclass UserService { // TypeScript private: Compile-time check only private _apiUrl: string; // JavaScript private field: Real access restriction #apiKey: string; } Use Parameter Properties Effectively\n// Parameter properties for concise code class Configuration { constructor( private readonly apiUrl: string, private readonly timeout: number, public readonly versionNumber: string ) {} } Use Interfaces Effectively\ninterface DataStore { save(data: any): Promise\u0026lt;void\u0026gt;; retrieve(id: string): Promise\u0026lt;any\u0026gt;; } class PostgreSQLStore implements DataStore { // Implement methods required by interface } class MongoDBStore implements DataStore { // Same interface, different implementation } Choose Access Modifiers Consciously\nclass BankAccount { private _balance: number; // No external access protected _accountNumber: string; // Accessible by subclasses public readonly accountType: string; // Everyone can read but not modify } Conclusion TypeScript classes combine all the features of JavaScript classes with type safety to provide a powerful object-oriented programming experience. Features like access modifiers, readonly properties, getter/setter methods, and abstract classes make your code safer and easier to maintain. When using classes, you can:\nIncrease code security by choosing the right access modifiers Write less code with parameter properties Ensure type safety with interfaces Reduce code duplication with abstract classes In our next article, we\u0026rsquo;ll examine more advanced features of TypeScript. See you then!\n",
        "tags": ["typescript","classes","oop"],
        "categories": ["TypeScript"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 5: The TypeScript Compiler",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part5/",
        "summary": "In this article, we will examine the TypeScript compiler in detail. We will learn how to manage compiler settings, configuration options, and project configuration.",
        "content": "Hello! In this part of our TypeScript series, we will examine how our code is transformed into JavaScript and how we can manage this process. The TypeScript compiler and configuration options are among the important topics that form the foundation of our projects.\nWhat is the TypeScript Compiler? The TypeScript compiler (tsc) is a tool that converts our TypeScript code into JavaScript code that browsers can understand. It transforms the code we write using modern TypeScript features into our targeted JavaScript version.\nStarting the Project: tsc \u0026ndash;init When starting our TypeScript project, our first step is to create a configuration file. We can do this with the tsc --init command:\nnpx tsc --init This command creates a tsconfig.json file in our project\u0026rsquo;s root directory. This file contains the basic settings that determine the compiler\u0026rsquo;s behavior:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es2016\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;strict\u0026#34;: true, \u0026#34;esModuleInterop\u0026#34;: true, \u0026#34;skipLibCheck\u0026#34;: true, \u0026#34;forceConsistentCasingInFileNames\u0026#34;: true } } Development with Watch Mode We can use Watch Mode to instantly compile changes we make to our code during development:\ntsc --watch # or tsc -w Advantages of Watch Mode:\nAutomatically detects file changes Performs instant compilation Shows errors in real-time Speeds up the development process // example.ts let message = \u0026#39;Hello TypeScript!\u0026#39;; console.log(message); // When Watch Mode is active, the JavaScript file // is automatically updated when you save the file Multiple File Compilation The TypeScript compiler can compile all TypeScript files in our project with a single command. When we run the tsc command in the project directory, all .ts files are automatically compiled:\n// user.ts export interface User { id: number; name: string; } // service.ts import { User } from \u0026#39;./user\u0026#39;; export class UserService { // service code } // Both files are compiled with a single command tsconfig.json and Basic Settings Files Option The files option in the tsconfig.json file allows us to explicitly specify which files to compile:\n{ \u0026#34;compilerOptions\u0026#34;: { // other options }, \u0026#34;files\u0026#34;: [\u0026#34;src/main.ts\u0026#34;, \u0026#34;src/utils/helpers.ts\u0026#34;, \u0026#34;src/types/index.d.ts\u0026#34;] } Include and Exclude Options We use include and exclude options to determine which files should be compiled and which should be excluded in our project:\n{ \u0026#34;compilerOptions\u0026#34;: { // other options }, \u0026#34;include\u0026#34;: [ \u0026#34;src/**/*\u0026#34; // all files under src directory ], \u0026#34;exclude\u0026#34;: [ \u0026#34;node_modules\u0026#34;, // exclude node_modules directory \u0026#34;**/*.test.ts\u0026#34;, // exclude test files \u0026#34;src/temp/*\u0026#34; // exclude temporary files ] } This configuration is particularly useful for:\nKeeping the node_modules directory out of compilation Separating test files from production code Excluding specific directories or file types OutDir: Determining Output Directory We specify where the compiled JavaScript files will be saved using the outDir option:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;outDir\u0026#34;: \u0026#34;./dist\u0026#34; } } With this configuration:\nTypeScript files stay in the src folder Compiled JavaScript files are saved to the dist folder Project structure becomes more organized Example project structure:\nproject/ ├── src/ │ ├── index.ts │ └── utils/ │ └── helpers.ts ├── dist/ │ ├── index.js │ └── utils/ │ └── helpers.js └── tsconfig.json Target: JavaScript Version Target The target option determines which JavaScript version our TypeScript code will be compiled to:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es2020\u0026#34; // Other options: \u0026#34;es3\u0026#34;, \u0026#34;es5\u0026#34;, \u0026#34;es6\u0026#34;, \u0026#34;es2016\u0026#34;, \u0026#34;es2017\u0026#34;, \u0026#34;esnext\u0026#34; } } An example showing the effect of different targets:\n// TypeScript code class Animal { constructor(public name: string) {} } // output for target: \u0026#34;es5\u0026#34; var Animal = /** @class */ (function () { function Animal(name) { this.name = name; } return Animal; })(); // output for target: \u0026#34;es2020\u0026#34; class Animal { constructor(name) { this.name = name; } } Strict Mode and Null Checks We can enable strict mode to use TypeScript\u0026rsquo;s type safety features at the highest level:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;strict\u0026#34;: true, \u0026#34;strictNullChecks\u0026#34;: true } } Effect of these settings:\n// strict: false let name: string; name = null; // No problem // strict: true and strictNullChecks: true let name: string; name = null; // Error! Type \u0026#39;null\u0026#39; is not assignable to type \u0026#39;string\u0026#39; // Correct usage let name: string | null; name = null; // Now it works Including JavaScript Files In some projects, we may need to use JavaScript and TypeScript files together. The allowJs option makes this possible:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;allowJs\u0026#34;: true, \u0026#34;checkJs\u0026#34;: true // Performs type checking in JavaScript files } } This feature is particularly useful when:\nConverting existing JavaScript projects to TypeScript Working with third-party JavaScript libraries Doing gradual TypeScript adaptation Example usage:\n// utils.js (JavaScript file) export function add(a, b) { return a + b; } // index.ts (TypeScript file) import { add } from \u0026#39;./utils.js\u0026#39;; const result = add(5, 3); // TypeScript type checking works Best Practices Adjust Configuration According to Project\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es2020\u0026#34;, // For modern browsers \u0026#34;module\u0026#34;: \u0026#34;esnext\u0026#34;, // Modern module system \u0026#34;strict\u0026#34;: true, // Strict type checking \u0026#34;outDir\u0026#34;: \u0026#34;./dist\u0026#34;, // Output directory \u0026#34;rootDir\u0026#34;: \u0026#34;./src\u0026#34; // Source directory } } Use Watch Mode Effectively\n# package.json { \u0026#34;scripts\u0026#34;: { \u0026#34;dev\u0026#34;: \u0026#34;tsc --watch\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;tsc\u0026#34; } } Keep Project Structure Organized\nproject/ ├── src/ # TypeScript source files ├── dist/ # Compiled JavaScript files ├── tests/ # Test files ├── tsconfig.json # TypeScript configuration └── package.json Appropriate Include/Exclude Configuration\n{ \u0026#34;include\u0026#34;: [\u0026#34;src/**/*\u0026#34;], \u0026#34;exclude\u0026#34;: [\u0026#34;node_modules\u0026#34;, \u0026#34;**/*.test.ts\u0026#34;, \u0026#34;**/*.spec.ts\u0026#34;] } Conclusion The TypeScript compiler and configuration options are an important part of modern web development processes. With proper configuration, you can:\nWrite safer code Speed up the development process Better organize project structure Make team collaboration easier In our next article, we\u0026rsquo;ll examine more advanced features of TypeScript. See you then!\n",
        "tags": ["typescript","compiler","configuration"],
        "categories": ["TypeScript"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 4: Interfaces",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part4/",
        "summary": "In this article, we will examine interfaces, one of the important features of TypeScript, in detail. We will understand their differences from Type Aliases and learn practical use cases.",
        "content": "Hello! In this part of our TypeScript series, we will examine interfaces in detail, one of the most powerful features of the type system. We will learn what interfaces are, why we should use them, and how they provide benefits in real-world applications.\nWhat is an Interface and Why Should We Use It? Interfaces are a feature in TypeScript that allows us to define the structure of objects. To explain it in simple terms:\n\u0026ldquo;Think of it like drawing a construction plan. In this plan, you have details like how many floors the building will have, room dimensions, window locations, etc. An interface is just like that - it draws a \u0026lsquo;plan\u0026rsquo; for an object. We determine in advance what properties it will have and what types these properties will be.\u0026rdquo;\nHere\u0026rsquo;s a simple example:\n// Simple interface definition interface User { // Required properties id: number; // Unique number of the user name: string; // Name of the user email: string; // Email address // Optional properties (marked with ?) phone?: string; // Phone number (optional) age?: number; // Age information (optional) } // Using the interface const newUser: User = { id: 1, name: \u0026#39;John Smith\u0026#39;, email: \u0026#39;john@example.com\u0026#39;, // phone and age are optional, so we don\u0026#39;t have to write them }; // TypeScript protects us here const invalidUser: User = { id: \u0026#39;1\u0026#39;, // Error! string value cannot be assigned to number type name: \u0026#39;Michael\u0026#39;, email: true, // Error! boolean value cannot be assigned to string type }; Advantages of Using Interfaces Code Safety: // Function defined with interface function updateUser(id: number, user: User) { // Thanks to TypeScript, we\u0026#39;re sure about the structure of the user object console.log(`Updating ${user.name}...`); // ... update operations } // This works updateUser(1, { id: 1, name: \u0026#39;John\u0026#39;, email: \u0026#39;john@example.com\u0026#39;, }); // This gives an error - missing and incorrect properties updateUser(1, { id: 1, firstName: \u0026#39;John\u0026#39;, // Error! should be \u0026#39;name\u0026#39; instead of \u0026#39;firstName\u0026#39; }); Code Completion: Interfaces enhance your IDE\u0026rsquo;s code completion feature. When accessing an object\u0026rsquo;s properties, the IDE shows you all available properties.\nDocumentation: Interfaces also serve as documentation. Other developers reading your code can quickly understand the structure of an object.\nInterface vs Type: Understanding the Differences Let\u0026rsquo;s examine the differences between Interface and Type with real examples:\n1. Declaration Merging // Declaration merging is possible with interfaces interface Car { brand: string; } interface Car { // We can add new properties with the same name model: string; } const tesla: Car = { brand: \u0026#39;Tesla\u0026#39;, // Properties from both model: \u0026#39;Model 3\u0026#39;, // interfaces are required }; // This is not possible with Type type Bicycle = { brand: string; }; // Error! Identifier \u0026#39;Bicycle\u0026#39; has already been declared type Bicycle = { model: string; }; This feature is especially useful when developing libraries. Users can add new properties to existing interfaces.\n2. Extends and Implements Interfaces are very useful in object-oriented programming:\n// Base interface interface Animal { name: string; species: string; makeSound(): void; } // Deriving from interface interface Cat extends Animal { hasPaws: boolean; canClimb: boolean; } // Usage in classes class TabbyCat implements Cat { // We must implement all properties and methods name: string; species: string = \u0026#39;Cat\u0026#39;; hasPaws: boolean = true; canClimb: boolean = true; constructor(name: string) { this.name = name; } makeSound() { console.log(\u0026#39;Meow!\u0026#39;); } } Interface Methods and Properties Interfaces can define not only data structure but also object behaviors. Let\u0026rsquo;s proceed with a real example:\n// Interface for product management in an e-commerce system interface ProductManagement { // Basic CRUD operations addProduct(product: Product): Promise\u0026lt;boolean\u0026gt;; updateProduct(id: string, product: Product): Promise\u0026lt;boolean\u0026gt;; deleteProduct(id: string): Promise\u0026lt;boolean\u0026gt;; getProduct(id: string): Promise\u0026lt;Product | null\u0026gt;; // Stock management methods updateStock(productId: string, quantity: number): Promise\u0026lt;void\u0026gt;; checkStock(productId: string): Promise\u0026lt;number\u0026gt;; // Statistics methods generateSalesReport(start: Date, end: Date): Promise\u0026lt;SalesReport\u0026gt;; } // A class using this interface class PostgreSQLProductManagement implements ProductManagement { constructor(private db: Database) {} async addProduct(product: Product): Promise\u0026lt;boolean\u0026gt; { try { await this.db.query(\u0026#39;INSERT INTO products (id, name, price, stock) VALUES ($1, $2, $3, $4)\u0026#39;, [ product.id, product.name, product.price, product.stock, ]); return true; } catch (error) { console.error(\u0026#39;Error adding product:\u0026#39;, error); return false; } } // Implementation of other methods... } Readonly and Optional Properties We can make some properties in interfaces readonly or optional. Here\u0026rsquo;s a real scenario:\n// Interface for user profile interface UserProfile { // Readonly properties (cannot be changed) readonly id: string; // User ID never changes readonly registrationDate: Date; // Registration date cannot be changed // Required properties email: string; // Email address name: string; // Username // Optional properties phone?: string; // Phone number address?: { // Address information city: string; district: string; postalCode?: string; }; profilePicture?: string; // Profile picture URL // Social media information (all optional) socialMedia?: { twitter?: string; linkedin?: string; github?: string; }; } // Usage example const newProfile: UserProfile = { id: \u0026#39;usr_123\u0026#39;, // readonly, can only be assigned during initialization registrationDate: new Date(), // readonly, can only be assigned during initialization email: \u0026#39;john@example.com\u0026#39;, name: \u0026#39;John Smith\u0026#39;, // We don\u0026#39;t have to add optional fields }; // INCORRECT USAGE - cannot modify readonly properties newProfile.id = \u0026#39;usr_456\u0026#39;; // Error! readonly property cannot be modified newProfile.registrationDate = new Date(); // Error! readonly property cannot be modified // We can add optional properties later newProfile.phone = \u0026#39;555-0123\u0026#39;; newProfile.address = { city: \u0026#39;New York\u0026#39;, district: \u0026#39;Manhattan\u0026#39;, }; Generic Interfaces Generics make interfaces more flexible and reusable. For example, a generic interface for API responses:\n// Generic API response interface interface APIResponse\u0026lt;T\u0026gt; { success: boolean; // Is the operation successful? data: T; // Generic data type timestamp: number; // Operation time statusCode: number; // HTTP status code message?: string; // Optional message errors?: string[]; // Error messages if any } // Usage with different data types interface User { id: number; name: string; email: string; } interface Product { id: number; name: string; price: number; } // API response for user list const usersResponse: APIResponse\u0026lt;User[]\u0026gt; = { success: true, data: [ { id: 1, name: \u0026#39;John\u0026#39;, email: \u0026#39;john@example.com\u0026#39; }, { id: 2, name: \u0026#39;Michael\u0026#39;, email: \u0026#39;michael@example.com\u0026#39; }, ], timestamp: Date.now(), statusCode: 200, }; // API response for a single product const productResponse: APIResponse\u0026lt;Product\u0026gt; = { success: false, data: { id: 0, name: \u0026#39;\u0026#39;, price: 0 }, // Empty product timestamp: Date.now(), statusCode: 404, message: \u0026#39;Product not found\u0026#39;, errors: [\u0026#39;Product with specified ID does not exist\u0026#39;], }; Interface Inheritance Interfaces can inherit from each other. This feature prevents code repetition and helps us create a modular structure:\n// Basic entity properties interface Entity { id: string; // Unique identifier createdAt: Date; // Creation date updatedAt: Date; // Last update date deleted: boolean; // Deletion status } // Basic person information interface Person extends Entity { firstName: string; lastName: string; email: string; phone?: string; } // Student information interface Student extends Person { studentId: string; department: string; year: number; courses: string[]; gpa?: number; } // Teacher information interface Teacher extends Person { employeeId: string; subject: string; coursesTaught: string[]; salary: number; } // Usage example const newStudent: Student = { // Properties from Entity id: \u0026#39;std_123\u0026#39;, createdAt: new Date(), updatedAt: new Date(), deleted: false, // Properties from Person firstName: \u0026#39;John\u0026#39;, lastName: \u0026#39;Smith\u0026#39;, email: \u0026#39;john@school.edu\u0026#39;, // Student-specific properties studentId: \u0026#39;2024001\u0026#39;, department: \u0026#39;Computer Engineering\u0026#39;, year: 2, courses: [\u0026#39;Algorithms\u0026#39;, \u0026#39;Data Structures\u0026#39;, \u0026#39;TypeScript 101\u0026#39;], }; Common Use Cases for Interfaces Interfaces appear in many areas of software development. Here are the most common use cases:\n1. API Communication We use interfaces to define incoming and outgoing data structures when working with APIs:\n// Structure of user data coming from API interface UserResponse { id: number; name: string; email: string; role: \u0026#39;admin\u0026#39; | \u0026#39;user\u0026#39;; lastLogin?: Date; } // Login information to be sent to API interface LoginCredentials { email: string; password: string; remember?: boolean; } 2. Database Operations We use interfaces to model table structures and query results in database operations:\ninterface Product { id: number; // Product unique number name: string; // Product name price: number; // Product price stock: number; // Stock quantity category: string; // Product category } 3. Form Management Interfaces help us when defining form data structure and validation rules:\ninterface RegistrationForm { email: string; // User email password: string; // Password passwordConfirm: string; // Password confirmation username: string; // Username } 4. Configuration Management We use interfaces when defining application settings and configurations:\ninterface AppSettings { apiUrl: string; // API server address maxAttempts: number; // Maximum number of attempts timeout: number; // Timeout duration debug: boolean; // Debug mode on/off } 5. Data Models We use interfaces when defining basic data structures in our application:\ninterface Order { id: string; // Order number customerId: string; // Customer number products: string[]; // Products in order totalAmount: number; // Total amount status: \u0026#39;pending\u0026#39; | \u0026#39;confirmed\u0026#39; | \u0026#39;cancelled\u0026#39;; // Order status } These use cases show how important interfaces are in terms of type safety and code organization. In each area, interfaces help make our code safer and easier to maintain.\nBest Practices and Tips There are some important points to consider when using TypeScript interfaces. By applying these practices, we can write more readable and maintainable code.\n1. Naming Conventions Interface names should clearly indicate what they do and follow certain standards:\n// ✅ Good Naming Examples interface UserService { getUser(id: string): Promise\u0026lt;User\u0026gt;; } interface ProductRepository { updateStock(productId: string, quantity: number): void; } // ❌ Naming Conventions to Avoid interface IUser { // Don\u0026#39;t use \u0026#39;I\u0026#39; prefix // ... } interface dataManager { // Use PascalCase // ... } interface DATA_SERVICE { // Don\u0026#39;t use UPPERCASE // ... } 2. Single Responsibility Principle Each interface should focus on a single task and do it well:\n// ✅ Well-Designed Interfaces interface IdentityInformation { id: string; ssn: string; passportNo?: string; } interface ContactInformation { email: string; phone?: string; address?: { state: string; city: string; }; } // Combine when needed interface User extends IdentityInformation, ContactInformation { firstName: string; lastName: string; } // ❌ Situation to Avoid: Putting everything in one interface interface HugeInterface { // Identity information id: string; ssn: string; // Contact information email: string; phone: string; // Address information state: string; city: string; // User information firstName: string; lastName: string; // Other information... // ... and many more properties } 3. Adding Descriptive Comments Add comments that clearly indicate how interfaces should be used:\n/** * Card information to be used during payment process. * This interface should only be used during payment processing and * sensitive information should be cleared from memory after the transaction. */ interface PaymentInformation { /** Amount to be paid (in USD, with cent precision) */ amount: number; /** 16-digit card number */ cardNumber: string; /** Expiration date in MM/YY format (example: 12/25) */ expirationDate: string; /** 3-digit security code on the back of the card */ securityCode: string; } 4. Using Optional Properties Correctly Be careful when using optional properties and document them:\ninterface UserProfile { // Required fields id: string; name: string; email: string; // Optional fields - explain why they\u0026#39;re optional /** User may choose not to provide a phone number */ phone?: string; /** Default will be used if no profile photo is uploaded */ profilePhotoUrl?: string; /** User may not have given location permission yet */ location?: { lat: number; lng: number; }; } Conclusion Interfaces are one of the most powerful features of TypeScript, and when used correctly, they:\nMake your code more readable Provide type safety Make maintenance easier Improve team collaboration Serve as documentation The examples and best practices we\u0026rsquo;ve seen in this article will guide you in your daily TypeScript development. See you in the next article.\nFeel free to leave comments if you have any questions. Happy coding! 🚀\n",
        "tags": ["typescript","interfaces","type-system"],
        "categories": ["TypeScript"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 3: Object Types",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part3/",
        "summary": "In this article, we will explore object types in TypeScript in detail. We will learn about object definitions, type aliases, and working with nested objects.",
        "content": "Hello! In this part of our TypeScript series, we will examine object types in detail. We\u0026rsquo;ll particularly focus on how object types are used in functions and why type aliases are important.\nWorking with Object Annotations In TypeScript, we can use two different approaches when defining object parameters in functions. First, we can define the object type directly within the function parameter:\n// Defining object type directly in parameter const printName = (name: { first: string; last: string }) =\u0026gt; { return `Name: ${name.first} ${name.last}`; }; // Usage printName({ first: \u0026#39;Will\u0026#39;, last: \u0026#39;Ferrell\u0026#39; }); While this approach is useful for simple objects, it can reduce readability and lead to code duplication in complex object structures. Especially if you\u0026rsquo;re going to use the same object type in multiple places, it\u0026rsquo;s recommended to use type aliases instead of this approach.\nUsing Curly Braces The use of curly braces when defining object types in function parameters can sometimes be confusing:\n// Syntax that might be confusing const printPerson = (person: { name: string; age: number }): { info: string } =\u0026gt; { return { info: `${person.name} is ${person.age} years old` }; }; // More readable version - Using Type Alias type Person = { name: string; age: number; }; type PersonInfo = { info: string; }; const printPerson2 = (person: Person): PersonInfo =\u0026gt; { return { info: `${person.name} is ${person.age} years old` }; }; The second approach is more readable because:\nType definitions are separated from function definition Types are reusable Code is more organized and easier to maintain Using Type Aliases Type aliases allow us to define object types separately and reuse these types throughout our code:\n// Defining type alias type Person = { name: string; age: number; }; // Using in function const sayHappyBirthday = (person: Person) =\u0026gt; { return `Hey ${person.name}, congrats on turning ${person.age}!`; }; // Using in variable const jerry: Person = { name: \u0026#39;Jerry\u0026#39;, age: 42, }; sayHappyBirthday(jerry); Advantages of using type aliases:\nPrevents code duplication Keeps type definitions in a central location Allows managing changes from a single point Improves code readability Nested Objects Defining nested object structures is quite common in TypeScript. Here\u0026rsquo;s an example:\nconst describePerson = (person: { name: string; age: number; parentNames: { mom: string; dad: string; }; }) =\u0026gt; { return `Person: ${name}, Age: ${age}, Parents: ${parentNames.mom}, ${parentNames.dad}`; }; // Usage describePerson({ name: \u0026#39;Jimmy\u0026#39;, age: 10, parentNames: { mom: \u0026#39;Kim\u0026#39;, dad: \u0026#39;Steve\u0026#39;, }, }); We can make this structure more organized using type aliases:\ntype ParentNames = { mom: string; dad: string; }; type PersonWithParents = { name: string; age: number; parentNames: ParentNames; }; const describePerson2 = (person: PersonWithParents) =\u0026gt; { const { name, age, parentNames } = person; return `Person: ${name}, Age: ${age}, Parents: ${parentNames.mom}, ${parentNames.dad}`; }; Excess Properties TypeScript warns you when you try to use properties that aren\u0026rsquo;t defined in an object type:\ntype BasicPerson = { name: string; age: number; }; // Will cause error const person: BasicPerson = { name: \u0026#39;John\u0026#39;, age: 30, location: \u0026#39;New York\u0026#39;, // Excess property error }; // Correct usage const personData = { name: \u0026#39;John\u0026#39;, age: 30, location: \u0026#39;New York\u0026#39;, }; const person2: BasicPerson = personData; // This works Optional Properties Sometimes we might want some properties in an object type to be optional:\ntype OptionalPerson = { name: string; age: number; phone?: string; // Optional property email?: string; // Optional property }; // Both usages are valid const person1: OptionalPerson = { name: \u0026#39;Alice\u0026#39;, age: 25, }; const person2: OptionalPerson = { name: \u0026#39;Bob\u0026#39;, age: 30, phone: \u0026#39;555-0123\u0026#39;, email: \u0026#39;bob@email.com\u0026#39;, }; Readonly Modifier In TypeScript, the readonly modifier is used to prevent properties of an object from being modified. This is very useful for maintaining data integrity and preventing unwanted changes:\ntype Person = { readonly name: string; readonly age: number; }; const john: Person = { name: \u0026#39;John\u0026#39;, age: 30, }; // The following lines will cause compilation errors // john.name = \u0026#34;Johnny\u0026#34;; // Error: Cannot assign to \u0026#39;name\u0026#39; because it is a read-only property // john.age = 31; // Error: Cannot assign to \u0026#39;age\u0026#39; because it is a read-only property The readonly modifier allows value assignment during object creation but prevents these properties from being modified later.\nReadonly Array The readonly modifier can also be used for arrays. This prevents the array\u0026rsquo;s contents from being modified:\nconst numbers: readonly number[] = [1, 2, 3, 4, 5]; // The following methods can no longer be used // numbers.push(6); // Error // numbers.pop(); // Error // numbers[2] = 10; // Error Intersection Types Intersection types allow us to combine multiple types to create a new type:\ntype Employee = { employeeId: number; department: string; }; type Person = { name: string; age: number; }; // A new type combining two types type EmployeePerson = Employee \u0026amp; Person; const worker: EmployeePerson = { employeeId: 1234, department: \u0026#39;Engineering\u0026#39;, name: \u0026#39;Alice\u0026#39;, age: 30, }; Intersection types are very useful when creating complex object structures. You can combine properties from multiple types into a single type.\nIntersection Type Example Let\u0026rsquo;s look at a simple example showing how intersection types can be used:\n// Type containing address information type Address = { street: string; city: string; country: string; }; // Type containing contact information type Contact = { email: string; phone: string; }; // Combining types to create a complete user profile type UserProfile = Person \u0026amp; Address \u0026amp; Contact; // Usage example: const user: UserProfile = { name: \u0026#39;John\u0026#39;, age: 30, street: \u0026#39;Main Street\u0026#39;, city: \u0026#39;New York\u0026#39;, country: \u0026#39;USA\u0026#39;, email: \u0026#39;john@email.com\u0026#39;, phone: \u0026#39;555-0123\u0026#39; }; // Example usage in a function function displayUserInfo(user: UserProfile) { console.log(` User: ${user.name} Age: ${user.age} Address: ${user.street}, ${user.city}, ${user.country} Contact: ${user.email}, ${user.phone} `); } displayUserInfo(user); This example demonstrates how intersection types can be used to combine different properties:\nType Composition: We define separate types for different purposes (Person, Address, Contact) Intersection Types: We combine these types using the \u0026amp; operator to create a more comprehensive type Modularity: Each type has its own responsibility and can be managed separately Reusability: These types can be used elsewhere in our code This pattern is particularly useful when you want to:\nLogically separate different groups of data Make your code more modular Reuse type definitions Organize complex data structures Array Types Array types in TypeScript can be defined in several ways:\n// First method: Using square brackets const numbers: number[] = [1, 2, 3, 4, 5]; // Second method: Using Generic Array type const strings: Array\u0026lt;string\u0026gt; = [\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;]; // Mixed type array const mixed: (number | string)[] = [1, \u0026#39;two\u0026#39;, 3, \u0026#39;four\u0026#39;]; // Tuple (Fixed-length array with different types) const employee: [number, string] = [1, \u0026#39;John Doe\u0026#39;]; // Readonly array const readonlyNumbers: readonly number[] = [1, 2, 3]; Array Methods and Type Inference TypeScript performs smart type inference with array methods:\nconst numbers = [1, 2, 3, 4, 5]; const doubled = numbers.map((x) =\u0026gt; x * 2); // doubled type is number[] const names = [\u0026#39;Alice\u0026#39;, \u0026#39;Bob\u0026#39;, \u0026#39;Charlie\u0026#39;]; const upperNames = names.map((name) =\u0026gt; name.toUpperCase()); // upperNames type is string[] Multi-Dimensional Arrays In TypeScript, multi-dimensional arrays are used to create nested arrays or matrix-like data structures:\n// 2D number array const matrix: number[][] = [ [1, 2, 3], [4, 5, 6], [7, 8, 9], ]; // 3D array example const threeDimensionalArray: number[][][] = [ [ [1, 2], [3, 4], ], [ [5, 6], [7, 8], ], [ [9, 10], [11, 12], ], ]; // Mixed type 2D array const mixedMatrix: (number | string)[][] = [ [1, \u0026#39;two\u0026#39;, 3], [\u0026#39;four\u0026#39;, 5, \u0026#39;six\u0026#39;], ]; // Operations on multi-dimensional array const sumMatrix = (matrix: number[][]): number =\u0026gt; { return matrix.flat().reduce((sum, num) =\u0026gt; sum + num, 0); }; console.log(sumMatrix(matrix)); // Returns sum of all elements // Checking array dimensions const printMatrixInfo = (matrix: number[][]) =\u0026gt; { console.log(`Matrix size: ${matrix.length} x ${matrix[0].length}`); }; printMatrixInfo(matrix); // Outputs \u0026#34;Matrix size: 3 x 3\u0026#34; Multi-dimensional arrays are frequently used in areas such as image processing, game development, and scientific calculations. TypeScript provides strong type checking in such complex array structures.\nConclusion TypeScript\u0026rsquo;s object types features increase your code\u0026rsquo;s type safety and help you write clearer, easier-to-debug code. The readonly modifier, intersection types, flexible array types, and multi-dimensional arrays are important parts of TypeScript\u0026rsquo;s powerful type system.\nSee you in our next article!\n",
        "tags": ["typescript","javascript","object-types"],
        "categories": ["TypeScript"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 2: Functions",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part2/",
        "summary": "In this article, we\u0026rsquo;ll explore TypeScript functions in detail, covering parameter types, return types, and special function types that make TypeScript unique and powerful.",
        "content": "Hello everyone! In our previous article, we covered the basic types in TypeScript. Today, we\u0026rsquo;ll dive into functions and how TypeScript makes them more powerful and safer to use. We\u0026rsquo;ll start with the basics and gradually move to more advanced concepts.\nFunction Parameter Types In TypeScript, we can specify the type of function parameters, which helps prevent errors by catching them before runtime. Let\u0026rsquo;s look at a simple example:\n// Creating a function with typed arguments const encourageStudent = (name: string) =\u0026gt; { return `Hey, ${name}, you\u0026#39;re doing GREAT!`; }; // This works fine encourageStudent(\u0026#39;you\u0026#39;); // Output: \u0026#34;Hey, you, you\u0026#39;re doing GREAT!\u0026#34; // This will cause a TypeScript error encourageStudent(85); // Error: Argument of type \u0026#39;number\u0026#39; is not assignable to parameter of type \u0026#39;string\u0026#39; The type annotation after each parameter (: string in the example) tells TypeScript what type of values the function expects. This helps us catch errors during development rather than at runtime.\nMultiple Parameters Functions can have multiple parameters, and each parameter can have its own type:\nfunction createUser(name: string, age: number, isActive: boolean) { return { name, age, isActive, }; } // Correct usage createUser(\u0026#39;John\u0026#39;, 25, true); // TypeScript will catch these errors createUser(\u0026#39;John\u0026#39;, \u0026#39;25\u0026#39;, true); // Error: age should be a number createUser(\u0026#39;John\u0026#39;); // Error: missing parameters createUser(\u0026#39;John\u0026#39;, 25, \u0026#39;yes\u0026#39;); // Error: isActive should be boolean In this example, TypeScript ensures that:\nAll required parameters are provided Each parameter is of the correct type Parameters are passed in the correct order Function Return Types TypeScript can also specify what type of value a function returns. While TypeScript can often infer the return type (called type inference), explicitly declaring it can make your code more maintainable and self-documenting:\nconst addNums = (x: number, y: number): number =\u0026gt; { return x + y; }; const concatenateStrings = (a: string, b: string): string =\u0026gt; { return a + \u0026#39; \u0026#39; + b; }; addNums(5, 5); // Returns: 10 concatenateStrings(\u0026#39;Hello\u0026#39;, \u0026#39;World\u0026#39;); // Returns: \u0026#34;Hello World\u0026#34; The : number and : string after the parameter lists indicate what type the functions must return. This helps in:\nProviding documentation about what the function returns Catching errors if you try to return the wrong type Letting other developers know what to expect from the function The void Return Type Sometimes functions don\u0026rsquo;t return any value, they just perform an action. In TypeScript, we use the void type to indicate this:\nconst warnUser = (message: string): void =\u0026gt; { alert(message); // No return statement needed }; const logData = (data: any): void =\u0026gt; { console.log(data); // We didn\u0026#39;t even write \u0026#39;return\u0026#39; }; // TypeScript will error if you try to use the return value const result = logData(\u0026#39;test\u0026#39;); // Error: Type \u0026#39;void\u0026#39; is not assignable... Using void is important because:\nIt tells other developers not to expect a return value TypeScript will error if you try to return a value It makes your APIs clearer and more predictable Optional Parameters and Default Values TypeScript provides two ways to make parameters flexible: optional parameters and default values.\nOptional Parameters Add a ? after the parameter name to make it optional:\nfunction greetPerson(name: string, title?: string) { if (title) { return `Hello ${title} ${name}`; } return `Hello ${name}`; } greetPerson(\u0026#39;John\u0026#39;); // Output: \u0026#34;Hello John\u0026#34; greetPerson(\u0026#39;John\u0026#39;, \u0026#39;Dr.\u0026#39;); // Output: \u0026#34;Hello Dr. John\u0026#34; Default Values Assign a value in the parameter declaration to set a default:\nfunction orderCoffee(type: string = \u0026#39;Americano\u0026#39;, size: string = \u0026#39;medium\u0026#39;, milk: boolean = false) { let order = `${size} ${type}`; if (milk) order += \u0026#39; with milk\u0026#39;; return order; } orderCoffee(); // \u0026#34;medium Americano\u0026#34; orderCoffee(\u0026#39;Latte\u0026#39;); // \u0026#34;medium Latte\u0026#34; orderCoffee(\u0026#39;Espresso\u0026#39;, \u0026#39;small\u0026#39;); // \u0026#34;small Espresso\u0026#34; orderCoffee(\u0026#39;Mocha\u0026#39;, \u0026#39;large\u0026#39;, true); // \u0026#34;large Mocha with milk\u0026#34; Key differences between optional parameters and default values:\nOptional Parameters (?)\nParameter becomes undefined if not provided Requires checks in the function body More flexible but requires more handling Default Values (= value)\nUses specified value if parameter is omitted No extra checks needed Less flexible but easier to use Important: Parameter Order When using both required and optional parameters, required parameters must come first:\n// CORRECT function correct(required: string, optional?: string) {} // WRONG - TypeScript will error function wrong(optional?: string, required: string) {} // Error! Anonymous Functions and Type Inference TypeScript is particularly good at inferring types in anonymous functions, especially in callbacks:\nconst numbers = [1, 2, 3, 4, 5]; // TypeScript automatically infers \u0026#39;number\u0026#39; type for \u0026#39;num\u0026#39; numbers.forEach((num) =\u0026gt; { console.log(num.toFixed(2)); // Works because TypeScript knows num is a number }); // Type inference in array methods const squares = numbers.map((num) =\u0026gt; num * num); // squares is inferred as number[] The never Type The never type is special in TypeScript and represents values that never occur. It has two main use cases:\nFunctions that never complete: function infiniteLoop(): never { while (true) { console.log(\u0026#34;I\u0026#39;m still going!\u0026#34;); } } function infiniteRecursion(): never { return infiniteRecursion(); } Functions that always throw errors: function throwError(message: string): never { throw new Error(message); } function validateUser(user: never): never { throw new Error(\u0026#39;Should never be called with a value\u0026#39;); } Don\u0026rsquo;t confuse never with void:\nvoid returns undefined or null (technically still a value) never means the function never completes execution Function Overloads TypeScript allows you to define multiple function signatures for different parameter types:\n// Overload signatures function combine(a: string, b: string): string; function combine(a: number, b: number): number; // Implementation function combine(a: string | number, b: string | number): string | number { if (typeof a === \u0026#39;string\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;string\u0026#39;) { return a.concat(b); } if (typeof a === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;number\u0026#39;) { return a + b; } throw new Error(\u0026#39;Parameters must be of the same type!\u0026#39;); } console.log(combine(\u0026#39;Hello, \u0026#39;, \u0026#39;World\u0026#39;)); // \u0026#34;Hello, World\u0026#34; console.log(combine(5, 10)); // 15 // combine(\u0026#34;5\u0026#34;, 10); // Error! This combination isn\u0026#39;t defined Best Practices Always Type Parameters\n// BAD function bad(name) { return `Hello ${name}`; } // GOOD function good(name: string): string { return `Hello ${name}`; } Consider Return Types\n// Type inference is sometimes enough const add = (a: number, b: number) =\u0026gt; a + b; // But explicit return types are better for complex functions function processData(data: any[]): ProcessedData { // Complex operations... return processedResult; } Use Optional Parameters Wisely\nPut required parameters before optional ones Consider using default values instead of optional parameters when appropriate Document the behavior of optional parameters Avoid any Type\n// BAD function processAny(data: any) { return data.someMethod(); // Dangerous! } // GOOD function processTyped\u0026lt;T\u0026gt;(data: T) { // Type-safe operations } Quick Reference Here\u0026rsquo;s a quick reference of function types in TypeScript:\n// Basic function with parameter and return types function basic(param: string): number {} // Arrow function with type annotations const arrow = (x: number): string =\u0026gt; {}; // Optional parameter function optional(name: string, age?: number) {} // Default value function defaultValue(name: string = \u0026#39;Anonymous\u0026#39;) {} // Void return type function noReturn(): void {} // Never return type function neverReturns(): never {} // Function overloads function overloaded(x: string): string; function overloaded(x: number): number; Conclusion TypeScript\u0026rsquo;s function features provide powerful tools for writing safer and more maintainable code. Through type checking, we can catch errors early and make our code more self-documenting. While it might seem like extra work at first, the benefits become clear as your projects grow in size and complexity.\n",
        "tags": ["typescript","javascript","functions"],
        "categories": ["TypeScript"],
        "lang": "en"
    },{
        "title": "Understanding TypeScript - Part 1: The Basics",
        "permalink": "/blog/en/typescript-series/understanding-typescript-part1/",
        "summary": "TypeScript is a powerful superset of JavaScript that adds static typing to the language. In this article series, we\u0026rsquo;ll explore TypeScript from basics to advanced concepts, starting with fundamental types and type inference.",
        "content": "Hello everyone! As a developer who has recently dived deep into TypeScript, I want to share my learning journey with you. In this series, we\u0026rsquo;ll explore TypeScript from the ground up, understanding why it exists and how it can make our JavaScript development experience better.\nWhat is TypeScript? TypeScript is JavaScript with types. That\u0026rsquo;s the simplest way to describe it, but there\u0026rsquo;s much more to it. It\u0026rsquo;s a superset of JavaScript developed by Microsoft that adds static typing to the language. This means that all valid JavaScript code is also valid TypeScript code, but TypeScript adds additional features that help us write more robust and maintainable code.\nLet\u0026rsquo;s look at a simple example of how TypeScript differs from JavaScript:\n// JavaScript let greeting = \u0026#34;Hello\u0026#34;; greeting = 42; // This works in JavaScript, but might cause problems // TypeScript let greeting: string = \u0026#34;Hello\u0026#34;; greeting = 42; // Error: Type \u0026#39;number\u0026#39; is not assignable to type \u0026#39;string\u0026#39; Why Types? TypeScript\u0026rsquo;s type system offers several key benefits:\nHelps us find errors: The type system can catch bugs before our code runs Analyzes our code as we type: Provides real-time feedback in our editor Only exists in development: Types are removed when code is compiled to JavaScript These features make TypeScript particularly valuable for large codebases and team collaborations.\nBasic Types in TypeScript Let\u0026rsquo;s explore the fundamental types in TypeScript:\n1. Strings Strings represent text values in TypeScript. Here\u0026rsquo;s how we work with them:\n// Declaring a string variable let myString: string = \u0026#34;Words!!!\u0026#34;; // CAN\u0026#39;T reassign to a different type myString = 100; // Error // CAN reassign to a value of same type myString = \u0026#34;New words!!!\u0026#34;; // Works fine In this example, once we declare myString as a string, TypeScript ensures we can only assign string values to it.\n2. Numbers TypeScript simplifies number handling compared to other languages:\n// Declaring a number variable let myNumber: number = 42; // CAN\u0026#39;T reassign to a different type myNumber = \u0026#34;I\u0026#39;m a string!\u0026#34;; // Error // CAN reassign to a value of same type myNumber = 60; // Works fine Unlike some programming languages that have multiple number types (float, int, etc.), TypeScript (like JavaScript) just uses the number type for all numeric values.\n3. Booleans Boolean values represent true/false conditions:\n// Declaring a boolean variable const myBoolean: boolean = true; // CAN\u0026#39;T reassign to a different type myBoolean = 87; // Error // CAN reassign to a value of same type myBoolean = false; // Works fine Type Inference One of TypeScript\u0026rsquo;s most powerful features is its ability to infer types automatically. This means you don\u0026rsquo;t always have to explicitly declare types:\n// Creating a variable with a value, // but without a type annotation let x = 27; x = \u0026#39;Twenty-seven\u0026#39;; // Error - Type \u0026#39;string\u0026#39; is not assignable to type \u0026#39;number\u0026#39; In this example, TypeScript automatically infers that x is a number based on its initial value. This feature helps us write more concise code while maintaining type safety.\nThe \u0026lsquo;any\u0026rsquo; Type Sometimes we need more flexibility than strict typing allows. That\u0026rsquo;s where the any type comes in:\n// Declaring a variable with type \u0026#39;any\u0026#39; const myComplicatedData: any = \u0026#34;I\u0026#39;m going to be complicated!\u0026#34;; // CAN reassign to any type - type checks are off! myComplicatedData = 87; // Works myComplicatedData = \u0026#39;abc...\u0026#39;; // Works myComplicatedData = true; // Works When Should We Use \u0026lsquo;any\u0026rsquo;? While it\u0026rsquo;s generally recommended to avoid any, there are legitimate use cases for it. Here\u0026rsquo;s a real-world scenario:\n// Working with external API data function handleAPIResponse(response: any) { // We might not know the exact structure of the API response // especially when working with third-party APIs console.log(response.data); // Works console.log(response.status); // Works console.log(response.someField); // Works } // Working with legacy JavaScript code declare const oldJavaScriptLibrary: any; // We can use the library without TypeScript errors oldJavaScriptLibrary.someOldMethod(); Common scenarios where any might be necessary:\nWhen integrating with external APIs where the response structure is unknown or dynamic During migration from JavaScript to TypeScript (temporary usage) When working with third-party libraries that don\u0026rsquo;t have TypeScript type definitions When dealing with truly dynamic content where the type cannot be predicted However, remember that using any removes all the benefits of TypeScript\u0026rsquo;s type checking. It should be used as a last resort, and you should always try to define proper types when possible.\nBest Practices Let TypeScript Infer When Possible\nDon\u0026rsquo;t add type annotations when TypeScript can infer the type correctly This makes your code cleaner and more maintainable Avoid \u0026lsquo;any\u0026rsquo;\nUsing any removes all the benefits of TypeScript Only use it when you have a very specific reason to do so Be Explicit When Necessary\nAdd type annotations when TypeScript\u0026rsquo;s inference isn\u0026rsquo;t sufficient This improves code readability and helps catch errors Practical Example Let\u0026rsquo;s look at a practical example combining what we\u0026rsquo;ve learned:\n// Creating variables with different types let username: string = \u0026#34;John Doe\u0026#34;; let age: number = 30; let isLoggedIn: boolean = true; // Using type inference let lastLoginDate = new Date(); // TypeScript infers Date type let loginCount = 5; // TypeScript infers number type // Working with these variables function displayUserInfo() { console.log(`User: ${username}`); console.log(`Age: ${age}`); console.log(`Logged In: ${isLoggedIn}`); console.log(`Last Login: ${lastLoginDate}`); console.log(`Login Count: ${loginCount}`); } // TypeScript will catch these errors: username = 123; // Error: Type \u0026#39;number\u0026#39; is not assignable to type \u0026#39;string\u0026#39; age = \u0026#34;thirty\u0026#34;; // Error: Type \u0026#39;string\u0026#39; is not assignable to type \u0026#39;number\u0026#39; isLoggedIn = \u0026#34;yes\u0026#34;; // Error: Type \u0026#39;string\u0026#39; is not assignable to type \u0026#39;boolean\u0026#39; This example shows how TypeScript helps us maintain type safety in a real application scenario, preventing common type-related bugs before they happen.\nConclusion This introduction to TypeScript covers the basics of types and type inference. TypeScript adds a powerful type system to JavaScript that can help us write more reliable code. In the next part of this series, we\u0026rsquo;ll dive deeper into interfaces, functions, and more advanced TypeScript features.\nRemember that TypeScript is designed to help us catch errors early and make our code more maintainable. While it might seem like extra work at first, the benefits become clear as your projects grow in size and complexity.\nStay tuned for Part 2 where we\u0026rsquo;ll explore more advanced TypeScript concepts!\n",
        "tags": ["typescript","javascript"],
        "categories": ["TypeScript"],
        "lang": "en"
    },{
        "title": "Basic Linux Commands - Part 1",
        "permalink": "/blog/en/linux/basic-linux-commands/",
        "summary": "In this article series, we will explore the most commonly used Linux commands that every developer should know. This first part covers 20 essential commands that will help you navigate and manage your Linux system effectively.",
        "content": "Hello everyone! In this article series, I will explain the most commonly used Linux commands that every developer should know. These commands are essential for effectively managing your Linux system and improving your productivity in the terminal.\nWhy Should We Learn Linux Commands? Linux commands are fundamental tools that allow us to interact with our operating system through the terminal. Understanding these commands is crucial because:\nThey provide more control over the system They\u0026rsquo;re often faster than using a graphical interface Many servers run on Linux and don\u0026rsquo;t have graphical interfaces They\u0026rsquo;re essential for automation and scripting They\u0026rsquo;re used extensively in DevOps and system administration Most Used Linux Commands Let\u0026rsquo;s explore the 20 most commonly used Linux commands:\nls (List) Lists files and directories in the current directory Common options: ls -l: Long format listing ls -a: Show hidden files ls -h: Human-readable file sizes user@linux:~$ ls -la total 32 drwxr-xr-x 2 user user 4096 Feb 8 10:00 . drwxr-xr-x 20 user user 4096 Feb 8 10:00 .. -rw-r--r-- 1 user user 220 Feb 8 10:00 .bash_profile -rw-r--r-- 1 user user 3526 Feb 8 10:00 .bashrc drwxr-xr-x 2 user user 4096 Feb 8 10:00 Documents cd (Change Directory) Changes your current directory Usage examples: cd /path/to/directory: Go to specific directory cd ..: Go up one directory cd ~: Go to home directory user@linux:~$ pwd /home/user user@linux:~$ cd Documents user@linux:~/Documents$ cd .. user@linux:~$ cd ~ pwd (Print Working Directory) Shows your current directory path Useful when you need to confirm your location in the file system user@linux:~$ pwd /home/user/Documents/projects mkdir (Make Directory) Creates new directories Options: mkdir -p: Creates parent directories if they don\u0026rsquo;t exist user@linux:~$ mkdir -p projects/new-project user@linux:~$ ls -l projects/ total 4 drwxr-xr-x 2 user user 4096 Feb 8 10:00 new-project rm (Remove) Deletes files and directories Important options: rm -r: Remove directories recursively rm -f: Force removal without confirmation user@linux:~$ ls file1.txt file2.txt test_dir user@linux:~$ rm file1.txt user@linux:~$ rm -r test_dir user@linux:~$ ls file2.txt cp (Copy) Copies files and directories Common usage: cp file1 file2: Copy file1 to file2 cp -r dir1 dir2: Copy directory recursively user@linux:~$ cp file1.txt backup.txt user@linux:~$ cp -r projects/ projects_backup/ user@linux:~$ ls backup.txt file1.txt projects projects_backup mv (Move) Moves or renames files and directories Examples: mv old.txt new.txt: Rename file mv file /path/to/dir: Move file to directory user@linux:~$ ls old.txt documents/ user@linux:~$ mv old.txt new.txt user@linux:~$ mv new.txt documents/ user@linux:~$ ls documents/ new.txt cat (Concatenate) Displays file contents Also used to concatenate files user@linux:~$ cat file.txt This is the content of file.txt user@linux:~$ cat file1.txt file2.txt \u0026gt; combined.txt user@linux:~$ cat combined.txt Content from file1 Content from file2 grep (Global Regular Expression Print) Searches for patterns in files Useful options: grep -i: Case-insensitive search grep -r: Recursive search user@linux:~$ grep -r \u0026#34;TODO\u0026#34; . ./src/app.js:// TODO: Implement error handling ./docs/readme.md:TODO: Update documentation user@linux:~$ grep -i \u0026#34;error\u0026#34; log.txt Error: Connection failed error: unable to connect ERROR: System failure chmod (Change Mode) Changes file permissions Format: chmod [options] mode file user@linux:~$ ls -l script.sh -rw-r--r-- 1 user user 256 Feb 8 10:00 script.sh user@linux:~$ chmod +x script.sh user@linux:~$ ls -l script.sh -rwxr-xr-x 1 user user 256 Feb 8 10:00 script.sh sudo (Superuser Do) Executes commands with superuser privileges Important for system administration tasks user@linux:~$ apt update E: Could not open lock file - open (13: Permission denied) user@linux:~$ sudo apt update [sudo] password for user: Reading package lists... Done Building dependency tree... Done top Shows running processes and system resources Interactive process viewer user@linux:~$ top top - 10:00:00 up 2 days, 3:45, 1 user, load average: 0.52, 0.58, 0.59 Tasks: 180 total, 1 running, 179 sleeping, 0 stopped, 0 zombie %Cpu(s): 5.9 us, 3.1 sy, 0.0 ni, 90.6 id, 0.4 wa, 0.0 hi, 0.0 si MiB Mem : 7861.1 total, 2457.2 free, 3245.5 used, 2158.4 buff/cache ps (Process Status) Displays running processes Common options: ps aux: Show all processes user@linux:~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND user 2345 0.0 0.1 169512 3252 pts/0 Ss 09:30 0:00 bash user 2789 0.0 0.2 170284 6432 pts/0 R+ 10:00 0:00 ps aux df (Disk Free) Shows disk space usage Useful options: df -h: Human-readable sizes user@linux:~$ df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 234G 67G 156G 31% / tmpfs 3.9G 0 3.9G 0% /tmp /dev/sda2 100G 45G 55G 45% /home du (Disk Usage) Shows directory space usage Common usage: du -sh *: Size of current directory contents user@linux:~$ du -sh * 156M Documents 1.2G Downloads 42M Pictures 890M projects tar Archives files and directories Common operations: tar -czf: Create archive tar -xzf: Extract archive user@linux:~$ tar -czf archive.tar.gz Documents/ user@linux:~$ ls -lh archive.tar.gz -rw-r--r-- 1 user user 145M Feb 8 10:00 archive.tar.gz user@linux:~$ tar -xzf archive.tar.gz find Searches for files in directory hierarchy Examples: find . -name \u0026quot;*.txt\u0026quot;: Find all .txt files user@linux:~$ find . -name \u0026#34;*.txt\u0026#34; ./documents/notes.txt ./projects/readme.txt ./backup/old.txt user@linux:~$ find . -type d -name \u0026#34;test\u0026#34; ./projects/test ./src/test wget Downloads files from the internet Useful options: wget -c: Continue interrupted download user@linux:~$ wget https://example.com/file.zip --2025-02-08 10:00:00-- https://example.com/file.zip Resolving example.com... 93.184.216.34 Connecting to example.com... connected. HTTP request sent, awaiting response... 200 OK Length: 52890112 (50M) [application/zip] Saving to: \u0026#39;file.zip\u0026#39; systemctl Controls the systemd system and service manager Common uses: systemctl start/stop/restart/status service user@linux:~$ sudo systemctl status nginx ● nginx.service - A high performance web server and reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2025-02-08 09:30:12 UTC; 30min ago history Shows command history Useful features: !n: Execute command number n !!: Execute last command user@linux:~$ history 1 pwd 2 cd Documents 3 ls -la 4 mkdir projects 5 cd projects user@linux:~$ !3 ls -la total 32 drwxr-xr-x 2 user user 4096 Feb 8 10:00 . drwxr-xr-x 5 user user 4096 Feb 8 10:00 .. Best Practices Always Use Tab Completion\nSaves time and prevents typos Shows available options Check Commands Before Execution\nUse --help or man command Be extra careful with rm and sudo Use Command History\nPress up arrow to browse previous commands Use Ctrl+R for reverse search Create Aliases\nSave commonly used commands as aliases Add them to your .bashrc or .zshrc Conclusion These 20 commands form the foundation of Linux command-line usage. Understanding and mastering them will significantly improve your productivity when working with Linux systems. In the next part of this series, we\u0026rsquo;ll explore more advanced commands and techniques for system administration and automation.\nRemember that practice is key to becoming proficient with these commands. Try to use them regularly in your daily work, and don\u0026rsquo;t be afraid to experiment in a safe environment.\nStay tuned for Part 2 where we\u0026rsquo;ll dive into more advanced Linux commands!\n",
        "tags": ["linux","commands","terminal"],
        "categories": ["Linux"],
        "lang": "en"
    },{
        "title": "Build a RESTful API with Fastify and Node.js",
        "permalink": "/blog/en/build-restful-api-with-fastify-nodejs/",
        "summary": "Fastify is a modern web framework for Node.js that focuses on providing high performance with low overhead. In this article, I will explain in detail what Fastify is, how it\u0026rsquo;s used, and best practice recommendations.",
        "content": "Hello everyone! In this article, I\u0026rsquo;m going to talk about Fastify, a fast and low overhead web framework for Node.js. We\u0026rsquo;ll build a simple TODO API together, and I\u0026rsquo;ll explain how Fastify\u0026rsquo;s features can make your development process more efficient.\nYou can access all the source code for this project on GitHub: fastify-nodejs-restful-api\nWhat is Fastify and Why Should We Use It? Fastify is a modern web framework for Node.js that focuses on providing high performance with low overhead. While most Node.js developers are familiar with Express.js, Fastify offers some compelling advantages:\nUp to 2x faster than Express Built-in schema validation Automatic Swagger documentation Plugin-based architecture Setting Up Our Project Let\u0026rsquo;s start by setting up our project. First, we need to install some dependencies. Create a new directory and run these commands:\nmkdir fastify-todo-api cd fastify-todo-api npm init -y Now, let\u0026rsquo;s install the packages we\u0026rsquo;ll need:\n{ \u0026#34;dependencies\u0026#34;: { \u0026#34;fastify\u0026#34;: \u0026#34;^3.29.0\u0026#34;, \u0026#34;fastify-swagger\u0026#34;: \u0026#34;^5.2.0\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;^8.3.2\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;nodemon\u0026#34;: \u0026#34;^2.0.16\u0026#34; } } Project Structure Before we start coding, let\u0026rsquo;s organize our project structure. We\u0026rsquo;ll follow a clean and maintainable approach:\nfastify-todo-api/ ├── app.js # Main server file ├── items.js # Our data store ├── routes/ │ └── todoRouter.js # Route definitions └── controllers/ └── todoController.js # Request handlers Creating Our First Fastify Server Let\u0026rsquo;s start with our main server file (app.js). Here\u0026rsquo;s how we set up a basic Fastify server:\nconst fastify = require(\u0026#39;fastify\u0026#39;)({ logger: true }); // Setting up Swagger documentation fastify.register(require(\u0026#39;fastify-swagger\u0026#39;), { exposeRoute: true, routePrefix: \u0026#39;/docs\u0026#39;, swagger: { info: { title: \u0026#39;fastify-api\u0026#39; }, }, }); // Registering our routes fastify.register(require(\u0026#39;./routes/todoRouter\u0026#39;)); const PORT = 5000; const start = async () =\u0026gt; { try { await fastify.listen(PORT); } catch (error) { fastify.log.error(error); process.exit(1); } }; start(); What\u0026rsquo;s happening in this code?\nWe create a Fastify instance with logging enabled We set up Swagger documentation (accessible at /docs) We register our routes using Fastify\u0026rsquo;s plugin system We start the server on port 5000 Understanding Fastify\u0026rsquo;s Schema Validation One of Fastify\u0026rsquo;s most powerful features is its schema validation system. Let\u0026rsquo;s look at how we can use it in our todoRouter.js:\n// First, we define what a TODO item looks like const Item = { type: \u0026#39;object\u0026#39;, properties: { id: { type: \u0026#39;string\u0026#39; }, name: { type: \u0026#39;string\u0026#39; }, }, }; // Then we create schemas for our endpoints const getItemsOpts = { schema: { response: { 200: { type: \u0026#39;array\u0026#39;, items: Item, }, }, }, handler: getItems, }; What makes this special?\nFastify automatically validates all incoming and outgoing data It generates Swagger documentation from these schemas It improves performance by optimizing serialization It catches errors before they reach your handlers Creating Our Controllers Now let\u0026rsquo;s look at how we handle requests in todoController.js:\nlet items = require(\u0026#39;../items\u0026#39;); const { v4: uuidv4 } = require(\u0026#39;uuid\u0026#39;); // Get all items const getItems = (req, reply) =\u0026gt; { reply.send(items); }; // Create new item const addItem = (req, reply) =\u0026gt; { const { name } = req.body; const item = { id: uuidv4(), name, }; items = [...items, item]; reply.code(201).send(item); }; Notice how Fastify makes response handling simple:\nNo need to set Content-Type headers manually Method chaining for status codes and sending responses Automatic response serialization Testing Our API Now that we have everything set up, let\u0026rsquo;s test our API. You can use curl or any API testing tool:\n# Get all items curl http://localhost:5000/items # Create a new item curl -X POST \\ http://localhost:5000/items \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Learn Fastify\u0026#34;}\u0026#39; Performance Features You Should Know About Fastify isn\u0026rsquo;t just fast by accident. Here\u0026rsquo;s why it performs so well:\nSchema-based Serialization\nPre-compiles schemas for faster validation Optimizes JSON serialization Reduces processing overhead Efficient Routing\nUses a radix tree for route matching Faster than regex-based routing Optimized parameter handling Lightweight Core\nMinimal baseline overhead Plugin system for adding features Efficient memory usage Best Practices I Recommend After working with Fastify, here are some practices I\u0026rsquo;ve found helpful:\nAlways Use Schemas\nfastify.get(\u0026#39;/items\u0026#39;, { schema: { response: { 200: itemSchema } } }) Organize with Plugins\nfastify.register(require(\u0026#39;./routes/items\u0026#39;)) fastify.register(require(\u0026#39;./routes/users\u0026#39;)) Handle Errors Properly\nfastify.setErrorHandler(function (error, request, reply) { reply.status(error.statusCode || 500).send({error: error.message}) }) Conclusion Fastify provides an excellent foundation for building high-performance Node.js APIs. Its built-in features like schema validation and swagger documentation make it a great choice for modern web applications. In future articles, I\u0026rsquo;ll explore more advanced Fastify features and how to use them effectively in production. Thanks for reading!\nRemember to check out the Fastify documentation for more detailed information about all these features.\n",
        "tags": ["nodejs","fastify"],
        "categories": ["Node.js"],
        "lang": "en"
    },{
        "title": "What is Git Flow?",
        "permalink": "/blog/en/git-flow/",
        "summary": "Git Flow is an effective approach that systematizes branch management in version control systems. In this article, I will explain in detail what Git Flow is, how it\u0026rsquo;s used, and best practice recommendations.",
        "content": "\nHello. In this article, I will briefly discuss version control systems and explain in detail what Git Flow is and how it\u0026rsquo;s used. Version control systems are important tools used in software development processes to track and manage code versions. Git Flow is a branching model that enables us to use this version control system more effectively.\nWhat is a Version Control System? A version control system is a system that records step-by-step changes we make to one or more files, documents (software project, office documents, etc.), allows us to revert to a specific version later, and if desired, enables us to store and manage this in an online repository. Git, SVN, BitKeeper, and Mercurial are examples of version control systems.\nWhy Do We Use Version Control Systems? A complete long-term change history is kept for each file.\nThis means tracking every change made to the file by multiple people over the years. This allows us to compare our old and new code to understand how we got to where we are. Allows team members to work simultaneously on the same code.\nIt\u0026rsquo;s possible to create sub-versions to carry out different work on the software and later integrate it into the main software. Enables tracking every change made to the software and linking it to project management.\nAllows software issues to be associated with and tracked through versions. Allows us to revert to old code records when we encounter errors in the project.\nVersion Control Systems Local VCS: This is the oldest version control system approach. Our project and the changes we make are stored in a database on the user machine. Each commit is stored as a version, and each version is distinguished by assigning a hash value to the commit. It also provides version viewing capability. However, only one user can work effectively in this system.\nCentralized VCS: This is a versioning system created for multiple people to work effectively on a project. CVS and SVN are centralized version control systems. In this system, the project is kept in a shared repository, and multiple developers perform checkout and commit operations on the same repository. While this method allows everyone to contribute to the project, it has some serious problems. If the single central server fails for 1 hour, users won\u0026rsquo;t be able to save their work or versioned copies of their project for that hour.\nDistributed VCS: This is a version system created due to the limitations of centralized version systems, such as developers\u0026rsquo; inability to work offline and difficulty in recovery if the repository is damaged. Systems like Git, Mercurial, and BitKeeper are examples of distributed version systems. In these systems, there is no central repository, and each machine working on the project keeps a copy of the project on their local computer. Developers don\u0026rsquo;t need to communicate with the remote repository when they want to make changes to the project or view project history. If one server crashes and there are systems working collaboratively on that server, the system can be recovered by having one of the developers restore the project to the server. In summary, it allows different developers on the same project to work in different ways with different workflows.\nWhat is Git Flow? On January 5, 2020, nvie proposed a model for keeping git repositories organized in a post at https://nvie.com/posts/a-successful-git-branching-model/. Later, he released a project called Git-Flow that includes git extensions to make using this model easier. The GitFlow model is fundamentally based on the git version control system. In other words, it\u0026rsquo;s possible to execute all model operations with git commands.\nAdvantages and Disadvantages of Git Flow Advantages:\nProvides an organized and predictable development process Offers an ideal structure for large teams Simplifies version management Each branch has a clear purpose Supports parallel development Disadvantages:\nCan be too complex for small projects May not be suitable for continuous delivery Branch structure can sometimes lead to unnecessary complexity Requires additional tool installation Learning curve is higher compared to other models Alternatives to Git Flow GitHub Flow: A simpler model, uses only master and feature branches GitLab Flow: Strikes a balance between Git Flow and GitHub Flow Trunk Based Development: Focuses on development on the main branch Git Flow Working Principle Git Flow model has 5 main branches:\nmaster: Master, one of the main branches, exists throughout the project. The master branch always contains code that can be deployed to production. Ideally, each commit to the master branch is a version and should be marked with \u0026ldquo;git tag\u0026rdquo; (given a version number). Direct commits to the master branch are not made; only merges from hotfix and release branches are allowed.\ndevelop: Develop is another main branch that exists throughout the project. The develop branch contains changes made for the next version. All feature branches are first merged into this branch. This branch is the main development branch of the project, and continuous integration (CI) processes typically run on this branch.\nhotfix: The hotfix branch is used when there\u0026rsquo;s a critical bug in the live version that needs to be fixed and deployed immediately. The hotfix branch is created from the master branch and is typically named in the format \u0026lsquo;hotfix/[version]-[description]\u0026rsquo;. When the bug fix is completed in the hotfix branch, this branch is merged with both Developer and Master. After merging with Master, the change is tagged with a new version number.\nfeature: When adding a new feature, a Feature branch is created for this feature. Feature branches are always created from the develop branch and are typically named in the format \u0026lsquo;feature/[feature-name]\u0026rsquo;. These can be considered as changes relative to features. Multiple feature branches can be opened simultaneously. This means different developers can work on different features. Developing features in separate branches both prevents the Develop branch from being filled with unnecessary commits and makes it easy to abandon a feature by simply deleting the feature branch. When the feature is complete, this branch is merged with the Develop branch and the feature branch is deleted. So feature branches only live during development. Of course, during this process, you may need to occasionally pull from the Develop branch for checking purposes because another developer might have finished their feature branch first and version might have been pushed to the Develop branch. Feature branches should not include names containing master, release, develop, or hotfix.\nrelease: Let\u0026rsquo;s say all changes are complete. When a new version is to be released, a new Release branch is created from the Develop branch. Release branches are typically named in the format \u0026lsquo;release/[version]\u0026rsquo;. Final changes in the version, changing version numbers, etc. are performed in this branch. Only bug fixes should be made in the Release branch, new features should not be added. When all necessary changes are completed, all changes completed in the Release branch are merged into both Master and Develop branches. The version number is tagged with git tag in the Master branch, and then the Release branch is deleted.\nGit Flow Example Project /brew install git-flow \u0026gt; git flow init First, we install with \u0026ldquo;brew install git-flow\u0026rdquo;. GitFlow doesn\u0026rsquo;t come with git. It needs to be installed separately. This is considered one of its disadvantages. In git, we used the \u0026ldquo;git init\u0026rdquo; command to start the project. For git-flow, we enter the \u0026ldquo;git flow init\u0026rdquo; command to start the git-flow process. When the command runs, if no repo exists, it first creates a repo. Then it asks the user for branch names to be used for the process. Branch names can be customized, but it\u0026rsquo;s recommended to keep the default values.\n/git flow feature start performance This command creates a new feature branch. Since the feature name is performance, the default branch will be feature/performance. We can do the same thing with the existing git command. The command we would need to enter for this would be \u0026ldquo;git checkout -b myFeature feature/performance\u0026rdquo;.\n/git flow feature finish performance This command closes a previously opened branch. The closing process starts with merging the feature branch into the develop branch and ends with deleting the feature branch. If changes are not committed when the command is run, it will give an error. If push is not done after commit, it will give an error. To do this operation with normal git commands, first commit is made in the relevant branch, then \u0026ldquo;git checkout develop \u0026gt; git merge \u0026ndash;no-ff feature/performance \u0026gt; git branch -D feature/performance\u0026rdquo; commands are run in order.\n/git flow release start 1.0.0 When this command is entered, a new version becomes ready for release. When the command is run, a new release/1.0.0 branch is created from the Develop branch. To perform this operation with the existing git command, the \u0026ldquo;git checkout -b release/1.0.0 develop\u0026rdquo; command is run.\n/git flow release finish 1.0.0 When the command is entered, the completed version is moved to the master branch. The changes are merged with both develop and master branches. The last commit in the master branch is tagged with the version number. Then the release branch is automatically deleted. To do the same operation with git commands, run the following commands in order:\ngit checkout master git merge --no-ff release/1.0.0 git tag -a 1.0.0 git checkout develop git merge --no-ff release/1.0.0 git branch -d release/1.0.0 /git flow hotfix start 1.0.1 A new hotfix is started with this command. Hotfix branches are used for urgent updates and are created from the master branch. When the command runs, a new hotfix/1.0.1 branch branching from the master branch is created. To do the operation with git command, run \u0026ldquo;git checkout -b hotfix/1.0.1 master\u0026rdquo; command.\n/git flow hotfix finish 1.0.1 The hotfix is completed with this command. Changes are taken to both Develop and Master branches. The Master branch is tagged with 1.0.1 and the hotfix branch is deleted. When the \u0026ldquo;git tag -l\u0026rdquo; command is run, version numbers are displayed. To do the same operation with existing git commands, run the following commands in order:\ngit checkout master git merge --no-ff hotfix/1.0.1 git tag -a 1.0.1 git checkout develop git merge --no-ff hotfix/1.0.1 git branch -d hotfix/1.0.1 Git Flow Best Practice Recommendations Branch Naming Conventions\nUse descriptive names for feature branches (e.g., feature/user-authentication) Use semantic versioning for hotfix and release branches Commit Messages\nWrite descriptive commit messages Follow the Conventional Commits standard Ensure each commit has a single purpose Code Review Process\nPerform code review before merging feature branches Use automated testing processes Don\u0026rsquo;t forget documentation updates Merge Strategy\nUse the \u0026ndash;no-ff (no fast-forward) parameter Resolve merge conflicts quickly Consider using squash commits Conclusion Git Flow is an effective approach that systematizes branch management in modern software development processes. Through this workflow, teams can work more organized, better control versioning, and improve code quality. Especially in large projects and team collaborations, the structured branch strategy offered by Git Flow significantly improves development processes. By following the best practices mentioned above, you can successfully implement Git Flow in your project and make your software development processes more efficient.\n",
        "tags": ["git-flow","git"],
        "categories": ["Git"],
        "lang": "en"
    },{
        "title": "Archives",
        "permalink": "/blog/en/archives/",
        "summary": "",
        "content": "",
        "tags": null,
        "categories": ["archives"],
        "lang": "en"
    },{
        "title": "How to Create JWT Tokens for using App Store Connect API?",
        "permalink": "/blog/en/how-to-create-jwt-token-using-app-store-connect-api/",
        "summary": "Hello everyone! Welcome to the first article about App Store Connect API. In this first article, I am going to give a brief explanation of App Store Connect API and how to create JWT tokens for the API.",
        "content": "\nHello everyone! Welcome to the first article about App Store Connect API. In this first article, I am going to give a brief explanation of App Store Connect API and how to create JWT tokens for the API.\nWhat is App Store Connect API? The App Store Connect API is a REST API that enables the automation of actions you take in App Store Connect. This API empowers developers to seamlessly handle various tasks, from app submission and updates to managing in-app purchases, monitoring app performance and user engagement through comprehensive reports, responding to customer reviews and feedback effectively, and many more.\nWhy JWT Token? Without a token, you won\u0026rsquo;t be able to get the response from the App Store Connect API but mainly API requires JSON Web Token (JWT) for authentication and authorization purposes. In the context of the API, the JWT token serves as a secure and standardized method to verify the identity of the client and to ensure that it has the necessary permissions to access the requested resources.\nHow to Create a JWT Token? Before creating JWT tokens for using the App Store Connect API we need a few steps to setup.\nGenerate API Key from App Store Connect GUI\nDownload the private key in the p8 format\nCopy your Issuer ID and API Key ID\nGenerate API Key To generate an API key, we have to log in to the App Store Connect web interface with our account and go to the Users and Access tab. On the page click Keys. To be able to see the Keys tab your account has to have permission. We can create an API key for a specific purpose or an admin API key that can access all the App Store Connect API.\nClick the plus icon next to the \u0026lsquo;active\u0026rsquo; text type a name for the key choose the roles that can access to key from the modal and click \u0026lsquo;Generate\u0026rsquo;. After that key will be created and listed.\nDownload Private Key Once the API Key is generated we will download the Private Key. The key is usually in the .p8 format. Some somethings are important to keep in mind whilst dealing with the private key.\nThe private key can be downloaded only once. Make sure to keep it secure once downloaded.\nThe private key never expires and is used to work as long as it\u0026rsquo;s valid even if it\u0026rsquo;s compromised so if you think that your key is not safe anymore, revoke it from App Store Connect as soon as possible and get a new key.\nCopy Your Issuer and API Key ID The last step before creating the JTW token is copying your issuer ID and API key ID which you can find on the Users\u0026amp;Access page.\nCreate JWT Token As mentioned earlier, JWT is used to generate the token that has been used by the App Store Connect API. The process of creating a token requires the following steps:\nIssuerID: The ID copied from the User\u0026amp;Access page.\nPrivate Key: Key downloaded in .p8 format.\nExpiration Time: 20 min maximum, the token cannot be valid for more than 20 minutes so we have to make sure that, we will create a new token before it expires.\nAudience: This is constant with API version value usually \u0026lsquo;applestoreconnect-v1\u0026rsquo;\nAlgorithm: The JWT algorithm required to create tokens e.g ES256\nOnce we have all the necessary details, we will be able to create a JWT token using the desired language. I am going to use Node.js for this process.\nRun these commands in order:\nmkdir appStoreToken cd appStoreToken npm init -y npm i jsonwebtoken touch index.js Paste this code into your index.js file, replace the necessary information with your information, and save.\nconst fs = require(\u0026#34;fs\u0026#34;); const jwt = require(\u0026#34;jsonwebtoken\u0026#34;); const privateKey = fs.readFileSync(\u0026#34;yourPrivateKey.p8\u0026#34;); const apiKeyId = \u0026#34;Your API Key ID\u0026#34;; const issuerId = \u0026#34;Your Issuer ID\u0026#34;; let now = Math.round(new Date().getTime() / 1000); // Notice the /1000 let nowPlus20 = now + 1199; // 1200 === 20 minutes let payload = { iss: issuerId, exp: nowPlus20, aud: \u0026#34;appstoreconnect-v1\u0026#34;, }; let signOptions = { algorithm: \u0026#34;ES256\u0026#34;, header: { alg: \u0026#34;ES256\u0026#34;, kid: apiKeyId, typ: \u0026#34;JWT\u0026#34;, }, }; let token = jwt.sign(payload, privateKey, signOptions); console.log(\u0026#34;@token: \u0026#34;, token); and lastly, run the command below.\nnode index.js This will return a long token that we can use to access an App Store Connect API, we also need to create another token if we want to continue using API after 20 minutes.\nConclusion In this post, you learned how the create a JWT token for the App Store Connect API using Node.js. This is a critical step in authenticating your requests to the API. In future posts, I will try to explore the other ways you can use the App Store Connect API. Thank you for your reading.\nRead this article in Turkish\n",
        "tags": ["jwt","app-store","api"],
        "categories": ["Development","iOS"],
        "lang": "en"
    }]

