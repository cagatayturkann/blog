[{"categories":["Linux"],"contents":"Hello everyone! In this article series, I will explain the most commonly used Linux commands that every developer should know. These commands are essential for effectively managing your Linux system and improving your productivity in the terminal.\nWhy Should We Learn Linux Commands? Linux commands are fundamental tools that allow us to interact with our operating system through the terminal. Understanding these commands is crucial because:\nThey provide more control over the system They\u0026rsquo;re often faster than using a graphical interface Many servers run on Linux and don\u0026rsquo;t have graphical interfaces They\u0026rsquo;re essential for automation and scripting They\u0026rsquo;re used extensively in DevOps and system administration Most Used Linux Commands Let\u0026rsquo;s explore the 20 most commonly used Linux commands:\nls (List) Lists files and directories in the current directory Common options: ls -l: Long format listing ls -a: Show hidden files ls -h: Human-readable file sizes user@linux:~$ ls -la total 32 drwxr-xr-x 2 user user 4096 Feb 8 10:00 . drwxr-xr-x 20 user user 4096 Feb 8 10:00 .. -rw-r--r-- 1 user user 220 Feb 8 10:00 .bash_profile -rw-r--r-- 1 user user 3526 Feb 8 10:00 .bashrc drwxr-xr-x 2 user user 4096 Feb 8 10:00 Documents cd (Change Directory) Changes your current directory Usage examples: cd /path/to/directory: Go to specific directory cd ..: Go up one directory cd ~: Go to home directory user@linux:~$ pwd /home/user user@linux:~$ cd Documents user@linux:~/Documents$ cd .. user@linux:~$ cd ~ pwd (Print Working Directory) Shows your current directory path Useful when you need to confirm your location in the file system user@linux:~$ pwd /home/user/Documents/projects mkdir (Make Directory) Creates new directories Options: mkdir -p: Creates parent directories if they don\u0026rsquo;t exist user@linux:~$ mkdir -p projects/new-project user@linux:~$ ls -l projects/ total 4 drwxr-xr-x 2 user user 4096 Feb 8 10:00 new-project rm (Remove) Deletes files and directories Important options: rm -r: Remove directories recursively rm -f: Force removal without confirmation user@linux:~$ ls file1.txt file2.txt test_dir user@linux:~$ rm file1.txt user@linux:~$ rm -r test_dir user@linux:~$ ls file2.txt cp (Copy) Copies files and directories Common usage: cp file1 file2: Copy file1 to file2 cp -r dir1 dir2: Copy directory recursively user@linux:~$ cp file1.txt backup.txt user@linux:~$ cp -r projects/ projects_backup/ user@linux:~$ ls backup.txt file1.txt projects projects_backup mv (Move) Moves or renames files and directories Examples: mv old.txt new.txt: Rename file mv file /path/to/dir: Move file to directory user@linux:~$ ls old.txt documents/ user@linux:~$ mv old.txt new.txt user@linux:~$ mv new.txt documents/ user@linux:~$ ls documents/ new.txt cat (Concatenate) Displays file contents Also used to concatenate files user@linux:~$ cat file.txt This is the content of file.txt user@linux:~$ cat file1.txt file2.txt \u0026gt; combined.txt user@linux:~$ cat combined.txt Content from file1 Content from file2 grep (Global Regular Expression Print) Searches for patterns in files Useful options: grep -i: Case-insensitive search grep -r: Recursive search user@linux:~$ grep -r \u0026#34;TODO\u0026#34; . ./src/app.js:// TODO: Implement error handling ./docs/readme.md:TODO: Update documentation user@linux:~$ grep -i \u0026#34;error\u0026#34; log.txt Error: Connection failed error: unable to connect ERROR: System failure chmod (Change Mode) Changes file permissions Format: chmod [options] mode file user@linux:~$ ls -l script.sh -rw-r--r-- 1 user user 256 Feb 8 10:00 script.sh user@linux:~$ chmod +x script.sh user@linux:~$ ls -l script.sh -rwxr-xr-x 1 user user 256 Feb 8 10:00 script.sh sudo (Superuser Do) Executes commands with superuser privileges Important for system administration tasks user@linux:~$ apt update E: Could not open lock file - open (13: Permission denied) user@linux:~$ sudo apt update [sudo] password for user: Reading package lists... Done Building dependency tree... Done top Shows running processes and system resources Interactive process viewer user@linux:~$ top top - 10:00:00 up 2 days, 3:45, 1 user, load average: 0.52, 0.58, 0.59 Tasks: 180 total, 1 running, 179 sleeping, 0 stopped, 0 zombie %Cpu(s): 5.9 us, 3.1 sy, 0.0 ni, 90.6 id, 0.4 wa, 0.0 hi, 0.0 si MiB Mem : 7861.1 total, 2457.2 free, 3245.5 used, 2158.4 buff/cache ps (Process Status) Displays running processes Common options: ps aux: Show all processes user@linux:~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND user 2345 0.0 0.1 169512 3252 pts/0 Ss 09:30 0:00 bash user 2789 0.0 0.2 170284 6432 pts/0 R+ 10:00 0:00 ps aux df (Disk Free) Shows disk space usage Useful options: df -h: Human-readable sizes user@linux:~$ df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 234G 67G 156G 31% / tmpfs 3.9G 0 3.9G 0% /tmp /dev/sda2 100G 45G 55G 45% /home du (Disk Usage) Shows directory space usage Common usage: du -sh *: Size of current directory contents user@linux:~$ du -sh * 156M Documents 1.2G Downloads 42M Pictures 890M projects tar Archives files and directories Common operations: tar -czf: Create archive tar -xzf: Extract archive user@linux:~$ tar -czf archive.tar.gz Documents/ user@linux:~$ ls -lh archive.tar.gz -rw-r--r-- 1 user user 145M Feb 8 10:00 archive.tar.gz user@linux:~$ tar -xzf archive.tar.gz find Searches for files in directory hierarchy Examples: find . -name \u0026quot;*.txt\u0026quot;: Find all .txt files user@linux:~$ find . -name \u0026#34;*.txt\u0026#34; ./documents/notes.txt ./projects/readme.txt ./backup/old.txt user@linux:~$ find . -type d -name \u0026#34;test\u0026#34; ./projects/test ./src/test wget Downloads files from the internet Useful options: wget -c: Continue interrupted download user@linux:~$ wget https://example.com/file.zip --2025-02-08 10:00:00-- https://example.com/file.zip Resolving example.com... 93.184.216.34 Connecting to example.com... connected. HTTP request sent, awaiting response... 200 OK Length: 52890112 (50M) [application/zip] Saving to: \u0026#39;file.zip\u0026#39; systemctl Controls the systemd system and service manager Common uses: systemctl start/stop/restart/status service user@linux:~$ sudo systemctl status nginx ● nginx.service - A high performance web server and reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2025-02-08 09:30:12 UTC; 30min ago history Shows command history Useful features: !n: Execute command number n !!: Execute last command user@linux:~$ history 1 pwd 2 cd Documents 3 ls -la 4 mkdir projects 5 cd projects user@linux:~$ !3 ls -la total 32 drwxr-xr-x 2 user user 4096 Feb 8 10:00 . drwxr-xr-x 5 user user 4096 Feb 8 10:00 .. Best Practices Always Use Tab Completion\nSaves time and prevents typos Shows available options Check Commands Before Execution\nUse --help or man command Be extra careful with rm and sudo Use Command History\nPress up arrow to browse previous commands Use Ctrl+R for reverse search Create Aliases\nSave commonly used commands as aliases Add them to your .bashrc or .zshrc Conclusion These 20 commands form the foundation of Linux command-line usage. Understanding and mastering them will significantly improve your productivity when working with Linux systems. In the next part of this series, we\u0026rsquo;ll explore more advanced commands and techniques for system administration and automation.\nRemember that practice is key to becoming proficient with these commands. Try to use them regularly in your daily work, and don\u0026rsquo;t be afraid to experiment in a safe environment.\nStay tuned for Part 2 where we\u0026rsquo;ll dive into more advanced Linux commands!\n","lang":"en","permalink":"/en/blog/linux/basic-linux-commands/","summary":"In this article series, we will explore the most commonly used Linux commands that every developer should know. This first part covers 20 essential commands that will help you navigate and manage your Linux system effectively.","tags":["linux","commands","terminal"],"title":"Basic Linux Commands - Part 1"},{"categories":["Node.js"],"contents":"Hello everyone! In this article, I\u0026rsquo;m going to talk about Fastify, a fast and low overhead web framework for Node.js. We\u0026rsquo;ll build a simple TODO API together, and I\u0026rsquo;ll explain how Fastify\u0026rsquo;s features can make your development process more efficient.\nYou can access all the source code for this project on GitHub: fastify-nodejs-restful-api\nWhat is Fastify and Why Should We Use It? Fastify is a modern web framework for Node.js that focuses on providing high performance with low overhead. While most Node.js developers are familiar with Express.js, Fastify offers some compelling advantages:\nUp to 2x faster than Express Built-in schema validation Automatic Swagger documentation Plugin-based architecture Setting Up Our Project Let\u0026rsquo;s start by setting up our project. First, we need to install some dependencies. Create a new directory and run these commands:\nmkdir fastify-todo-api cd fastify-todo-api npm init -y Now, let\u0026rsquo;s install the packages we\u0026rsquo;ll need:\n{ \u0026#34;dependencies\u0026#34;: { \u0026#34;fastify\u0026#34;: \u0026#34;^3.29.0\u0026#34;, \u0026#34;fastify-swagger\u0026#34;: \u0026#34;^5.2.0\u0026#34;, \u0026#34;uuid\u0026#34;: \u0026#34;^8.3.2\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;nodemon\u0026#34;: \u0026#34;^2.0.16\u0026#34; } } Project Structure Before we start coding, let\u0026rsquo;s organize our project structure. We\u0026rsquo;ll follow a clean and maintainable approach:\nfastify-todo-api/ ├── app.js # Main server file ├── items.js # Our data store ├── routes/ │ └── todoRouter.js # Route definitions └── controllers/ └── todoController.js # Request handlers Creating Our First Fastify Server Let\u0026rsquo;s start with our main server file (app.js). Here\u0026rsquo;s how we set up a basic Fastify server:\nconst fastify = require(\u0026#39;fastify\u0026#39;)({ logger: true }); // Setting up Swagger documentation fastify.register(require(\u0026#39;fastify-swagger\u0026#39;), { exposeRoute: true, routePrefix: \u0026#39;/docs\u0026#39;, swagger: { info: { title: \u0026#39;fastify-api\u0026#39; }, }, }); // Registering our routes fastify.register(require(\u0026#39;./routes/todoRouter\u0026#39;)); const PORT = 5000; const start = async () =\u0026gt; { try { await fastify.listen(PORT); } catch (error) { fastify.log.error(error); process.exit(1); } }; start(); What\u0026rsquo;s happening in this code?\nWe create a Fastify instance with logging enabled We set up Swagger documentation (accessible at /docs) We register our routes using Fastify\u0026rsquo;s plugin system We start the server on port 5000 Understanding Fastify\u0026rsquo;s Schema Validation One of Fastify\u0026rsquo;s most powerful features is its schema validation system. Let\u0026rsquo;s look at how we can use it in our todoRouter.js:\n// First, we define what a TODO item looks like const Item = { type: \u0026#39;object\u0026#39;, properties: { id: { type: \u0026#39;string\u0026#39; }, name: { type: \u0026#39;string\u0026#39; }, }, }; // Then we create schemas for our endpoints const getItemsOpts = { schema: { response: { 200: { type: \u0026#39;array\u0026#39;, items: Item, }, }, }, handler: getItems, }; What makes this special?\nFastify automatically validates all incoming and outgoing data It generates Swagger documentation from these schemas It improves performance by optimizing serialization It catches errors before they reach your handlers Creating Our Controllers Now let\u0026rsquo;s look at how we handle requests in todoController.js:\nlet items = require(\u0026#39;../items\u0026#39;); const { v4: uuidv4 } = require(\u0026#39;uuid\u0026#39;); // Get all items const getItems = (req, reply) =\u0026gt; { reply.send(items); }; // Create new item const addItem = (req, reply) =\u0026gt; { const { name } = req.body; const item = { id: uuidv4(), name, }; items = [...items, item]; reply.code(201).send(item); }; Notice how Fastify makes response handling simple:\nNo need to set Content-Type headers manually Method chaining for status codes and sending responses Automatic response serialization Testing Our API Now that we have everything set up, let\u0026rsquo;s test our API. You can use curl or any API testing tool:\n# Get all items curl http://localhost:5000/items # Create a new item curl -X POST \\ http://localhost:5000/items \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Learn Fastify\u0026#34;}\u0026#39; Performance Features You Should Know About Fastify isn\u0026rsquo;t just fast by accident. Here\u0026rsquo;s why it performs so well:\nSchema-based Serialization\nPre-compiles schemas for faster validation Optimizes JSON serialization Reduces processing overhead Efficient Routing\nUses a radix tree for route matching Faster than regex-based routing Optimized parameter handling Lightweight Core\nMinimal baseline overhead Plugin system for adding features Efficient memory usage Best Practices I Recommend After working with Fastify, here are some practices I\u0026rsquo;ve found helpful:\nAlways Use Schemas\nfastify.get(\u0026#39;/items\u0026#39;, { schema: { response: { 200: itemSchema } } }) Organize with Plugins\nfastify.register(require(\u0026#39;./routes/items\u0026#39;)) fastify.register(require(\u0026#39;./routes/users\u0026#39;)) Handle Errors Properly\nfastify.setErrorHandler(function (error, request, reply) { reply.status(error.statusCode || 500).send({error: error.message}) }) Conclusion Fastify provides an excellent foundation for building high-performance Node.js APIs. Its built-in features like schema validation and swagger documentation make it a great choice for modern web applications. In future articles, I\u0026rsquo;ll explore more advanced Fastify features and how to use them effectively in production. Thanks for reading!\nRemember to check out the Fastify documentation for more detailed information about all these features.\n","lang":"en","permalink":"/en/blog/build-restful-api-with-fastify-nodejs/","summary":"Fastify is a modern web framework for Node.js that focuses on providing high performance with low overhead. In this article, I will explain in detail what Fastify is, how it\u0026rsquo;s used, and best practice recommendations.","tags":["nodejs","fastify"],"title":"Build a RESTful API with Fastify and Node.js"},{"categories":["Git"],"contents":"\nHello. In this article, I will briefly discuss version control systems and explain in detail what Git Flow is and how it\u0026rsquo;s used. Version control systems are important tools used in software development processes to track and manage code versions. Git Flow is a branching model that enables us to use this version control system more effectively.\nWhat is a Version Control System? A version control system is a system that records step-by-step changes we make to one or more files, documents (software project, office documents, etc.), allows us to revert to a specific version later, and if desired, enables us to store and manage this in an online repository. Git, SVN, BitKeeper, and Mercurial are examples of version control systems.\nWhy Do We Use Version Control Systems? A complete long-term change history is kept for each file.\nThis means tracking every change made to the file by multiple people over the years. This allows us to compare our old and new code to understand how we got to where we are. Allows team members to work simultaneously on the same code.\nIt\u0026rsquo;s possible to create sub-versions to carry out different work on the software and later integrate it into the main software. Enables tracking every change made to the software and linking it to project management.\nAllows software issues to be associated with and tracked through versions. Allows us to revert to old code records when we encounter errors in the project.\nVersion Control Systems Local VCS: This is the oldest version control system approach. Our project and the changes we make are stored in a database on the user machine. Each commit is stored as a version, and each version is distinguished by assigning a hash value to the commit. It also provides version viewing capability. However, only one user can work effectively in this system.\nCentralized VCS: This is a versioning system created for multiple people to work effectively on a project. CVS and SVN are centralized version control systems. In this system, the project is kept in a shared repository, and multiple developers perform checkout and commit operations on the same repository. While this method allows everyone to contribute to the project, it has some serious problems. If the single central server fails for 1 hour, users won\u0026rsquo;t be able to save their work or versioned copies of their project for that hour.\nDistributed VCS: This is a version system created due to the limitations of centralized version systems, such as developers\u0026rsquo; inability to work offline and difficulty in recovery if the repository is damaged. Systems like Git, Mercurial, and BitKeeper are examples of distributed version systems. In these systems, there is no central repository, and each machine working on the project keeps a copy of the project on their local computer. Developers don\u0026rsquo;t need to communicate with the remote repository when they want to make changes to the project or view project history. If one server crashes and there are systems working collaboratively on that server, the system can be recovered by having one of the developers restore the project to the server. In summary, it allows different developers on the same project to work in different ways with different workflows.\nWhat is Git Flow? On January 5, 2020, nvie proposed a model for keeping git repositories organized in a post at https://nvie.com/posts/a-successful-git-branching-model/. Later, he released a project called Git-Flow that includes git extensions to make using this model easier. The GitFlow model is fundamentally based on the git version control system. In other words, it\u0026rsquo;s possible to execute all model operations with git commands.\nAdvantages and Disadvantages of Git Flow Advantages:\nProvides an organized and predictable development process Offers an ideal structure for large teams Simplifies version management Each branch has a clear purpose Supports parallel development Disadvantages:\nCan be too complex for small projects May not be suitable for continuous delivery Branch structure can sometimes lead to unnecessary complexity Requires additional tool installation Learning curve is higher compared to other models Alternatives to Git Flow GitHub Flow: A simpler model, uses only master and feature branches GitLab Flow: Strikes a balance between Git Flow and GitHub Flow Trunk Based Development: Focuses on development on the main branch Git Flow Working Principle Git Flow model has 5 main branches:\nmaster: Master, one of the main branches, exists throughout the project. The master branch always contains code that can be deployed to production. Ideally, each commit to the master branch is a version and should be marked with \u0026ldquo;git tag\u0026rdquo; (given a version number). Direct commits to the master branch are not made; only merges from hotfix and release branches are allowed.\ndevelop: Develop is another main branch that exists throughout the project. The develop branch contains changes made for the next version. All feature branches are first merged into this branch. This branch is the main development branch of the project, and continuous integration (CI) processes typically run on this branch.\nhotfix: The hotfix branch is used when there\u0026rsquo;s a critical bug in the live version that needs to be fixed and deployed immediately. The hotfix branch is created from the master branch and is typically named in the format \u0026lsquo;hotfix/[version]-[description]\u0026rsquo;. When the bug fix is completed in the hotfix branch, this branch is merged with both Developer and Master. After merging with Master, the change is tagged with a new version number.\nfeature: When adding a new feature, a Feature branch is created for this feature. Feature branches are always created from the develop branch and are typically named in the format \u0026lsquo;feature/[feature-name]\u0026rsquo;. These can be considered as changes relative to features. Multiple feature branches can be opened simultaneously. This means different developers can work on different features. Developing features in separate branches both prevents the Develop branch from being filled with unnecessary commits and makes it easy to abandon a feature by simply deleting the feature branch. When the feature is complete, this branch is merged with the Develop branch and the feature branch is deleted. So feature branches only live during development. Of course, during this process, you may need to occasionally pull from the Develop branch for checking purposes because another developer might have finished their feature branch first and version might have been pushed to the Develop branch. Feature branches should not include names containing master, release, develop, or hotfix.\nrelease: Let\u0026rsquo;s say all changes are complete. When a new version is to be released, a new Release branch is created from the Develop branch. Release branches are typically named in the format \u0026lsquo;release/[version]\u0026rsquo;. Final changes in the version, changing version numbers, etc. are performed in this branch. Only bug fixes should be made in the Release branch, new features should not be added. When all necessary changes are completed, all changes completed in the Release branch are merged into both Master and Develop branches. The version number is tagged with git tag in the Master branch, and then the Release branch is deleted.\nGit Flow Example Project /brew install git-flow \u0026gt; git flow init First, we install with \u0026ldquo;brew install git-flow\u0026rdquo;. GitFlow doesn\u0026rsquo;t come with git. It needs to be installed separately. This is considered one of its disadvantages. In git, we used the \u0026ldquo;git init\u0026rdquo; command to start the project. For git-flow, we enter the \u0026ldquo;git flow init\u0026rdquo; command to start the git-flow process. When the command runs, if no repo exists, it first creates a repo. Then it asks the user for branch names to be used for the process. Branch names can be customized, but it\u0026rsquo;s recommended to keep the default values.\n/git flow feature start performance This command creates a new feature branch. Since the feature name is performance, the default branch will be feature/performance. We can do the same thing with the existing git command. The command we would need to enter for this would be \u0026ldquo;git checkout -b myFeature feature/performance\u0026rdquo;.\n/git flow feature finish performance This command closes a previously opened branch. The closing process starts with merging the feature branch into the develop branch and ends with deleting the feature branch. If changes are not committed when the command is run, it will give an error. If push is not done after commit, it will give an error. To do this operation with normal git commands, first commit is made in the relevant branch, then \u0026ldquo;git checkout develop \u0026gt; git merge \u0026ndash;no-ff feature/performance \u0026gt; git branch -D feature/performance\u0026rdquo; commands are run in order.\n/git flow release start 1.0.0 When this command is entered, a new version becomes ready for release. When the command is run, a new release/1.0.0 branch is created from the Develop branch. To perform this operation with the existing git command, the \u0026ldquo;git checkout -b release/1.0.0 develop\u0026rdquo; command is run.\n/git flow release finish 1.0.0 When the command is entered, the completed version is moved to the master branch. The changes are merged with both develop and master branches. The last commit in the master branch is tagged with the version number. Then the release branch is automatically deleted. To do the same operation with git commands, run the following commands in order:\ngit checkout master git merge --no-ff release/1.0.0 git tag -a 1.0.0 git checkout develop git merge --no-ff release/1.0.0 git branch -d release/1.0.0 /git flow hotfix start 1.0.1 A new hotfix is started with this command. Hotfix branches are used for urgent updates and are created from the master branch. When the command runs, a new hotfix/1.0.1 branch branching from the master branch is created. To do the operation with git command, run \u0026ldquo;git checkout -b hotfix/1.0.1 master\u0026rdquo; command.\n/git flow hotfix finish 1.0.1 The hotfix is completed with this command. Changes are taken to both Develop and Master branches. The Master branch is tagged with 1.0.1 and the hotfix branch is deleted. When the \u0026ldquo;git tag -l\u0026rdquo; command is run, version numbers are displayed. To do the same operation with existing git commands, run the following commands in order:\ngit checkout master git merge --no-ff hotfix/1.0.1 git tag -a 1.0.1 git checkout develop git merge --no-ff hotfix/1.0.1 git branch -d hotfix/1.0.1 Git Flow Best Practice Recommendations Branch Naming Conventions\nUse descriptive names for feature branches (e.g., feature/user-authentication) Use semantic versioning for hotfix and release branches Commit Messages\nWrite descriptive commit messages Follow the Conventional Commits standard Ensure each commit has a single purpose Code Review Process\nPerform code review before merging feature branches Use automated testing processes Don\u0026rsquo;t forget documentation updates Merge Strategy\nUse the \u0026ndash;no-ff (no fast-forward) parameter Resolve merge conflicts quickly Consider using squash commits Conclusion Git Flow is an effective approach that systematizes branch management in modern software development processes. Through this workflow, teams can work more organized, better control versioning, and improve code quality. Especially in large projects and team collaborations, the structured branch strategy offered by Git Flow significantly improves development processes. By following the best practices mentioned above, you can successfully implement Git Flow in your project and make your software development processes more efficient.\n","lang":"en","permalink":"/en/blog/git-flow/","summary":"Git Flow is an effective approach that systematizes branch management in version control systems. In this article, I will explain in detail what Git Flow is, how it\u0026rsquo;s used, and best practice recommendations.","tags":["git-flow","git"],"title":"What is Git Flow?"},{"categories":["archives"],"contents":"","lang":"en","permalink":"/en/archives/","summary":"","tags":null,"title":"Archives"},{"categories":["Development","iOS"],"contents":"\nHello everyone! Welcome to the first article about App Store Connect API. In this first article, I am going to give a brief explanation of App Store Connect API and how to create JWT tokens for the API.\nWhat is App Store Connect API? The App Store Connect API is a REST API that enables the automation of actions you take in App Store Connect. This API empowers developers to seamlessly handle various tasks, from app submission and updates to managing in-app purchases, monitoring app performance and user engagement through comprehensive reports, responding to customer reviews and feedback effectively, and many more.\nWhy JWT Token? Without a token, you won\u0026rsquo;t be able to get the response from the App Store Connect API but mainly API requires JSON Web Token (JWT) for authentication and authorization purposes. In the context of the API, the JWT token serves as a secure and standardized method to verify the identity of the client and to ensure that it has the necessary permissions to access the requested resources.\nHow to Create a JWT Token? Before creating JWT tokens for using the App Store Connect API we need a few steps to setup.\nGenerate API Key from App Store Connect GUI\nDownload the private key in the p8 format\nCopy your Issuer ID and API Key ID\nGenerate API Key To generate an API key, we have to log in to the App Store Connect web interface with our account and go to the Users and Access tab. On the page click Keys. To be able to see the Keys tab your account has to have permission. We can create an API key for a specific purpose or an admin API key that can access all the App Store Connect API.\nClick the plus icon next to the \u0026lsquo;active\u0026rsquo; text type a name for the key choose the roles that can access to key from the modal and click \u0026lsquo;Generate\u0026rsquo;. After that key will be created and listed.\nDownload Private Key Once the API Key is generated we will download the Private Key. The key is usually in the .p8 format. Some somethings are important to keep in mind whilst dealing with the private key.\nThe private key can be downloaded only once. Make sure to keep it secure once downloaded.\nThe private key never expires and is used to work as long as it\u0026rsquo;s valid even if it\u0026rsquo;s compromised so if you think that your key is not safe anymore, revoke it from App Store Connect as soon as possible and get a new key.\nCopy Your Issuer and API Key ID The last step before creating the JTW token is copying your issuer ID and API key ID which you can find on the Users\u0026amp;Access page.\nCreate JWT Token As mentioned earlier, JWT is used to generate the token that has been used by the App Store Connect API. The process of creating a token requires the following steps:\nIssuerID: The ID copied from the User\u0026amp;Access page.\nPrivate Key: Key downloaded in .p8 format.\nExpiration Time: 20 min maximum, the token cannot be valid for more than 20 minutes so we have to make sure that, we will create a new token before it expires.\nAudience: This is constant with API version value usually \u0026lsquo;applestoreconnect-v1\u0026rsquo;\nAlgorithm: The JWT algorithm required to create tokens e.g ES256\nOnce we have all the necessary details, we will be able to create a JWT token using the desired language. I am going to use Node.js for this process.\nRun these commands in order:\nmkdir appStoreToken cd appStoreToken npm init -y npm i jsonwebtoken touch index.js Paste this code into your index.js file, replace the necessary information with your information, and save.\nconst fs = require(\u0026#34;fs\u0026#34;); const jwt = require(\u0026#34;jsonwebtoken\u0026#34;); const privateKey = fs.readFileSync(\u0026#34;yourPrivateKey.p8\u0026#34;); const apiKeyId = \u0026#34;Your API Key ID\u0026#34;; const issuerId = \u0026#34;Your Issuer ID\u0026#34;; let now = Math.round(new Date().getTime() / 1000); // Notice the /1000 let nowPlus20 = now + 1199; // 1200 === 20 minutes let payload = { iss: issuerId, exp: nowPlus20, aud: \u0026#34;appstoreconnect-v1\u0026#34;, }; let signOptions = { algorithm: \u0026#34;ES256\u0026#34;, header: { alg: \u0026#34;ES256\u0026#34;, kid: apiKeyId, typ: \u0026#34;JWT\u0026#34;, }, }; let token = jwt.sign(payload, privateKey, signOptions); console.log(\u0026#34;@token: \u0026#34;, token); and lastly, run the command below.\nnode index.js This will return a long token that we can use to access an App Store Connect API, we also need to create another token if we want to continue using API after 20 minutes.\nConclusion In this post, you learned how the create a JWT token for the App Store Connect API using Node.js. This is a critical step in authenticating your requests to the API. In future posts, I will try to explore the other ways you can use the App Store Connect API. Thank you for your reading.\nRead this article in Turkish\n","lang":"en","permalink":"/en/blog/how-to-create-jwt-token-using-app-store-connect-api/","summary":"Hello everyone! Welcome to the first article about App Store Connect API. In this first article, I am going to give a brief explanation of App Store Connect API and how to create JWT tokens for the API.","tags":["jwt","app-store","api"],"title":"How to Create JWT Tokens for using App Store Connect API?"},{"categories":null,"contents":"I graduated in Computer Engineering with both a bachelor\u0026rsquo;s and master\u0026rsquo;s degree. Over the past 5 years in the tech industry, I\u0026rsquo;ve worked in various sectors, including travel and finance. However, my primary focus has been on AI chatbot projects. I spent 2.5 years as a business analyst and the rest as a software developer, refining my skills and expertise in this specialized field. Currently, I am thrilled to be a software developer at Juphy, contributing to exciting and innovative technology projects, particularly in the realm of AI chatbots.\n","lang":"en","permalink":"/en/about/","summary":"\u003cp\u003eI graduated in Computer Engineering with both a bachelor\u0026rsquo;s and master\u0026rsquo;s degree. Over the past 5 years in the tech industry, I\u0026rsquo;ve worked in various sectors, including travel and finance. However, my primary focus has been on AI chatbot projects. I spent 2.5 years as a business analyst and the rest as a software developer, refining my skills and expertise in this specialized field. Currently, I am thrilled to be a software developer at Juphy, contributing to exciting and innovative technology projects, particularly in the realm of AI chatbots.\u003c/p\u003e","tags":null,"title":""}]